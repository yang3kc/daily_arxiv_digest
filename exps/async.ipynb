{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from openai import AsyncOpenAI\n",
    "import asyncio\n",
    "import json\n",
    "import gzip\n",
    "import os\n",
    "from pydantic import BaseModel, Field, RootModel\n",
    "\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config.json\") as f:\n",
    "        config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_list = []\n",
    "with gzip.open(os.path.join(root_dir, \"2024-04-23.json.gz\")) as f:\n",
    "    paper_list = json.loads(f.read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(paper_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_paper_list = dict()\n",
    "for key, value in paper_list.items():\n",
    "    temp_paper_list[key] = value\n",
    "    if len(temp_paper_list) > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/test_arxiv_paper_info.json\", \"w\") as f:\n",
    "    json.dump(temp_paper_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'oai:arXiv.org:2404.13060v1': {'id': 'oai:arXiv.org:2404.13060v1',\n",
       "  'title': 'The Necessity of AI Audit Standards Boards',\n",
       "  'abstract': \"arXiv:2404.13060v1 Announce Type: new \\nAbstract: Auditing of AI systems is a promising way to understand and manage ethical problems and societal risks associated with contemporary AI systems, as well as some anticipated future risks. Efforts to develop standards for auditing Artificial Intelligence (AI) systems have therefore understandably gained momentum. However, we argue that creating auditing standards is not just insufficient, but actively harmful by proliferating unheeded and inconsistent standards, especially in light of the rapid evolution and ethical and safety challenges of AI. Instead, the paper proposes the establishment of an AI Audit Standards Board, responsible for developing and updating auditing methods and standards in line with the evolving nature of AI technologies. Such a body would ensure that auditing practices remain relevant, robust, and responsive to the rapid advancements in AI. The paper argues that such a governance structure would also be helpful for maintaining public trust in AI and for promoting a culture of safety and ethical responsibility within the AI industry.\\n  Throughout the paper, we draw parallels with other industries, including safety-critical industries like aviation and nuclear energy, as well as more prosaic ones such as financial accounting and pharmaceuticals. AI auditing should emulate those fields, and extend beyond technical assessments to include ethical considerations and stakeholder engagement, but we explain that this is not enough; emulating other fields' governance mechanisms for these processes, and for audit standards creation, is a necessity. We also emphasize the importance of auditing the entire development process of AI systems, not just the final products...\",\n",
       "  'url': 'https://arxiv.org/abs/2404.13060',\n",
       "  'authors': ['David Manheim, Sammy Martin, Mark Bailey, Mikhail Samin, Ross Greutzmacher']},\n",
       " 'oai:arXiv.org:2404.13068v1': {'id': 'oai:arXiv.org:2404.13068v1',\n",
       "  'title': 'SmartPathfinder: Pushing the Limits of Heuristic Solutions for Vehicle Routing Problem with Drones Using Reinforcement Learning',\n",
       "  'abstract': 'arXiv:2404.13068v1 Announce Type: new \\nAbstract: The Vehicle Routing Problem with Drones (VRPD) seeks to optimize the routing paths for both trucks and drones, where the trucks are responsible for delivering parcels to customer locations, and the drones are dispatched from these trucks for parcel delivery, subsequently being retrieved by the trucks. Given the NP-Hard complexity of VRPD, numerous heuristic approaches have been introduced. However, improving solution quality and reducing computation time remain significant challenges. In this paper, we conduct a comprehensive examination of heuristic methods designed for solving VRPD, distilling and standardizing them into core elements. We then develop a novel reinforcement learning (RL) framework that is seamlessly integrated with the heuristic solution components, establishing a set of universal principles for incorporating the RL framework with heuristic strategies in an aim to improve both the solution quality and computation speed. This integration has been applied to a state-of-the-art heuristic solution for VRPD, showcasing the substantial benefits of incorporating the RL framework. Our evaluation results demonstrated that the heuristic solution incorporated with our RL framework not only elevated the quality of solutions but also achieved rapid computation speeds, especially when dealing with extensive customer locations.',\n",
       "  'url': 'https://arxiv.org/abs/2404.13068',\n",
       "  'authors': ['Navid Mohammad Imran, Myounggyu Won']},\n",
       " 'oai:arXiv.org:2404.13131v1': {'id': 'oai:arXiv.org:2404.13131v1',\n",
       "  'title': 'From Model Performance to Claim: How a Change of Focus in Machine Learning Replicability Can Help Bridge the Responsibility Gap',\n",
       "  'abstract': \"arXiv:2404.13131v1 Announce Type: new \\nAbstract: Two goals - improving replicability and accountability of Machine Learning research respectively, have accrued much attention from the AI ethics and the Machine Learning community. Despite sharing the measures of improving transparency, the two goals are discussed in different registers - replicability registers with scientific reasoning whereas accountability registers with ethical reasoning. Given the existing challenge of the Responsibility Gap - holding Machine Learning scientists accountable for Machine Learning harms due to them being far from sites of application, this paper posits that reconceptualizing replicability can help bridge the gap. Through a shift from model performance replicability to claim replicability, Machine Learning scientists can be held accountable for producing non-replicable claims that are prone to eliciting harm due to misuse and misinterpretation. In this paper, I make the following contributions. First, I define and distinguish two forms of replicability for ML research that can aid constructive conversations around replicability. Second, I formulate an argument for claim-replicability's advantage over model performance replicability in justifying assigning accountability to Machine Learning scientists for producing non-replicable claims and show how it enacts a sense of responsibility that is actionable. In addition, I characterize the implementation of claim replicability as more of a social project than a technical one by discussing its competing epistemological principles, practical implications on Circulating Reference, Interpretative Labor, and research communication.\",\n",
       "  'url': 'https://arxiv.org/abs/2404.13131',\n",
       "  'authors': ['Tianqi Kou']},\n",
       " 'oai:arXiv.org:2404.13172v1': {'id': 'oai:arXiv.org:2404.13172v1',\n",
       "  'title': 'Insights from an experiment crowdsourcing data from thousands of US Amazon users: The importance of transparency, money, and data use',\n",
       "  'abstract': 'arXiv:2404.13172v1 Announce Type: new \\nAbstract: Data generated by users on digital platforms are a crucial resource for advocates and researchers interested in uncovering digital inequities, auditing algorithms, and understanding human behavior. Yet data access is often restricted. How can researchers both effectively and ethically collect user data? This paper shares an innovative approach to crowdsourcing user data to collect otherwise inaccessible Amazon purchase histories, spanning 5 years, from more than 5000 US users. We developed a data collection tool that prioritizes participant consent and includes an experimental study design. The design allows us to study multiple aspects of privacy perception and data sharing behavior. Experiment results (N=6325) reveal both monetary incentives and transparency can significantly increase data sharing. Age, race, education, and gender also played a role, where female and less-educated participants were more likely to share. Our study design enables a unique empirical evaluation of the \"privacy paradox\", where users claim to value their privacy more than they do in practice. We set up both real and hypothetical data sharing scenarios and find measurable similarities and differences in share rates across these contexts. For example, increasing monetary incentives had a 6 times higher impact on share rates in real scenarios. In addition, we study participants\\' opinions on how data should be used by various third parties, again finding demographics have a significant impact. Notably, the majority of participants disapproved of government agencies using purchase data yet the majority approved of use by researchers. Overall, our findings highlight the critical role that transparency, incentive design, and user demographics play in ethical data collection practices, and provide guidance for future researchers seeking to crowdsource user generated data.',\n",
       "  'url': 'https://arxiv.org/abs/2404.13172',\n",
       "  'authors': ['Alex Berke, Robert Mahari, Sandy Pentland, Kent Larson, D. Calacci']},\n",
       " 'oai:arXiv.org:2404.13426v1': {'id': 'oai:arXiv.org:2404.13426v1',\n",
       "  'title': 'Data Privacy Vocabulary (DPV) -- Version 2',\n",
       "  'abstract': \"arXiv:2404.13426v1 Announce Type: new \\nAbstract: The Data Privacy Vocabulary (DPV), developed by the W3C Data Privacy Vocabularies and Controls Community Group (DPVCG), enables the creation of machine-readable, interoperable, and standards-based representations for describing the processing of personal data. The group has also published extensions to the DPV to describe specific applications to support legislative requirements such as the EU's GDPR. The DPV fills a crucial niche in the state of the art by providing a vocabulary that can be embedded and used alongside other existing standards such as W3C ODRL, and which can be customised and extended for adapting to specifics of use-cases or domains. This article describes the version 2 iteration of the DPV in terms of its contents, methodology, current adoptions and uses, and future potential. It also describes the relevance and role of DPV in acting as a common vocabulary to support various regulatory (e.g. EU's DGA and AI Act) and community initiatives (e.g. Solid) emerging across the globe.\",\n",
       "  'url': 'https://arxiv.org/abs/2404.13426',\n",
       "  'authors': ['Harshvardhan J. Pandit, Beatriz Esteves, Georg P. Krog, Paul Ryan, Delaram Golpayegani, Julian Flake']}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_paper_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(os.path.join(root_dir, \"2024-04-26.json.gz\"), 'wb') as f:\n",
    "    bytes_to_write = json.dumps(temp_paper_list).encode(\"utf-8\")\n",
    "    f.write(bytes_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_paper_list = []\n",
    "for paper in paper_list.values():\n",
    "    exp_paper_list.append(paper)\n",
    "    if len(exp_paper_list) > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exp_paper_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'oai:arXiv.org:2404.13060v1',\n",
       " 'title': 'The Necessity of AI Audit Standards Boards',\n",
       " 'abstract': \"arXiv:2404.13060v1 Announce Type: new \\nAbstract: Auditing of AI systems is a promising way to understand and manage ethical problems and societal risks associated with contemporary AI systems, as well as some anticipated future risks. Efforts to develop standards for auditing Artificial Intelligence (AI) systems have therefore understandably gained momentum. However, we argue that creating auditing standards is not just insufficient, but actively harmful by proliferating unheeded and inconsistent standards, especially in light of the rapid evolution and ethical and safety challenges of AI. Instead, the paper proposes the establishment of an AI Audit Standards Board, responsible for developing and updating auditing methods and standards in line with the evolving nature of AI technologies. Such a body would ensure that auditing practices remain relevant, robust, and responsive to the rapid advancements in AI. The paper argues that such a governance structure would also be helpful for maintaining public trust in AI and for promoting a culture of safety and ethical responsibility within the AI industry.\\n  Throughout the paper, we draw parallels with other industries, including safety-critical industries like aviation and nuclear energy, as well as more prosaic ones such as financial accounting and pharmaceuticals. AI auditing should emulate those fields, and extend beyond technical assessments to include ethical considerations and stakeholder engagement, but we explain that this is not enough; emulating other fields' governance mechanisms for these processes, and for audit standards creation, is a necessity. We also emphasize the importance of auditing the entire development process of AI systems, not just the final products...\",\n",
       " 'url': 'https://arxiv.org/abs/2404.13060',\n",
       " 'authors': ['David Manheim, Sammy Martin, Mark Bailey, Mikhail Samin, Ross Greutzmacher']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_paper_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMPaperReader:\n",
    "    system_message = \"\"\"\n",
    "        You are an assistant to help the user decide if a paper is very relevant to the topics of interests.\n",
    "        \"\"\"\n",
    "\n",
    "    user_message = \"\"\"\n",
    "        Please read the following paper title and abstract:\n",
    "        --------------\n",
    "        Title: {title}\n",
    "        Abstract: {abstract}\n",
    "        --------------\n",
    "        Based on the title and abstract, please rate the direct relevance of the paper with the following topics:\n",
    "        --------------\n",
    "        {topics}\n",
    "        --------------\n",
    "        For each topic, rate the relevance as a number between 0 and 1, where 0 means not relevant and 1 means very relevant.\n",
    "        The paper MUST directly mention the topics to be relevant; papers with indirect relations and potential implications should have scores close to 0.\n",
    "        If the paper is relevant to the topic, provide a short explanation; otherwise, leave the explanation empty.\n",
    "        Use your best guess when you are not sure.\n",
    "        The output should be in JSON format and follow the following schema:\n",
    "        --------------\n",
    "        ```json\n",
    "        {{\n",
    "            'topic 1': {{\n",
    "                'relevance': 0,\n",
    "                'reason': ''\n",
    "            }},\n",
    "            'topic 2': {{\n",
    "                'relevance': 0.9,\n",
    "                'reason': 'The paper ....'\n",
    "            }}\n",
    "        }}\n",
    "         ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, topics):\n",
    "        self.model = model\n",
    "        self.topics = topics\n",
    "        self.client = AsyncOpenAI()\n",
    "\n",
    "    async def read_papers(self, papers):\n",
    "        responses = await asyncio.gather(\n",
    "            *[self.read_paper(paper) for paper in papers]\n",
    "        )\n",
    "        return responses\n",
    "\n",
    "    async def read_paper(self, paper):\n",
    "        response = await self._call_api(paper)\n",
    "        response_content = response.choices[0].message.content\n",
    "        return response_content\n",
    "        # response_json = json.loads(response_content)\n",
    "        # return response_json\n",
    "\n",
    "    async def _call_api(self, paper):\n",
    "        response = await self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_message},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": self.user_message.format(\n",
    "                        title=paper[\"title\"],\n",
    "                        abstract=paper[\"abstract\"],\n",
    "                        topics=self.topics,\n",
    "                    ),\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_reader = LLMPaperReader(config[\"openai_model\"], config[\"topics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = await llm_reader.read_paper(exp_paper_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    {\\n        \"Security of AI and language models\": {\\n            \"relevance\": 0,\\n            \"reason\": \"\"\\n        },\\n        \"Applications of AI and language models in social science research\": {\\n            \"relevance\": 0,\\n            \"reason\": \"\"\\n        },\\n        \"Using AI to simulate humans in various contexts\": {\\n            \"relevance\": 0,\\n            \"reason\": \"\"\\n        },\\n        \"Methods to increase the factuality of language model response\": {\\n            \"relevance\": 0,\\n            \"reason\": \"\"\\n        },\\n        \"AI and language models for generating misinformation or fact-checking\": {\\n            \"relevance\": 0.8,\\n            \"reason\": \"The paper focuses on the importance of auditing AI systems for ethical and safety reasons, which indirectly relates to the need for fact-checking and ensuring accuracy in AI-generated content.\"\\n        }\\n    }\\n    '"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Judgement(BaseModel):\n",
    "    relevance: float = Field(ge=0, le=1)\n",
    "    reason: str\n",
    "\n",
    "JudgementDict = RootModel[Dict[str, Judgement]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'relevance': {'maximum': 1.0,\n",
       "   'minimum': 0.0,\n",
       "   'title': 'Relevance',\n",
       "   'type': 'number'},\n",
       "  'reason': {'title': 'Reason', 'type': 'string'}},\n",
       " 'required': ['relevance', 'reason'],\n",
       " 'title': 'Judgement',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Judgement.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_judgement = results[\"AI and language models for generating misinformation or fact-checking\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_result = JudgementDict.model_validate_json(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"Security of AI and language models\":{\"relevance\":0.0,\"reason\":\"\"},\"Applications of AI and language models in social science research\":{\"relevance\":0.0,\"reason\":\"\"},\"Using AI to simulate humans in various contexts\":{\"relevance\":0.0,\"reason\":\"\"},\"Methods to increase the factuality of language model response\":{\"relevance\":0.0,\"reason\":\"\"},\"AI and language models for generating misinformation or fact-checking\":{\"relevance\":0.8,\"reason\":\"The paper focuses on the importance of auditing AI systems for ethical and safety reasons, which indirectly relates to the need for fact-checking and ensuring accuracy in AI-generated content.\"}}'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_result.model_dump_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RootModel[Dict[str, Judgement]]' object has no attribute 'to_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dump_result \u001b[38;5;241m=\u001b[39m \u001b[43mvalidation_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_json\u001b[49m()\n",
      "File \u001b[0;32m~/mambaforge/envs/daily_arxiv_digest/lib/python3.10/site-packages/pydantic/main.py:811\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    810\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[0;32m--> 811\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RootModel[Dict[str, Judgement]]' object has no attribute 'to_json'"
     ]
    }
   ],
   "source": [
    "dump_result = validation_result.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dump_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"properties\": {\n",
      "    \"relevance\": {\n",
      "      \"maximum\": 1.0,\n",
      "      \"minimum\": 0.0,\n",
      "      \"title\": \"Relevance\",\n",
      "      \"type\": \"number\"\n",
      "    },\n",
      "    \"reason\": {\n",
      "      \"title\": \"Reason\",\n",
      "      \"type\": \"string\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"relevance\",\n",
      "    \"reason\"\n",
      "  ],\n",
      "  \"title\": \"Judgement\",\n",
      "  \"type\": \"object\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(Judgement.schema_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = await llm_reader.read_papers(exp_paper_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = asyncio.get_event_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_paper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_paper_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/daily_arxiv_digest/lib/python3.10/asyncio/base_events.py:625\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run until the Future is done.\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \n\u001b[1;32m    616\u001b[0m \u001b[38;5;124;03mIf the argument is a coroutine, it is wrapped in a Task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03mReturn the Future's result, or raise its exception.\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m--> 625\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m new_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m futures\u001b[38;5;241m.\u001b[39misfuture(future)\n\u001b[1;32m    628\u001b[0m future \u001b[38;5;241m=\u001b[39m tasks\u001b[38;5;241m.\u001b[39mensure_future(future, loop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/daily_arxiv_digest/lib/python3.10/asyncio/base_events.py:584\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_running\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[0;32m--> 584\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis event loop is already running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    587\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot run the event loop while another loop is running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    }
   ],
   "source": [
    "loop.run_until_complete(llm_reader.read_paper(exp_paper_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "judgement_list = []\n",
    "with open(\"../data/2024-04-26.resp.json\") as f:\n",
    "    for line in f:\n",
    "        temp_judgement = json.loads(line)\n",
    "        judgement_list.append(temp_judgement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'oai:arXiv.org:2404.13060v1',\n",
       " 'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "   'reason': ''},\n",
       "  'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "   'reason': ''},\n",
       "  'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "   'reason': ''},\n",
       "  'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "   'reason': ''},\n",
       "  'AI and language models for generating misinformation or fact-checking': {'relevance': 0.9,\n",
       "   'reason': 'The paper focuses on the auditing of AI systems and proposes the establishment of an AI Audit Standards Board to manage ethical problems and societal risks associated with AI systems. While it does not directly mention generating misinformation or fact-checking, the emphasis on ethical considerations and governance mechanisms aligns with the broader discussion around ensuring the responsible use of AI, which could indirectly relate to fact-checking and misinformation detection.'},\n",
       "  'id': 'oai:arXiv.org:2404.13060v1'}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judgement_list[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daily_arxiv_digest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
