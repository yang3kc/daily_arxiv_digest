{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1581303c-eed3-429a-8894-9301e04655f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "710b4d28-eaaa-486a-9870-04cf95d33d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivRSS:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.paper_df = None\n",
    "\n",
    "    def fetch_paper_list(self):\n",
    "        feed = self._fetch_n_parse_rss()\n",
    "\n",
    "        paper_list = []\n",
    "        for rss_entry in feed[\"entries\"]:\n",
    "            paper_information = self._extract_paper_information(rss_entry)\n",
    "            paper_list.append(paper_information)\n",
    "        self.paper_df = pd.DataFrame(paper_list)\n",
    "        return self.paper_df\n",
    "\n",
    "    def _fetch_n_parse_rss(self):\n",
    "        feed = feedparser.parse(self.url)\n",
    "        return feed\n",
    "\n",
    "    def _parse_html_element(self, raw_string):\n",
    "        soup = BeautifulSoup(raw_string, \"html.parser\")\n",
    "        return soup.text\n",
    "\n",
    "    def _extract_paper_information(self, rss_entry):\n",
    "        paper_id = rss_entry[\"id\"]\n",
    "        paper_title = rss_entry[\"title\"]\n",
    "        paper_abstract = self._parse_html_element(rss_entry[\"summary\"])\n",
    "        paper_url = rss_entry[\"link\"]\n",
    "        paper_authors = []\n",
    "        for author_info in rss_entry[\"authors\"]:\n",
    "            author_name = self._parse_html_element(author_info[\"name\"])\n",
    "            paper_authors.append(author_name)\n",
    "        return {\n",
    "            \"id\": paper_id,\n",
    "            \"title\": paper_title,\n",
    "            \"abstract\": paper_abstract,\n",
    "            \"url\": paper_url,\n",
    "            \"authors\": paper_authors,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0d1ae171-8240-4190-bb2d-5c147386581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Judgement(BaseModel):\n",
    "    topic: str = Field(description=\"The topic of the paper\")\n",
    "    relevance: float = Field(description=\"The relevance of the paper to the topic, a number between 0 and 1\")\n",
    "    reason: str = Field(description=\"The reason for the relevance\")\n",
    "\n",
    "class Judgements(BaseModel):\n",
    "    judgement: List[Rating] = Field(description=\"A list of topics with relevance and reasoning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "39dce7e5-40e7-4348-aefd-4355dd022465",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"topic1\": {\n",
    "        \"relevance\": 0,\n",
    "        \"reason\": \"\"\n",
    "    },\n",
    "    \"topic2\": {\n",
    "        \"relevance\": 0.9,\n",
    "        \"reason\": \"The paper ....\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b0745a5c-fb94-4e2e-9482-ed1e94831deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RatingDict(root={'topic1': Rating(relevance=0.0, reason=''), 'topic2': Rating(relevance=0.9, reason='The paper ....')})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RatingDict.model_validate(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19023507-aca9-44fb-a655-c61269257a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(timeout=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "840946fe-a5e1-4739-ad7a-299694d2e6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "    You are an assistant to help the user decide if a paper is very relevant to the topics of interests.\n",
    "    \"\"\"\n",
    "\n",
    "user_message = \"\"\"\n",
    "    Please read the following paper title and abstract:\n",
    "    --------------\n",
    "    Title: {title}\n",
    "    Abstract: {abstract}\n",
    "    --------------\n",
    "    Based on the title and abstract, please rate the direct relevance of the paper with the following topics:\n",
    "    --------------\n",
    "    {topics}\n",
    "    --------------\n",
    "    For each topic, rate the relevance as a number between 0 and 1, where 0 means not relevant and 1 means very relevant.\n",
    "    The paper MUST directly mention the topics to be relevant; papers with indirect relations and potential implications should have scores close to 0.\n",
    "    If the paper is relevant to the topic, provide a short explanation; otherwise, leave the explanation empty.\n",
    "    Use your best guess when you are not sure.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2e79516-13eb-453a-83de-efdf8abc3530",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = {\n",
    "    \"title\": \"Position: Restructuring of Categories and Implementation of Guidelines\",\n",
    "    \"abstract\": \"\"\"The intricate and multifaceted nature of vision language model (VLM)\n",
    "development, adaptation, and application necessitates the establishment of\n",
    "clear and standardized reporting protocols, particularly within the high-stakes\n",
    "context of healthcare. Defining these reporting standards is inherently\n",
    "challenging due to the diverse nature of studies involving VLMs, which vary\n",
    "significantly from the development of all new VLMs or finetuning for domain\n",
    "alignment to off-the-shelf use of VLM for targeted diagnosis and prediction\n",
    "tasks. In this position paper, we argue that traditional machine learning\n",
    "reporting standards and evaluation guidelines must be restructured to\n",
    "accommodate multiphase VLM studies; it also has to be organized for intuitive\n",
    "understanding of developers while maintaining rigorous standards for\n",
    "reproducibility. To facilitate community adoption, we propose a categorization\n",
    "framework for VLM studies and outline corresponding reporting standards that\n",
    "comprehensively address performance evaluation, data reporting protocols, and\n",
    "recommendations for manuscript composition. These guidelines are organized\n",
    "according to the proposed categorization scheme. Lastly, we present a checklist\n",
    "that consolidates reporting standards, offering a standardized tool to ensure\n",
    "consistency and quality in the publication of VLM-related research.\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eead0cd0-fc3d-46e7-873f-fd603a23d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config.json\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9bd97f13-973b-4856-b5b4-66c4c89bf57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.parse(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    temperature=0.0,\n",
    "    instructions=system_message,\n",
    "    input=user_message.format(\n",
    "        title=paper['title'],\n",
    "        abstract=paper['abstract'],\n",
    "        topics=config['topics']\n",
    "    ),\n",
    "    text_format=RatingList,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d896b377-6203-4461-8c04-086726c9a82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_judgement_df = pd.DataFrame(response.output_parsed.model_dump()['ratings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b79bbf0a-d6bc-47e1-bbdc-f559eecfcf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_judgement_df['id'] = 'oai:arXiv.org:2505.08818v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "525d9757-8436-4b89-a244-868b2326370c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    Please read the following paper title and abstract:\\n    --------------\\n    Title: Position: Restructuring of Categories and Implementation of Guidelines\\n    Abstract: The intricate and multifaceted nature of vision language model (VLM)\\ndevelopment, adaptation, and application necessitates the establishment of\\nclear and standardized reporting protocols, particularly within the high-stakes\\ncontext of healthcare. Defining these reporting standards is inherently\\nchallenging due to the diverse nature of studies involving VLMs, which vary\\nsignificantly from the development of all new VLMs or finetuning for domain\\nalignment to off-the-shelf use of VLM for targeted diagnosis and prediction\\ntasks. In this position paper, we argue that traditional machine learning\\nreporting standards and evaluation guidelines must be restructured to\\naccommodate multiphase VLM studies; it also has to be organized for intuitive\\nunderstanding of developers while maintaining rigorous standards for\\nreproducibility. To facilitate community adoption, we propose a categorization\\nframework for VLM studies and outline corresponding reporting standards that\\ncomprehensively address performance evaluation, data reporting protocols, and\\nrecommendations for manuscript composition. These guidelines are organized\\naccording to the proposed categorization scheme. Lastly, we present a checklist\\nthat consolidates reporting standards, offering a standardized tool to ensure\\nconsistency and quality in the publication of VLM-related research.\\n    --------------\\n    Based on the title and abstract, please rate the direct relevance of the paper with the following topics:\\n    --------------\\n    ['Security of AI and language models', 'Applications of AI and language models in social science research', 'Using AI to simulate humans in various contexts', 'Methods to increase the factuality of language model response', 'AI and language models for generating misinformation or fact-checking', 'Methods and applications of using AI for image and video analysis']\\n    --------------\\n    For each topic, rate the relevance as a number between 0 and 1, where 0 means not relevant and 1 means very relevant.\\n    The paper MUST directly mention the topics to be relevant; papers with indirect relations and potential implications should have scores close to 0.\\n    If the paper is relevant to the topic, provide a short explanation; otherwise, leave the explanation empty.\\n    Use your best guess when you are not sure.\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_message.format(\n",
    "        title=paper['title'],\n",
    "        abstract=paper['abstract'],\n",
    "        topics=config['topics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0040fe10-ac8d-42f0-ae69-e559c0637a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.output_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe6f8022-cdf5-4324-95a7-2ec15021961b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ParsedResponseOutputMessage[NoneType](id='msg_68264dae0e0c8191834afa3e099a0db1013b1895f6eb3685', content=[ParsedResponseOutputText[NoneType](annotations=[], text='{\\n  \"Security of AI and language models\": 0,\\n  \"Applications of AI and language models in social science research\": 0,\\n  \"Using AI to simulate humans in various contexts\": 0,\\n  \"Methods to increase the factuality of language model response\": 0,\\n  \"AI and language models for generating misinformation or fact-checking\": 0,\\n  \"Methods and applications of using AI for image and video analysis\": 0.7,\\n  \"explanations\": {\\n    \"Methods and applications of using AI for image and video analysis\": \"The paper discusses vision language models (VLMs), which inherently involve image and language data, and addresses reporting standards for their development and application, including targeted diagnosis and prediction tasks that likely involve image analysis.\"\\n  }\\n}', type='output_text', parsed=None)], role='assistant', status='completed', type='message')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cbf0ce68-9758-4425-b1f7-a98e73756870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arxiv_rss_base_url': 'http://arxiv.org/rss/',\n",
       " 'arxiv_subjects': ['cs.CY', 'cs.CL'],\n",
       " 'topics': ['Security of AI and language models',\n",
       "  'Applications of AI and language models in social science research',\n",
       "  'Using AI to simulate humans in various contexts',\n",
       "  'Methods to increase the factuality of language model response',\n",
       "  'AI and language models for generating misinformation or fact-checking',\n",
       "  'Methods and applications of using AI for image and video analysis'],\n",
       " 'openai_model': 'gpt-4.1-mini',\n",
       " 'number_of_concurrent_tasks': 50,\n",
       " 'timeout_seconds': 30}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b5bf1c95-d1e6-4b33-9f64-ff9308176f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_subject= [\"\"]\n",
    "rss_url = config[\"arxiv_rss_base_url\"] + config[\"arxiv_subjects\"][0]\n",
    "arss = ArxivRSS(rss_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5ccdcf0f-3772-4117-b571-5f9755ba99bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_list_df = arss.fetch_paper_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b211f645-9d97-4158-8582-9bb931241d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>authors</th>\n",
       "      <th>topic</th>\n",
       "      <th>relevance</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oai:arXiv.org:2505.08818v1</td>\n",
       "      <td>Position: Restructuring of Categories and Impl...</td>\n",
       "      <td>arXiv:2505.08818v1 Announce Type: new \\nAbstra...</td>\n",
       "      <td>https://arxiv.org/abs/2505.08818</td>\n",
       "      <td>[Amara Tariq, Rimita Lahiri, Charles Kahn, Imo...</td>\n",
       "      <td>Security of AI and language models</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oai:arXiv.org:2505.08818v1</td>\n",
       "      <td>Position: Restructuring of Categories and Impl...</td>\n",
       "      <td>arXiv:2505.08818v1 Announce Type: new \\nAbstra...</td>\n",
       "      <td>https://arxiv.org/abs/2505.08818</td>\n",
       "      <td>[Amara Tariq, Rimita Lahiri, Charles Kahn, Imo...</td>\n",
       "      <td>Applications of AI and language models in soci...</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oai:arXiv.org:2505.08818v1</td>\n",
       "      <td>Position: Restructuring of Categories and Impl...</td>\n",
       "      <td>arXiv:2505.08818v1 Announce Type: new \\nAbstra...</td>\n",
       "      <td>https://arxiv.org/abs/2505.08818</td>\n",
       "      <td>[Amara Tariq, Rimita Lahiri, Charles Kahn, Imo...</td>\n",
       "      <td>Using AI to simulate humans in various contexts</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oai:arXiv.org:2505.08818v1</td>\n",
       "      <td>Position: Restructuring of Categories and Impl...</td>\n",
       "      <td>arXiv:2505.08818v1 Announce Type: new \\nAbstra...</td>\n",
       "      <td>https://arxiv.org/abs/2505.08818</td>\n",
       "      <td>[Amara Tariq, Rimita Lahiri, Charles Kahn, Imo...</td>\n",
       "      <td>Methods to increase the factuality of language...</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oai:arXiv.org:2505.08818v1</td>\n",
       "      <td>Position: Restructuring of Categories and Impl...</td>\n",
       "      <td>arXiv:2505.08818v1 Announce Type: new \\nAbstra...</td>\n",
       "      <td>https://arxiv.org/abs/2505.08818</td>\n",
       "      <td>[Amara Tariq, Rimita Lahiri, Charles Kahn, Imo...</td>\n",
       "      <td>AI and language models for generating misinfor...</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>oai:arXiv.org:2505.08818v1</td>\n",
       "      <td>Position: Restructuring of Categories and Impl...</td>\n",
       "      <td>arXiv:2505.08818v1 Announce Type: new \\nAbstra...</td>\n",
       "      <td>https://arxiv.org/abs/2505.08818</td>\n",
       "      <td>[Amara Tariq, Rimita Lahiri, Charles Kahn, Imo...</td>\n",
       "      <td>Methods and applications of using AI for image...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>The paper discusses vision language models (VL...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id  \\\n",
       "0  oai:arXiv.org:2505.08818v1   \n",
       "1  oai:arXiv.org:2505.08818v1   \n",
       "2  oai:arXiv.org:2505.08818v1   \n",
       "3  oai:arXiv.org:2505.08818v1   \n",
       "4  oai:arXiv.org:2505.08818v1   \n",
       "5  oai:arXiv.org:2505.08818v1   \n",
       "\n",
       "                                               title  \\\n",
       "0  Position: Restructuring of Categories and Impl...   \n",
       "1  Position: Restructuring of Categories and Impl...   \n",
       "2  Position: Restructuring of Categories and Impl...   \n",
       "3  Position: Restructuring of Categories and Impl...   \n",
       "4  Position: Restructuring of Categories and Impl...   \n",
       "5  Position: Restructuring of Categories and Impl...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  arXiv:2505.08818v1 Announce Type: new \\nAbstra...   \n",
       "1  arXiv:2505.08818v1 Announce Type: new \\nAbstra...   \n",
       "2  arXiv:2505.08818v1 Announce Type: new \\nAbstra...   \n",
       "3  arXiv:2505.08818v1 Announce Type: new \\nAbstra...   \n",
       "4  arXiv:2505.08818v1 Announce Type: new \\nAbstra...   \n",
       "5  arXiv:2505.08818v1 Announce Type: new \\nAbstra...   \n",
       "\n",
       "                                url  \\\n",
       "0  https://arxiv.org/abs/2505.08818   \n",
       "1  https://arxiv.org/abs/2505.08818   \n",
       "2  https://arxiv.org/abs/2505.08818   \n",
       "3  https://arxiv.org/abs/2505.08818   \n",
       "4  https://arxiv.org/abs/2505.08818   \n",
       "5  https://arxiv.org/abs/2505.08818   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [Amara Tariq, Rimita Lahiri, Charles Kahn, Imo...   \n",
       "1  [Amara Tariq, Rimita Lahiri, Charles Kahn, Imo...   \n",
       "2  [Amara Tariq, Rimita Lahiri, Charles Kahn, Imo...   \n",
       "3  [Amara Tariq, Rimita Lahiri, Charles Kahn, Imo...   \n",
       "4  [Amara Tariq, Rimita Lahiri, Charles Kahn, Imo...   \n",
       "5  [Amara Tariq, Rimita Lahiri, Charles Kahn, Imo...   \n",
       "\n",
       "                                               topic  relevance  \\\n",
       "0                 Security of AI and language models        0.0   \n",
       "1  Applications of AI and language models in soci...        0.0   \n",
       "2    Using AI to simulate humans in various contexts        0.0   \n",
       "3  Methods to increase the factuality of language...        0.0   \n",
       "4  AI and language models for generating misinfor...        0.0   \n",
       "5  Methods and applications of using AI for image...        0.7   \n",
       "\n",
       "                                              reason  \n",
       "0                                                     \n",
       "1                                                     \n",
       "2                                                     \n",
       "3                                                     \n",
       "4                                                     \n",
       "5  The paper discusses vision language models (VL...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_list_df.merge(paper_judgement_df, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ad5119c2-e95e-4576-ab2a-963fb53b8cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in paper_list_df.iterrows():\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "837d6d1b-44d8-4172-8661-2da452eb3913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'oai:arXiv.org:2505.08818v1',\n",
       "  'title': 'Position: Restructuring of Categories and Implementation of Guidelines Essential for VLM Adoption in Healthcare',\n",
       "  'abstract': 'arXiv:2505.08818v1 Announce Type: new \\nAbstract: The intricate and multifaceted nature of vision language model (VLM) development, adaptation, and application necessitates the establishment of clear and standardized reporting protocols, particularly within the high-stakes context of healthcare. Defining these reporting standards is inherently challenging due to the diverse nature of studies involving VLMs, which vary significantly from the development of all new VLMs or finetuning for domain alignment to off-the-shelf use of VLM for targeted diagnosis and prediction tasks. In this position paper, we argue that traditional machine learning reporting standards and evaluation guidelines must be restructured to accommodate multiphase VLM studies; it also has to be organized for intuitive understanding of developers while maintaining rigorous standards for reproducibility. To facilitate community adoption, we propose a categorization framework for VLM studies and outline corresponding reporting standards that comprehensively address performance evaluation, data reporting protocols, and recommendations for manuscript composition. These guidelines are organized according to the proposed categorization scheme. Lastly, we present a checklist that consolidates reporting standards, offering a standardized tool to ensure consistency and quality in the publication of VLM-related research.',\n",
       "  'url': 'https://arxiv.org/abs/2505.08818',\n",
       "  'authors': ['Amara Tariq, Rimita Lahiri, Charles Kahn, Imon Banerjee']},\n",
       " {'id': 'oai:arXiv.org:2505.08822v1',\n",
       "  'title': 'The Geography of Transportation Cybersecurity: Visitor Flows, Industry Clusters, and Spatial Dynamics',\n",
       "  'abstract': 'arXiv:2505.08822v1 Announce Type: new \\nAbstract: The rapid evolution of the transportation cybersecurity ecosystem, encompassing cybersecurity, automotive, and transportation and logistics sectors, will lead to the formation of distinct spatial clusters and visitor flow patterns across the US. This study examines the spatiotemporal dynamics of visitor flows, analyzing how socioeconomic factors shape industry clustering and workforce distribution within these evolving sectors. To model and predict visitor flow patterns, we develop a BiTransGCN framework, integrating an attention-based Transformer architecture with a Graph Convolutional Network backbone. By integrating AI-enabled forecasting techniques with spatial analysis, this study improves our ability to track, interpret, and anticipate changes in industry clustering and mobility trends, thereby supporting strategic planning for a secure and resilient transportation network. It offers a data-driven foundation for economic planning, workforce development, and targeted investments in the transportation cybersecurity ecosystem.',\n",
       "  'url': 'https://arxiv.org/abs/2505.08822',\n",
       "  'authors': ['Yuhao Wang (Jack), Kailai Wang (Jack), Songhua Hu (Jack),  Yunpeng (Jack),  Zhang, Gino Lim, Pengyu Zhu']},\n",
       " {'id': 'oai:arXiv.org:2505.08841v1',\n",
       "  'title': 'Will AI Take My Job? Evolving Perceptions of Automation and Labor Risk in Latin America',\n",
       "  'abstract': \"arXiv:2505.08841v1 Announce Type: new \\nAbstract: As artificial intelligence and robotics increasingly reshape the global labor market, understanding public perceptions of these technologies becomes critical. We examine how these perceptions have evolved across Latin America, using survey data from the 2017, 2018, 2020, and 2023 waves of the Latinobar\\\\'ometro. Drawing on responses from over 48,000 individuals across 16 countries, we analyze fear of job loss due to artificial intelligence and robotics. Using statistical modeling and latent class analysis, we identify key structural and ideological predictors of concern, with education level and political orientation emerging as the most consistent drivers. Our findings reveal substantial temporal and cross-country variation, with a notable peak in fear during 2018 and distinct attitudinal profiles emerging from latent segmentation. These results offer new insights into the social and structural dimensions of AI anxiety in emerging economies and contribute to a broader understanding of public attitudes toward automation beyond the Global North.\",\n",
       "  'url': 'https://arxiv.org/abs/2505.08841',\n",
       "  'authors': ['Andrea Cremaschi, Dae-Jin Lee, Manuele Leonelli']},\n",
       " {'id': 'oai:arXiv.org:2505.08904v1',\n",
       "  'title': 'FareShare: A Tool for Labor Organizers to Estimate Lost Wages and Contest Arbitrary AI and Algorithmic Deactivations',\n",
       "  'abstract': \"arXiv:2505.08904v1 Announce Type: new \\nAbstract: What happens when a rideshare driver is suddenly locked out of the platform connecting them to riders, wages, and daily work? Deactivation-the abrupt removal of gig workers' platform access-typically occurs through arbitrary AI and algorithmic decisions with little explanation or recourse. This represents one of the most severe forms of algorithmic control and often devastates workers' financial stability. Recent U.S. state policies now mandate appeals processes and recovering compensation during the period of wrongful deactivation based on past earnings. Yet, labor organizers still lack effective tools to support these complex, error-prone workflows. We designed FareShare, a computational tool automating lost wage estimation for deactivated drivers, through a 6 month partnership with the State of Washington's largest rideshare labor union. Over the following 3 months, our field deployment of FareShare registered 178 account signups. We observed that the tool could reduce lost wage calculation time by over 95%, eliminate manual data entry errors, and enable legal teams to generate arbitration-ready reports more efficiently. Beyond these gains, the deployment also surfaced important socio-technical challenges around trust, consent, and tool adoption in high-stakes labor contexts.\",\n",
       "  'url': 'https://arxiv.org/abs/2505.08904',\n",
       "  'authors': [\"Varun Nagaraj Rao, Samantha Dalal, Andrew Schwartz, Amna Liaqat, Dana Calacci, Andr\\\\'es Monroy-Hern\\\\'andez\"]},\n",
       " {'id': 'oai:arXiv.org:2505.09224v1',\n",
       "  'title': 'Ethical Aspects of the Use of Social Robots in Elderly Care -- A Systematic Qualitative Review',\n",
       "  'abstract': 'arXiv:2505.09224v1 Announce Type: new \\nAbstract: Background: The use of social robotics in elderly care is increasingly discussed as one way of meeting emerging care needs due to scarce resources. While many potential benefits are associated with robotic care technologies, there is a variety of ethical challenges. To support steps towards a responsible implementation and use, this review develops an overview on ethical aspects of the use of social robots in elderly care from a decision-makers\\' perspective.\\n  Methods: Electronic databases were queried using a comprehensive search strategy based on the key concepts of \"ethical aspects\", \"social robotics\" and \"elderly care\". Abstract and title screening was conducted by two authors independently. Full-text screening was conducted by one author following a joint consolidation phase. Data was extracted using MAXQDA24 by one author, based on a consolidated coding framework. Analysis was performed through modified qualitative content analysis.\\n  Results: A total of 1,518 publications were screened, and 248 publications were included. We have organized our analysis in a scheme of ethical hazards, ethical opportunities and unsettled questions, identifying at least 60 broad ethical aspects affecting three different stakeholder groups. While some ethical issues are well-known and broadly discussed our analysis shows a plethora of potentially relevant aspects, often only marginally recognized, that are worthy of consideration from a practical perspective.\\n  Discussion: The findings highlight the need for a contextual and detailed evaluation of implementation scenarios. To make use of the vast knowledge of the ethical discourse, we hypothesize that decision-makers need to understand the specific nature of this discourse to be able to engage in careful ethical deliberation.',\n",
       "  'url': 'https://arxiv.org/abs/2505.09224',\n",
       "  'authors': ['Marianne Leineweber, Clara Victoria Keusgen, Marc Bubeck, Joschka Haltaufderheide, Robert Ranisch, Corinna Klingler']},\n",
       " {'id': 'oai:arXiv.org:2505.09295v1',\n",
       "  'title': 'Toward Fair Federated Learning under Demographic Disparities and Data Imbalance',\n",
       "  'abstract': 'arXiv:2505.09295v1 Announce Type: new \\nAbstract: Ensuring fairness is critical when applying artificial intelligence to high-stakes domains such as healthcare, where predictive models trained on imbalanced and demographically skewed data risk exacerbating existing disparities. Federated learning (FL) enables privacy-preserving collaboration across institutions, but remains vulnerable to both algorithmic bias and subgroup imbalance - particularly when multiple sensitive attributes intersect. We propose FedIDA (Fed erated Learning for Imbalance and D isparity A wareness), a framework-agnostic method that combines fairness-aware regularization with group-conditional oversampling. FedIDA supports multiple sensitive attributes and heterogeneous data distributions without altering the convergence behavior of the underlying FL algorithm. We provide theoretical analysis establishing fairness improvement bounds using Lipschitz continuity and concentration inequalities, and show that FedIDA reduces the variance of fairness metrics across test sets. Empirical results on both benchmark and real-world clinical datasets confirm that FedIDA consistently improves fairness while maintaining competitive predictive performance, demonstrating its effectiveness for equitable and privacy-preserving modeling in healthcare. The source code is available on GitHub.',\n",
       "  'url': 'https://arxiv.org/abs/2505.09295',\n",
       "  'authors': ['Qiming Wu, Siqi Li, Doudou Zhou, Nan Liu']},\n",
       " {'id': 'oai:arXiv.org:2505.09576v1',\n",
       "  'title': 'Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural Rhetorical Approach',\n",
       "  'abstract': 'arXiv:2505.09576v1 Announce Type: new \\nAbstract: Since 2022, versions of generative AI chatbots such as ChatGPT and Claude have been trained using a specialized technique called Reinforcement Learning from Human Feedback (RLHF) to fine-tune language model output using feedback from human annotators. As a result, the integration of RLHF has greatly enhanced the outputs of these large language models (LLMs) and made the interactions and responses appear more \"human-like\" than those of previous versions using only supervised learning. The increasing convergence of human and machine-written text has potentially severe ethical, sociotechnical, and pedagogical implications relating to transparency, trust, bias, and interpersonal relations. To highlight these implications, this paper presents a rhetorical analysis of some of the central procedures and processes currently being reshaped by RLHF-enhanced generative AI chatbots: upholding language conventions, information seeking practices, and expectations for social relationships. Rhetorical investigations of generative AI and LLMs have, to this point, focused largely on the persuasiveness of the content generated. Using Ian Bogost\\'s concept of procedural rhetoric, this paper shifts the site of rhetorical investigation from content analysis to the underlying mechanisms of persuasion built into RLHF-enhanced LLMs. In doing so, this theoretical investigation opens a new direction for further inquiry in AI ethics that considers how procedures rerouted through AI-driven technologies might reinforce hegemonic language use, perpetuate biases, decontextualize learning, and encroach upon human relationships. It will therefore be of interest to educators, researchers, scholars, and the growing number of users of generative AI chatbots.',\n",
       "  'url': 'https://arxiv.org/abs/2505.09576',\n",
       "  'authors': ['Shannon Lodoen, Alexi Orchard']},\n",
       " {'id': 'oai:arXiv.org:2505.09598v1',\n",
       "  'title': 'How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference',\n",
       "  'abstract': \"arXiv:2505.09598v1 Announce Type: new \\nAbstract: As large language models (LLMs) spread across industries, understanding their environmental footprint at the inference level is no longer optional; it is essential. However, most existing studies exclude proprietary models, overlook infrastructural variability and overhead, or focus solely on training, even as inference increasingly dominates AI's environmental impact. To bridge this gap, this paper introduces a novel infrastructure-aware benchmarking framework for quantifying the environmental footprint of LLM inference across 30 state-of-the-art models as deployed in commercial data centers. Our framework combines public API performance data with region-specific environmental multipliers and statistical inference of hardware configurations. We additionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank models by performance relative to environmental cost. Our results show that o3 and DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33 Wh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and that Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short GPT-4o query consumes 0.43 Wh, scaling this to 700 million queries/day results in substantial annual environmental impacts. These include electricity use comparable to 35,000 U.S. homes, freshwater evaporation matching the annual drinking needs of 1.2 million people, and carbon emissions requiring a Chicago-sized forest to offset. These findings illustrate a growing paradox: although individual queries are efficient, their global scale drives disproportionate resource consumption. Our study provides a standardized, empirically grounded methodology for benchmarking the sustainability of LLM deployments, laying a foundation for future environmental accountability in AI development and sustainability standards.\",\n",
       "  'url': 'https://arxiv.org/abs/2505.09598',\n",
       "  'authors': ['Nidhal Jegham, Marwen Abdelatti, Lassad Elmoubarki, Abdeltawab Hendawi']},\n",
       " {'id': 'oai:arXiv.org:2505.08792v1',\n",
       "  'title': 'A Preliminary Framework for Intersectionality in ML Pipelines',\n",
       "  'abstract': \"arXiv:2505.08792v1 Announce Type: cross \\nAbstract: Machine learning (ML) has become a go-to solution for improving how we use, experience, and interact with technology (and the world around us). Unfortunately, studies have repeatedly shown that machine learning technologies may not provide adequate support for societal identities and experiences. Intersectionality is a sociological framework that provides a mechanism for explicitly considering complex social identities, focusing on social justice and power. While the framework of intersectionality can support the development of technologies that acknowledge and support all members of society, it has been adopted and adapted in ways that are not always true to its foundations, thereby weakening its potential for impact. To support the appropriate adoption and use of intersectionality for more equitable technological outcomes, we amplify the foundational intersectionality scholarship--Crenshaw, Combahee, and Collins (three C's), to create a socially relevant preliminary framework in developing machine-learning solutions. We use this framework to evaluate and report on the (mis)alignments of intersectionality application in machine learning literature.\",\n",
       "  'url': 'https://arxiv.org/abs/2505.08792',\n",
       "  'authors': ['Michelle Nashla Turcios, Alicia E. Boyd, Angela D. R. Smith, Brittany Johnson']},\n",
       " {'id': 'oai:arXiv.org:2505.08797v1',\n",
       "  'title': 'Visibility and Influence in Digital Social Relations: Towards a New Symbolic Capital?',\n",
       "  'abstract': 'arXiv:2505.08797v1 Announce Type: cross \\nAbstract: This study explores the dynamics of visibility and influence in digital social relations, examining their implications for the emergence of a new symbolic capital. Using a mixedmethods design, the research combined semi-structured interviews with 20 digitally active individuals and quantitative social media data analysis to identify key predictors of digital symbolic capital. Findings reveal that visibility is influenced by content quality, network size, and engagement strategies, while influence depends on credibility, authority, and trust. The study identifies a new form of symbolic capital based on online visibility, influence, and reputation, distinct from traditional forms. The research discusses the ethical implications of these dynamics and suggests future research directions, emphasizing the need to update social theories to account for digital transformations.',\n",
       "  'url': 'https://arxiv.org/abs/2505.08797',\n",
       "  'authors': ['F. Annaki, S. Ouassou, S. Igamane']},\n",
       " {'id': 'oai:arXiv.org:2505.08828v1',\n",
       "  'title': 'Human-AI Collaboration or Academic Misconduct? Measuring AI Use in Student Writing Through Stylometric Evidence',\n",
       "  'abstract': \"arXiv:2505.08828v1 Announce Type: cross \\nAbstract: As human-AI collaboration becomes increasingly prevalent in educational contexts, understanding and measuring the extent and nature of such interactions pose significant challenges. This research investigates the use of authorship verification (AV) techniques not as a punitive measure, but as a means to quantify AI assistance in academic writing, with a focus on promoting transparency, interpretability, and student development. Building on prior work, we structured our investigation into three stages: dataset selection and expansion, AV method development, and systematic evaluation. Using three datasets - including a public dataset (PAN-14) and two from University of Melbourne students from various courses - we expanded the data to include LLM-generated texts, totalling 1,889 documents and 540 authorship problems from 506 students. We developed an adapted Feature Vector Difference AV methodology to construct robust academic writing profiles for students, designed to capture meaningful, individual characteristics of their writing. The method's effectiveness was evaluated across multiple scenarios, including distinguishing between student-authored and LLM-generated texts and testing resilience against LLMs' attempts to mimic student writing styles. Results demonstrate the enhanced AV classifier's ability to identify stylometric discrepancies and measure human-AI collaboration at word and sentence levels while providing educators with a transparent tool to support academic integrity investigations. This work advances AV technology, offering actionable insights into the dynamics of academic writing in an AI-driven era.\",\n",
       "  'url': 'https://arxiv.org/abs/2505.08828',\n",
       "  'authors': [\"Eduardo Araujo Oliveira, Madhavi Mohoni, Sonsoles L\\\\'opez-Pernas, Mohammed Saqr\"]},\n",
       " {'id': 'oai:arXiv.org:2505.08894v1',\n",
       "  'title': 'WaLLM -- Insights from an LLM-Powered Chatbot deployment via WhatsApp',\n",
       "  'abstract': 'arXiv:2505.08894v1 Announce Type: cross \\nAbstract: Recent advances in generative AI, such as ChatGPT, have transformed access to information in education, knowledge-seeking, and everyday decision-making. However, in many developing regions, access remains a challenge due to the persistent digital divide. To help bridge this gap, we developed WaLLM - a custom AI chatbot over WhatsApp, a widely used communication platform in developing regions. Beyond answering queries, WaLLM offers several features to enhance user engagement: a daily top question, suggested follow-up questions, trending and recent queries, and a leaderboard-based reward system. Our service has been operational for over 6 months, amassing over 14.7K queries from approximately 100 users. In this paper, we present WaLLM\\'s design and a systematic analysis of logs to understand user interactions. Our results show that 55% of user queries seek factual information. \"Health and well-being\" was the most popular topic (28%), including queries about nutrition and disease, suggesting users view WaLLM as a reliable source. Two-thirds of users\\' activity occurred within 24 hours of the daily top question. Users who accessed the \"Leaderboard\" interacted with WaLLM 3x as those who did not. We conclude by discussing implications for culture-based customization, user interface design, and appropriate calibration of users\\' trust in AI systems for developing regions.',\n",
       "  'url': 'https://arxiv.org/abs/2505.08894',\n",
       "  'authors': ['Hiba Eltigani, Rukhshan Haroon, Asli Kocak, Abdullah Bin Faisal, Noah Martin, Fahad Dogar']},\n",
       " {'id': 'oai:arXiv.org:2505.09054v1',\n",
       "  'title': 'EcoSphere: A Decision-Support Tool for Automated Carbon Emission and Cost Optimization in Sustainable Urban Development',\n",
       "  'abstract': 'arXiv:2505.09054v1 Announce Type: cross \\nAbstract: The construction industry is a major contributor to global greenhouse gas emissions, with embodied carbon being a key component. This study develops EcoSphere, an innovative software designed to evaluate and balance embodied and operational carbon emissions with construction and environmental costs in urban planning. Using high-resolution data from the National Structure Inventory, combined with computer vision and natural language processing applied to Google Street View and satellite imagery, EcoSphere categorizes buildings by structural and material characteristics with a bottom-up approach, creating a baseline emissions dataset. By simulating policy scenarios and mitigation strategies, EcoSphere provides policymakers and non-experts with actionable insights for sustainable development in cities and provide them with a vision of the environmental and financial results of their decisions. Case studies in Chicago and Indianapolis showcase how EcoSphere aids in assessing policy impacts on carbon emissions and costs, supporting data-driven progress toward carbon neutrality.',\n",
       "  'url': 'https://arxiv.org/abs/2505.09054',\n",
       "  'authors': ['Siavash Ghorbany, Ming Hu, Siyuan Yao, Matthew Sisk, Chaoli Wang']},\n",
       " {'id': 'oai:arXiv.org:2505.09116v1',\n",
       "  'title': \"A Method for Assisting Novices Creating Class Diagrams Based on the Instructor's Class Layout\",\n",
       "  'abstract': \"arXiv:2505.09116v1 Announce Type: cross \\nAbstract: Nowadays, modeling exercises on software development objects are conducted in higher education institutions for information technology. Not only are there many defects such as missing elements in the models created by learners during the exercises, but the layout of elements in the class diagrams often differs significantly from the correct answers created by the instructors. In this paper, we focus on the above problem and propose a method to provide effective support to learners during modeling exercises by automatically converting the layout of the learner's class diagram to that of the instructor, in addition to indicating the correctness of the artifacts to the learners during the exercises. The proposed method was implemented and evaluated as a tool, and the results indicate that the automatic layout conversion was an effective feedback to the learners.\",\n",
       "  'url': 'https://arxiv.org/abs/2505.09116',\n",
       "  'authors': ['Yuta Saito, Takehiro Kokubu, Takafumi Tanaka, Atsuo Hazeyama, Hiroaki Hashiura']},\n",
       " {'id': 'oai:arXiv.org:2505.09287v1',\n",
       "  'title': 'Ranking-Based At-Risk Student Prediction Using Federated Learning and Differential Features',\n",
       "  'abstract': 'arXiv:2505.09287v1 Announce Type: cross \\nAbstract: Digital textbooks are widely used in various educational contexts, such as university courses and online lectures. Such textbooks yield learning log data that have been used in numerous educational data mining (EDM) studies for student behavior analysis and performance prediction. However, these studies have faced challenges in integrating confidential data, such as academic records and learning logs, across schools due to privacy concerns. Consequently, analyses are often conducted with data limited to a single school, which makes developing high-performing and generalizable models difficult. This study proposes a method that combines federated learning and differential features to address these issues. Federated learning enables model training without centralizing data, thereby preserving student privacy. Differential features, which utilize relative values instead of absolute values, enhance model performance and generalizability. To evaluate the proposed method, a model for predicting at-risk students was trained using data from 1,136 students across 12 courses conducted over 4 years, and validated on hold-out test data from 5 other courses. Experimental results demonstrated that the proposed method addresses privacy concerns while achieving performance comparable to that of models trained via centralized learning in terms of Top-n precision, nDCG, and PR-AUC. Furthermore, using differential features improved prediction performance across all evaluation datasets compared to non-differential approaches. The trained models were also applicable for early prediction, achieving high performance in detecting at-risk students in earlier stages of the semester within the validation datasets.',\n",
       "  'url': 'https://arxiv.org/abs/2505.09287',\n",
       "  'authors': [\"Shunsuke Yoneda, Valdemar \\\\v{S}v\\\\'abensk\\\\'y, Gen Li, Daisuke Deguchi, Atsushi Shimada\"]},\n",
       " {'id': 'oai:arXiv.org:2505.09595v1',\n",
       "  'title': 'WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models',\n",
       "  'abstract': 'arXiv:2505.09595v1 Announce Type: cross \\nAbstract: Large Language Models (LLMs) are predominantly trained and aligned in ways that reinforce Western-centric epistemologies and socio-cultural norms, leading to cultural homogenization and limiting their ability to reflect global civilizational plurality. Existing benchmarking frameworks fail to adequately capture this bias, as they rely on rigid, closed-form assessments that overlook the complexity of cultural inclusivity. To address this, we introduce WorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity (GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our approach is grounded in the Multiplex Worldview proposed by Senturk et al., which distinguishes between Uniplex models, reinforcing cultural homogenization, and Multiplex models, which integrate diverse perspectives. WorldView-Bench measures Cultural Polarization, the exclusion of alternative perspectives, through free-form generative evaluation rather than conventional categorical benchmarks. We implement applied multiplexity through two intervention strategies: (1) Contextually-Implemented Multiplex LLMs, where system prompts embed multiplexity principles, and (2) Multi-Agent System (MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing distinct cultural perspectives collaboratively generate responses. Our results demonstrate a significant increase in Perspectives Distribution Score (PDS) entropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs, alongside a shift toward positive sentiment (67.7%) and enhanced cultural balance. These findings highlight the potential of multiplex-aware AI evaluation in mitigating cultural bias in LLMs, paving the way for more inclusive and ethically aligned AI systems.',\n",
       "  'url': 'https://arxiv.org/abs/2505.09595',\n",
       "  'authors': ['Abdullah Mushtaq, Imran Taj, Rafay Naeem, Ibrahim Ghaznavi, Junaid Qadir']},\n",
       " {'id': 'oai:arXiv.org:2505.09605v1',\n",
       "  'title': 'The Niche Connectivity Paradox: Multichrome Contagions Overcome Vaccine Hesitancy more effectively than Monochromacy',\n",
       "  'abstract': 'arXiv:2505.09605v1 Announce Type: cross \\nAbstract: The rise of vaccine hesitancy has caused a resurgence of vaccine-preventable diseases such as measles and pertussis, alongside widespread skepticism and refusals of COVID-19 vaccinations. While categorizing individuals as either supportive of or opposed to vaccines provides a convenient dichotomy of vaccine attitudes, vaccine hesitancy is far more complex and dynamic. It involves wavering individuals whose attitudes fluctuate -- those who may exhibit pro-vaccine attitudes at one time and anti-vaccine attitudes at another. Here, we identify and analyze multichrome contagions as potential targets for intervention by leveraging a dataset of known pro-vax and anti-vax Twitter users ($n =135$ million) and a large COVID-19 Twitter dataset ($n = 3.5$ billion; including close analysis of $1,563,472$ unique individuals). We reconstruct an evolving multiplex sentiment landscape using top co-spreading issues, characterizing them as monochrome and multichrome contagions, based on their conceptual overlap with vaccination. We demonstrate switchers as deliberative: they are more moderate, engage with a wider range of topics, and occupy more central positions in their networks. Further examination of their information consumption shows that their discourse often engages with progressive issues such as climate change, which can serve as avenues for multichrome contagion interventions to promote pro-vaccine attitudes. Using data-driven intervention simulations, we demonstrate a paradox of niche connectivity, where multichrome contagions with fragmented, non-overlapping communities generate the highest levels of diffusion for pro-vaccine attitudes. Our work offers insights into harnessing synergistic hitchhiking effect of multichrome contagions to drive desired attitude and behavior changes in network-based interventions, particularly for overcoming vaccine hesitancy.',\n",
       "  'url': 'https://arxiv.org/abs/2505.09605',\n",
       "  'authors': ['Ho-Chun Herbert Chang, Feng Fu']},\n",
       " {'id': 'oai:arXiv.org:2008.03665v2',\n",
       "  'title': 'Evacuation decisions in response to natural disasters: Insights from a large-scale social media survey',\n",
       "  'abstract': 'arXiv:2008.03665v2 Announce Type: replace \\nAbstract: Evacuation in response to natural disasters is a complex process involving multiple decision-makers at the personal, household, community, and government levels. Consequently, many disparate factors influence who evacuates, when, and how to respond to a nearby disaster. In this paper, we leverage a novel method of data collection through social media to explore the evacuation response decisions of people in areas affected by the 2019-2020 Australian bushfires. We explore the validity of this data collection method for generating plausible estimates of evacuation and its ability to supplement cell phone location data using survey responses. Ultimately, we identify several key factors influencing household decisions on evacuation, specifically focusing on the phenomenon of household members evacuating or returning from evacuation at different times.',\n",
       "  'url': 'https://arxiv.org/abs/2008.03665',\n",
       "  'authors': ['Paige Maas, Zack Almquist, Eugenia Giraudy, JW Schneider']},\n",
       " {'id': 'oai:arXiv.org:2311.00810v3',\n",
       "  'title': 'A Call to Arms: AI Should be Critical for Social Media Analysis of Conflict Zones',\n",
       "  'abstract': 'arXiv:2311.00810v3 Announce Type: replace \\nAbstract: The massive proliferation of social media data represents a transformative opportunity for conflict studies and for tracking the proliferation and use of weaponry, as conflicts are increasingly documented in these online spaces. At the same time, the scale and types of data available are problematic for traditional open-source intelligence. This paper focuses on identifying specific weapon systems and the insignias of the armed groups using them as documented in the Ukraine war, as these tasks are critical to operational intelligence and tracking weapon proliferation, especially given the scale of international military aid given to Ukraine. The large scale of social media makes manual assessment difficult, however, so this paper presents early work that uses computer vision models to support this task. We demonstrate that these models can both identify weapons embedded in images shared in social media and how the resulting collection of military-relevant images and their post times interact with the offline, real-world conflict. Not only can we then track changes in the prevalence of images of tanks, land mines, military trucks, etc., we find correlations among time series data associated with these images and the daily fatalities in this conflict. This work shows substantial opportunity for examining similar online documentation of conflict contexts, and we also point to future avenues where computer vision can be further improved for these open-source intelligence tasks.',\n",
       "  'url': 'https://arxiv.org/abs/2311.00810',\n",
       "  'authors': ['Afia Abedin, Abdul Bais, Cody Buntain, Laura Courchesne, Brian McQuinn, Matthew E. Taylor, Muhib Ullah']},\n",
       " {'id': 'oai:arXiv.org:2406.16696v2',\n",
       "  'title': 'Public Constitutional AI',\n",
       "  'abstract': 'arXiv:2406.16696v2 Announce Type: replace \\nAbstract: We are increasingly subjected to the power of AI authorities. As AI decisions become inescapable, entering domains such as healthcare, education, and law, we must confront a vital question: how can we ensure AI systems have the legitimacy necessary for effective governance? This essay argues that to secure AI legitimacy, we need methods that engage the public in designing and constraining AI systems, ensuring these technologies reflect the community\\'s shared values. Constitutional AI, proposed by Anthropic, represents a step towards this goal, offering a model for democratic control of AI. However, while Constitutional AI\\'s commitment to hardcoding explicit principles into AI models enhances transparency and accountability, it falls short in two crucial aspects: addressing the opacity of individual AI decisions and fostering genuine democratic legitimacy. To overcome these limitations, this essay proposes \"Public Constitutional AI.\" This approach envisions a participatory process where diverse stakeholders, including ordinary citizens, deliberate on the principles guiding AI development. The resulting \"AI Constitution\" would carry the legitimacy of popular authorship, grounding AI governance in the public will. Furthermore, the essay proposes \"AI Courts\" to develop \"AI case law,\" providing concrete examples for operationalizing constitutional principles in AI training. This evolving combination of constitutional principles and case law aims to make AI governance more responsive to public values. By grounding AI governance in deliberative democratic processes, Public Constitutional AI offers a path to imbue automated authorities with genuine democratic legitimacy, addressing the unique challenges posed by increasingly powerful AI systems while ensuring their alignment with the public interest.',\n",
       "  'url': 'https://arxiv.org/abs/2406.16696',\n",
       "  'authors': ['Gilad Abiri']},\n",
       " {'id': 'oai:arXiv.org:2410.08007v3',\n",
       "  'title': 'Time Can Invalidate Algorithmic Recourse',\n",
       "  'abstract': 'arXiv:2410.08007v3 Announce Type: replace-cross \\nAbstract: Algorithmic Recourse (AR) aims to provide users with actionable steps to overturn unfavourable decisions made by machine learning predictors. However, these actions often take time to implement (e.g., getting a degree can take years), and their effects may vary as the world evolves. Thus, it is natural to ask for recourse that remains valid in a dynamic environment. In this paper, we study the robustness of algorithmic recourse over time by casting the problem through the lens of causality. We demonstrate theoretically and empirically that (even robust) causal AR methods can fail over time, except in the -- unlikely -- case that the world is stationary. Even more critically, unless the world is fully deterministic, counterfactual AR cannot be solved optimally. To account for this, we propose a simple yet effective algorithm for temporal AR that explicitly accounts for time under the assumption of having access to an estimator approximating the stochastic process. Our simulations on synthetic and realistic datasets show how considering time produces more resilient solutions to potential trends in the data distribution.',\n",
       "  'url': 'https://arxiv.org/abs/2410.08007',\n",
       "  'authors': ['Giovanni De Toni, Stefano Teso, Bruno Lepri, Andrea Passerini']},\n",
       " {'id': 'oai:arXiv.org:2502.11248v2',\n",
       "  'title': 'Synthetic Politics: Prevalence, Spreaders, and Emotional Reception of AI-Generated Political Images on X',\n",
       "  'abstract': \"arXiv:2502.11248v2 Announce Type: replace-cross \\nAbstract: Despite widespread concerns about the risks of AI-generated content (AIGC) to the integrity of social media discourse, little is known about its scale and scope, the actors responsible for its dissemination online, and the user responses it elicits. In this work, we measure and characterize the prevalence, spreaders, and emotional reception of AI-generated political images. Analyzing a large-scale dataset from Twitter/X related to the 2024 U.S. Presidential Election, we find that approximately 12% of shared images are detected as AI-generated, and around 10% of users are responsible for sharing 80% of AI-generated images. AIGC superspreaders--defined as the users who not only share a high volume of AI-generated images but also receive substantial engagement through retweets--are more likely to be X Premium subscribers, have a right-leaning orientation, and exhibit automated behavior. Their profiles contain a higher proportion of AI-generated images than non-superspreaders, and some engage in extreme levels of AIGC sharing. Moreover, superspreaders' AI image tweets elicit more positive and less toxic responses than their non-AI image tweets. This study serves as one of the first steps toward understanding the role generative AI plays in shaping online socio-political environments and offers implications for platform governance.\",\n",
       "  'url': 'https://arxiv.org/abs/2502.11248',\n",
       "  'authors': ['Zhiyi Chen, Jinyi Ye, Beverlyn Tsai, Emilio Ferrara, Luca Luceri']},\n",
       " {'id': 'oai:arXiv.org:2503.10652v2',\n",
       "  'title': 'Simulating and Analysing Human Survey Responses with Large Language Models: A Case Study in Energy Stated Preference',\n",
       "  'abstract': 'arXiv:2503.10652v2 Announce Type: replace-cross \\nAbstract: Survey research plays a crucial role in studies by capturing consumer preferences and informing policy decisions. Stated preference (SP) surveys help researchers understand how individuals make trade-offs in hypothetical, potentially futuristic, scenarios. However, traditional methods are costly, time-consuming, and affected by respondent fatigue and ethical constraints. Large language models (LLMs) have shown remarkable capabilities in generating human-like responses, prompting interest in their use in survey research. This study investigates LLMs for simulating consumer choices in energy-related SP surveys and explores their integration into data collection and analysis workflows. Test scenarios were designed to assess the simulation performance of several LLMs (LLaMA 3.1, Mistral, GPT-3.5, DeepSeek-R1) at individual and aggregated levels, considering prompt design, in-context learning (ICL), chain-of-thought (CoT) reasoning, model types, integration with traditional choice models, and potential biases. While LLMs achieve accuracy above random guessing, performance remains insufficient for practical simulation use. Cloud-based LLMs do not consistently outperform smaller local models. DeepSeek-R1 achieves the highest average accuracy (77%) and outperforms non-reasoning LLMs in accuracy, factor identification, and choice distribution alignment. Previous SP choices are the most effective input; longer prompts with more factors reduce accuracy. Mixed logit models can support LLM prompt refinement. Reasoning LLMs show potential in data analysis by indicating factor significance, offering a qualitative complement to statistical models. Despite limitations, pre-trained LLMs offer scalability and require minimal historical data. Future work should refine prompts, further explore CoT reasoning, and investigate fine-tuning techniques.',\n",
       "  'url': 'https://arxiv.org/abs/2503.10652',\n",
       "  'authors': ['Han Wang, Jacek Pawlak, Aruna Sivakumar']},\n",
       " {'id': 'oai:arXiv.org:2503.16623v4',\n",
       "  'title': 'ICLR Points: How Many ICLR Publications Is One Paper in Each Area?',\n",
       "  'abstract': \"arXiv:2503.16623v4 Announce Type: replace-cross \\nAbstract: Scientific publications significantly impact academic-related decisions in computer science, where top-tier conferences are particularly influential. However, efforts required to produce a publication differ drastically across various subfields. While existing citation-based studies compare venues within areas, cross-area comparisons remain challenging due to differing publication volumes and citation practices.\\n  To address this gap, we introduce the concept of ICLR points, defined as the average effort required to produce one publication at top-tier machine learning conferences such as ICLR, ICML, and NeurIPS. Leveraging comprehensive publication data from DBLP (2019--2023) and faculty information from CSRankings, we quantitatively measure and compare the average publication effort across 27 computer science sub-areas. Our analysis reveals significant differences in average publication effort, validating anecdotal perceptions: systems conferences generally require more effort per publication than AI conferences.\\n  We further demonstrate the utility of the ICLR points metric by evaluating publication records of universities, current faculties and recent faculty candidates. Our findings highlight how using this metric enables more meaningful cross-area comparisons in academic evaluation processes. Lastly, we discuss the metric's limitations and caution against its misuse, emphasizing the necessity of holistic assessment criteria beyond publication metrics alone.\",\n",
       "  'url': 'https://arxiv.org/abs/2503.16623',\n",
       "  'authors': ['Zhongtang Luo']}]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_list_df.to_dict(orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
