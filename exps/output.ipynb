{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d36033-6ea0-49c4-82a9-f3758128e017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f08210-4cf7-4475-a868-517ae6c91817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_parentheses_content(s):\n",
    "    return re.sub(r\"\\([^)]*\\)\", \"\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2ec5995-a5b7-4c77-b66a-7ad81c39d603",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.datetime.now().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a2e05ab-5bfa-452d-bbdb-651f8ed51102",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data_dir = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27c9b25f-3346-452c-847a-0d1cb768d3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_file = os.path.join(temp_data_dir, f\"{date}.json.gz\")\n",
    "with gzip.open(paper_file, \"rb\") as f:\n",
    "    paper_dict = json.loads(f.read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c4067ad-f2f5-4bdf-8c08-2cc9dbeb5deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'http://arxiv.org/abs/2312.06707': {'id': 'http://arxiv.org/abs/2312.06707',\n",
       "  'title': \"Exploring Public's Perception of Safety and Video Surveillance Technology: A Survey Approach. (arXiv:2312.06707v1 [cs.CY])\",\n",
       "  'abstract': \"Addressing public safety effectively requires incorporating diverse\\nstakeholder perspectives, particularly those of the community, which are often\\nunderrepresented compared to other stakeholders. This study presents a\\ncomprehensive analysis of the community's general public safety concerns, their\\nview of existing surveillance technologies, and their perception of AI-driven\\nsolutions for enhancing safety in urban environments, focusing on Charlotte,\\nNC. Through a survey approach, including in-person surveys conducted in August\\nand September 2023 with 410 participants, this research investigates\\ndemographic factors such as age, gender, ethnicity, and educational level to\\ngain insights into public perception and concerns toward public safety and\\npossible solutions. Based on the type of dependent variables, we utilized\\ndifferent statistical and significance analyses, such as logit regression and\\nordinal logistic regression, to explore the effects of demographic factors on\\nthe various dependent variables. Our results reveal demographic differences in\\npublic safety concerns. Younger females tend to feel less secure yet trust\\nexisting video surveillance systems, whereas older, educated individuals are\\nmore concerned about violent crimes in malls. Additionally, attitudes towards\\nAI-driven surveillance differ: older Black individuals demonstrate support for\\nit despite having concerns about data privacy, while educated females show a\\ntendency towards skepticism.\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2312.06707',\n",
       "  'authors': ['Babak Rahimi Ardabili, Armin Danesh Pazho, Ghazal Alinezhad Noghre, Vinit Katariya, Gordon Hull, Shannon Reid, Hamed Tabkhi'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': 'The paper does not directly address security of AI and language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.9,\n",
       "    'reason': 'The paper directly addresses the application of AI and surveillance technology in social science research, particularly in the context of public safety concerns.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.06833': {'id': 'http://arxiv.org/abs/2312.06833',\n",
       "  'title': 'The unreasonable effectiveness of AI CADe polyp detectors to generalize to new countries. (arXiv:2312.06833v1 [cs.LG])',\n",
       "  'abstract': '$\\\\textbf{Background and aims}$: Artificial Intelligence (AI) Computer-Aided\\nDetection (CADe) is commonly used for polyp detection, but data seen in\\nclinical settings can differ from model training. Few studies evaluate how well\\nCADe detectors perform on colonoscopies from countries not seen during\\ntraining, and none are able to evaluate performance without collecting\\nexpensive and time-intensive labels.\\n\\n$\\\\textbf{Methods}$: We trained a CADe polyp detector on Israeli colonoscopy\\nvideos (5004 videos, 1106 hours) and evaluated on Japanese videos (354 videos,\\n128 hours) by measuring the True Positive Rate (TPR) versus false alarms per\\nminute (FAPM). We introduce a colonoscopy dissimilarity measure called \"MAsked\\nmediCal Embedding Distance\" (MACE) to quantify differences between\\ncolonoscopies, without labels. We evaluated CADe on all Japan videos and on\\nthose with the highest MACE.\\n\\n$\\\\textbf{Results}$: MACE correctly quantifies that narrow-band imaging (NBI)\\nand chromoendoscopy (CE) frames are less similar to Israel data than Japan\\nwhitelight (bootstrapped z-test, |z| > 690, p < $10^{-8}$ for both). Despite\\ndifferences in the data, CADe performance on Japan colonoscopies was\\nnon-inferior to Israel ones without additional training (TPR at 0.5 FAPM: 0.957\\nand 0.972 for Israel and Japan; TPR at 1.0 FAPM: 0.972 and 0.989 for Israel and\\nJapan; superiority test t > 45.2, p < $10^{-8}$). Despite not being trained on\\nNBI or CE, TPR on those subsets were non-inferior to Japan overall\\n(non-inferiority test t > 47.3, p < $10^{-8}$, $\\\\delta$ = 1.5% for both).\\n\\n$\\\\textbf{Conclusion}$: Differences that prevent CADe detectors from\\nperforming well in non-medical settings do not degrade the performance of our\\nAI CADe polyp detector when applied to data from a new country. MACE can help\\nmedical AI models internationalize by identifying the most \"dissimilar\" data on\\nwhich to evaluate models.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.06833',\n",
       "  'authors': ['Joel Shor, Hiro-o Yamano, Daisuke Tsurumaru, Yotami Intrator, Hiroki Kayama, Joe Ledsam, Atsushi Hamabe, Koji Ando, Mitsuhiko Ota, Haruei Ogino, Hiroshi Nakase, Kaho Kobayashi, Eiji Oki, Roman Goldenberg, Ehud Rivlin, Ichiro Takemasa'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.9,\n",
       "    'reason': 'The paper does not directly address this topic.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.06841': {'id': 'http://arxiv.org/abs/2312.06841',\n",
       "  'title': 'memorAIs: an Optical Character Recognition and Rule-Based Medication Intake Reminder-Generating Solution. (arXiv:2312.06841v1 [cs.IR])',\n",
       "  'abstract': 'Memory-based medication non-adherence is an unsolved problem that is\\nresponsible for considerable disease burden in the United States. Digital\\nmedication intake reminder solutions with minimal onboarding requirements that\\nare usable at the point of medication acquisition may help to alleviate this\\nproblem by offering a low barrier way to help people remember to take their\\nmedications. In this paper, we propose memorAIs, a digital medication intake\\nreminder solution that mitigates onboarding friction by leveraging optical\\ncharacter recognition strategies for text extraction from medication bottles\\nand rule based expressions for text processing to create configured medication\\nreminders as local device calendar invitations. We describe our ideation and\\ndevelopment process, as well as limitations of the current implementation.\\nmemorAIs was the winner of the Patient Safety award at the 2023 Columbia\\nUniversity DivHacks Hackathon, presented by the Patient Safety Technology\\nChallenge, sponsored by the Pittsburgh Regional Health Initiative.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.06841',\n",
       "  'authors': ['Eden Shaveet, Utkarsh Singh, Nicholas Assaderaghi, Maximo Librandi'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.06861': {'id': 'http://arxiv.org/abs/2312.06861',\n",
       "  'title': 'Disentangling Perceptions of Offensiveness: Cultural and Moral Correlates. (arXiv:2312.06861v1 [cs.CY])',\n",
       "  'abstract': \"Perception of offensiveness is inherently subjective, shaped by the lived\\nexperiences and socio-cultural values of the perceivers. Recent years have seen\\nsubstantial efforts to build AI-based tools that can detect offensive language\\nat scale, as a means to moderate social media platforms, and to ensure safety\\nof conversational AI technologies such as ChatGPT and Bard. However, existing\\napproaches treat this task as a technical endeavor, built on top of data\\nannotated for offensiveness by a global crowd workforce without any attention\\nto the crowd workers' provenance or the values their perceptions reflect. We\\nargue that cultural and psychological factors play a vital role in the\\ncognitive processing of offensiveness, which is critical to consider in this\\ncontext. We re-frame the task of determining offensiveness as essentially a\\nmatter of moral judgment -- deciding the boundaries of ethically wrong vs.\\nright language within an implied set of socio-cultural norms. Through a\\nlarge-scale cross-cultural study based on 4309 participants from 21 countries\\nacross 8 cultural regions, we demonstrate substantial cross-cultural\\ndifferences in perceptions of offensiveness. More importantly, we find that\\nindividual moral values play a crucial role in shaping these variations: moral\\nconcerns about Care and Purity are significant mediating factors driving\\ncross-cultural differences. These insights are of crucial importance as we\\nbuild AI models for the pluralistic world, where the values they espouse should\\naim to respect and account for moral values in diverse geo-cultural contexts.\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2312.06861',\n",
       "  'authors': ['Aida Davani, Mark Díaz, Dylan Baker, Vinodkumar Prabhakaran'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': 'The paper does not directly discuss the security of AI and language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.9,\n",
       "    'reason': 'The paper directly discusses the application of AI-based tools in moderating social media platforms and ensuring safety of conversational AI technologies, which aligns with the topic of applications of AI and language models in social science research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.06941': {'id': 'http://arxiv.org/abs/2312.06941',\n",
       "  'title': 'Humans vs Large Language Models: Judgmental Forecasting in an Era of Advanced AI. (arXiv:2312.06941v1 [cs.LG])',\n",
       "  'abstract': 'This study investigates the forecasting accuracy of human experts versus\\nLarge Language Models (LLMs) in the retail sector, particularly during standard\\nand promotional sales periods. Utilizing a controlled experimental setup with\\n123 human forecasters and five LLMs, including ChatGPT4, ChatGPT3.5, Bard,\\nBing, and Llama2, we evaluated forecasting precision through Mean Absolute\\nPercentage Error. Our analysis centered on the effect of the following factors\\non forecasters performance: the supporting statistical model (baseline and\\nadvanced), whether the product was on promotion, and the nature of external\\nimpact. The findings indicate that LLMs do not consistently outperform humans\\nin forecasting accuracy and that advanced statistical forecasting models do not\\nuniformly enhance the performance of either human forecasters or LLMs. Both\\nhuman and LLM forecasters exhibited increased forecasting errors, particularly\\nduring promotional periods and under the influence of positive external\\nimpacts. Our findings call for careful consideration when integrating LLMs into\\npractical forecasting processes.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.06941',\n",
       "  'authors': ['MAhdi Abolghasemi, Odkhishig Ganbold, Kristian Rotaru'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.8,\n",
       "    'reason': 'The paper investigates the forecasting accuracy of human experts versus Large Language Models (LLMs) in the retail sector, which involves social science research related to forecasting and decision-making.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.07420': {'id': 'http://arxiv.org/abs/2312.07420',\n",
       "  'title': 'FairSISA: Ensemble Post-Processing to Improve Fairness of Unlearning in LLMs. (arXiv:2312.07420v1 [cs.LG])',\n",
       "  'abstract': 'Training large language models (LLMs) is a costly endeavour in terms of time\\nand computational resources. The large amount of training data used during the\\nunsupervised pre-training phase makes it difficult to verify all data and,\\nunfortunately, undesirable data may be ingested during training. Re-training\\nfrom scratch is impractical and has led to the creation of the \\'unlearning\\'\\ndiscipline where models are modified to \"unlearn\" undesirable information\\nwithout retraining. However, any modification can alter the behaviour of LLMs,\\nespecially on key dimensions such as fairness. This is the first work that\\nexamines this interplay between unlearning and fairness for LLMs. In\\nparticular, we focus on a popular unlearning framework known as SISA [Bourtoule\\net al., 2021], which creates an ensemble of models trained on disjoint shards.\\nWe evaluate the performance-fairness trade-off for SISA, and empirically\\ndemsontrate that SISA can indeed reduce fairness in LLMs. To remedy this, we\\npropose post-processing bias mitigation techniques for ensemble models produced\\nby SISA. We adapt the post-processing fairness improvement technique from\\n[Hardt et al., 2016] to design three methods that can handle model ensembles,\\nand prove that one of the methods is an optimal fair predictor for ensemble of\\nmodels. Through experimental results, we demonstrate the efficacy of our\\npost-processing framework called \\'FairSISA\\'.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.07420',\n",
       "  'authors': ['Swanand Ravindra Kadhe, Anisa Halimi, Ambrish Rawat, Nathalie Baracaldo'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0.7,\n",
       "    'reason': \"The paper discusses modifying language models to 'unlearn' undesirable information, which could be relevant to security concerns related to AI and language models.\"},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.3,\n",
       "    'reason': 'The paper does not directly discuss methods to increase the factuality of language model responses.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.4,\n",
       "    'reason': 'The paper indirectly relates to the topic as it addresses post-processing bias mitigation techniques for language model ensembles, which could potentially relate to fact-checking and misinformation.'}}},\n",
       " 'http://arxiv.org/abs/2312.07492': {'id': 'http://arxiv.org/abs/2312.07492',\n",
       "  'title': 'SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models. (arXiv:2312.07492v1 [cs.CL])',\n",
       "  'abstract': 'Current datasets for unwanted social bias auditing are limited to studying\\nprotected demographic features such as race and gender. In this work, we\\nintroduce a comprehensive benchmark that is meant to capture the amplification\\nof social bias, via stigmas, in generative language models. We start with a\\ncomprehensive list of 93 stigmas documented in social science literature and\\ncurate a question-answering (QA) dataset which involves simple social\\nsituations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts, with a\\nvariety of prompt styles, carefully constructed to systematically test for both\\nsocial bias and model robustness. We present results for SocialStigmaQA with\\ntwo widely used open source generative language models and we demonstrate that\\nthe output generated by these models considerably amplifies existing social\\nbias against stigmatized groups. Specifically, we find that the proportion of\\nsocially biased output ranges from 45% to 59% across a variety of decoding\\nstrategies and prompting styles. We discover that the deliberate design of the\\ntemplates in our benchmark (e.g., by adding biasing text to the prompt or\\nvarying the answer that indicates bias) impact the model tendencies to generate\\nsocially biased output. Additionally, we report on patterns in the generated\\nchain-of-thought output, finding a variety of problems from subtle bias to\\nevidence of a lack of reasoning.\\n\\nWarning: This paper contains examples of text which is toxic, biased, and\\nharmful.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.07492',\n",
       "  'authors': ['Manish Nagireddy, Lamogha Chiazor, Moninder Singh, Ioana Baldini'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0.9,\n",
       "    'reason': 'The paper discusses the amplification of social bias in generative language models, which directly relates to the security aspect of AI and language models as it highlights the potential harm caused by biased outputs.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 1,\n",
       "    'reason': 'The paper directly addresses the application of generative language models in social science research by introducing a comprehensive benchmark to capture the amplification of social bias via stigmas in language models.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.6,\n",
       "    'reason': 'The paper indirectly relates to AI and language models for generating misinformation or fact-checking by demonstrating that generative language models considerably amplify existing social bias against stigmatized groups, indicating the potential for generating biased and misinformation-laden content.'}}},\n",
       " 'http://arxiv.org/abs/2110.04133': {'id': 'http://arxiv.org/abs/2110.04133',\n",
       "  'title': 'Quantifying disparities in intimate partner violence: a machine learning method to correct for underreporting. (arXiv:2110.04133v4 [cs.CY] UPDATED)',\n",
       "  'abstract': \"Estimating the prevalence of a medical condition, or the proportion of the\\npopulation in which it occurs, is a fundamental problem in healthcare and\\npublic health. Accurate estimates of the relative prevalence across groups --\\ncapturing, for example, that a condition affects women more frequently than men\\n-- facilitate effective and equitable health policy which prioritizes groups\\nwho are disproportionately affected by a condition. However, it is difficult to\\nestimate relative prevalence when a medical condition is underreported. In this\\nwork, we provide a method for accurately estimating the relative prevalence of\\nunderreported medical conditions, building upon the positive unlabeled learning\\nframework. We show that under the commonly made covariate shift assumption --\\ni.e., that the probability of having a disease conditional on symptoms remains\\nconstant across groups -- we can recover the relative prevalence, even without\\nrestrictive assumptions commonly made in positive unlabeled learning and even\\nif it is impossible to recover the absolute prevalence. We conduct experiments\\non synthetic and real health data which demonstrate our method's ability to\\nrecover the relative prevalence more accurately than do baselines, and\\ndemonstrate the method's robustness to plausible violations of the covariate\\nshift assumption. We conclude by illustrating the applicability of our method\\nto case studies of intimate partner violence and hate speech.\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2110.04133',\n",
       "  'authors': ['Divya Shanmugam, Kaihua Hou, Emma Pierson'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': 'The paper does not directly mention security of AI and language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.8,\n",
       "    'reason': 'The paper discusses using machine learning methods to correct for underreporting of intimate partner violence, which can be considered an application of AI and language models in social science research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2303.00357': {'id': 'http://arxiv.org/abs/2303.00357',\n",
       "  'title': 'Collective moderation of hate, toxicity, and extremity in online discussions. (arXiv:2303.00357v4 [cs.CY] UPDATED)',\n",
       "  'abstract': 'How can citizens address hate in online discourse? We analyze a large corpus\\nof more than 130,000 discussions on Twitter over four years. With the help of\\nhuman annotators, language models and machine learning classifiers, we identify\\ndifferent dimensions of discourse that might be related to the probability of\\nhate speech in subsequent tweets. We use a matching approach and longitudinal\\nstatistical analyses to discern the effectiveness of different counter speech\\nstrategies on the micro-level (individual tweet pairs), meso-level (discussion\\ntrees) and macro-level (days) of discourse. We find that expressing simple\\nopinions, not necessarily supported by facts, but without insults, relates to\\nthe least hate in subsequent discussions. Sarcasm can be helpful as well, in\\nparticular in the presence of organized extreme groups. Mentioning either\\noutgroups or ingroups is typically related to a deterioration of discourse. A\\npronounced emotional tone, either negative such as anger or fear, or positive\\nsuch as enthusiasm and pride, also leads to worse discourse quality. We obtain\\nsimilar results for other measures of quality of discourse beyond hate speech,\\nincluding toxicity, extremity of speech, and the presence of extreme speakers.\\nGoing beyond one-shot analyses on smaller samples of discourse, our findings\\nhave implications for the successful management of online commons through\\ncollective civic moderation.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2303.00357',\n",
       "  'authors': ['Jana Lasser, Alina Herderich, Joshua Garland, Segun Taofeek Aroyehun, David Garcia, Mirta Galesic'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.9,\n",
       "    'reason': 'The paper directly addresses the application of language models in analyzing online discourse and the effectiveness of counter speech strategies, which is clearly relevant to social science research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.8,\n",
       "    'reason': 'The paper discusses the presence of extreme speakers and the quality of discourse, which can be related to the generation of misinformation and fact-checking.'}}},\n",
       " 'http://arxiv.org/abs/2308.01218': {'id': 'http://arxiv.org/abs/2308.01218',\n",
       "  'title': 'The Sequence Matters in Learning -- A Systematic Literature Review. (arXiv:2308.01218v2 [cs.CY] UPDATED)',\n",
       "  'abstract': 'Describing and analysing learner behaviour using sequential data and analysis\\nis becoming more and more popular in Learning Analytics. Nevertheless, we found\\na variety of definitions of learning sequences, as well as choices regarding\\ndata aggregation and the methods implemented for analysis. Furthermore,\\nsequences are used to study different educational settings and serve as a base\\nfor various interventions. In this literature review, the authors aim to\\ngenerate an overview of these aspects to describe the current state of using\\nsequence analysis in educational support and learning analytics. The 74\\nincluded articles were selected based on the criteria that they conduct\\nempirical research on an educational environment using sequences of learning\\nactions as the main focus of their analysis. The results enable us to highlight\\ndifferent learning tasks where sequences are analysed, identify data mapping\\nstrategies for different types of sequence actions, differentiate techniques\\nbased on purpose and scope, and identify educational interventions based on the\\noutcomes of sequence analysis.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2308.01218',\n",
       "  'authors': ['Manuel Valle Torre, Catharine Oertel, Marcus Specht'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.8,\n",
       "    'reason': 'The paper is focused on analyzing learner behavior using sequential data in educational settings, which could potentially be applicable to social science research related to learning analytics.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2309.03648': {'id': 'http://arxiv.org/abs/2309.03648',\n",
       "  'title': 'Promoting Fairness in GNNs: A Characterization of Stability. (arXiv:2309.03648v3 [cs.LG] UPDATED)',\n",
       "  'abstract': \"The Lipschitz bound, a technique from robust statistics, can limit the\\nmaximum changes in the output concerning the input, taking into account\\nassociated irrelevant biased factors. It is an efficient and provable method\\nfor examining the output stability of machine learning models without incurring\\nadditional computation costs. Recently, Graph Neural Networks (GNNs), which\\noperate on non-Euclidean data, have gained significant attention. However, no\\nprevious research has investigated the GNN Lipschitz bounds to shed light on\\nstabilizing model outputs, especially when working on non-Euclidean data with\\ninherent biases. Given the inherent biases in common graph data used for GNN\\ntraining, it poses a serious challenge to constraining the GNN output\\nperturbations induced by input biases, thereby safeguarding fairness during\\ntraining. Recently, despite the Lipschitz constant's use in controlling the\\nstability of Euclideanneural networks, the calculation of the precise Lipschitz\\nconstant remains elusive for non-Euclidean neural networks like GNNs,\\nespecially within fairness contexts. To narrow this gap, we begin with the\\ngeneral GNNs operating on an attributed graph, and formulate a Lipschitz bound\\nto limit the changes in the output regarding biases associated with the input.\\nAdditionally, we theoretically analyze how the Lipschitz constant of a GNN\\nmodel could constrain the output perturbations induced by biases learned from\\ndata for fairness training. We experimentally validate the Lipschitz bound's\\neffectiveness in limiting biases of the model output. Finally, from a training\\ndynamics perspective, we demonstrate why the theoretical Lipschitz bound can\\neffectively guide the GNN training to better trade-off between accuracy and\\nfairness.\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2309.03648',\n",
       "  'authors': ['Yaning Jia, Chunhui Zhang'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.7,\n",
       "    'reason': 'The paper does not directly mention social science research, but it discusses the application of Graph Neural Networks (GNNs) on non-Euclidean data with inherent biases. This concept could potentially be applied in social science research where data may exhibit similar characteristics.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2309.16808': {'id': 'http://arxiv.org/abs/2309.16808',\n",
       "  'title': 'Granularity at Scale: Estimating Neighborhood Socioeconomic Indicators from High-Resolution Orthographic Imagery and Hybrid Learning. (arXiv:2309.16808v2 [cs.CV] UPDATED)',\n",
       "  'abstract': 'Many areas of the world are without basic information on the socioeconomic\\nwell-being of the residing population due to limitations in existing data\\ncollection methods. Overhead images obtained remotely, such as from satellite\\nor aircraft, can help serve as windows into the state of life on the ground and\\nhelp \"fill in the gaps\" where community information is sparse, with estimates\\nat smaller geographic scales requiring higher resolution sensors. Concurrent\\nwith improved sensor resolutions, recent advancements in machine learning and\\ncomputer vision have made it possible to quickly extract features from and\\ndetect patterns in image data, in the process correlating these features with\\nother information. In this work, we explore how well two approaches, a\\nsupervised convolutional neural network and semi-supervised clustering based on\\nbag-of-visual-words, estimate population density, median household income, and\\neducational attainment of individual neighborhoods from publicly available\\nhigh-resolution imagery of cities throughout the United States. Results and\\nanalyses indicate that features extracted from the imagery can accurately\\nestimate the density (R$^2$ up to 0.81) of neighborhoods, with the supervised\\napproach able to explain about half the variation in a population\\'s income and\\neducation. In addition to the presented approaches serving as a basis for\\nfurther geographic generalization, the novel semi-supervised approach provides\\na foundation for future work seeking to estimate fine-scale information from\\naerial imagery without the need for label data.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2309.16808',\n",
       "  'authors': ['Ethan Brewer, Giovani Valdrighi, Parikshit Solunke, Joao Rulff, Yurii Piadyk, Zhonghui Lv, Jorge Poco, Claudio Silva'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': 'The paper does not directly mention any focus on security of AI and language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 1,\n",
       "    'reason': 'The paper directly addresses the application of machine learning and computer vision in estimating neighborhood socioeconomic indicators, which is a form of social science research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.06668': {'id': 'http://arxiv.org/abs/2312.06668',\n",
       "  'title': 'Evaluating Self-supervised Speech Models on a Taiwanese Hokkien Corpus. (arXiv:2312.06668v1 [cs.CL])',\n",
       "  'abstract': \"Taiwanese Hokkien is declining in use and status due to a language shift\\ntowards Mandarin in Taiwan. This is partly why it is a low resource language in\\nNLP and speech research today. To ensure that the state of the art in speech\\nprocessing does not leave Taiwanese Hokkien behind, we contribute a 1.5-hour\\ndataset of Taiwanese Hokkien to ML-SUPERB's hidden set. Evaluating ML-SUPERB's\\nsuite of self-supervised learning (SSL) speech representations on our dataset,\\nwe find that model size does not consistently determine performance. In fact,\\ncertain smaller models outperform larger ones. Furthermore, linguistic\\nalignment between pretraining data and the target language plays a crucial\\nrole.\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2312.06668',\n",
       "  'authors': ['Yi-Hui Chou, Kalvin Chang, Meng-Ju Wu, Winston Ou, Alice Wen-Hsin Bi, Carol Yang, Bryan Y. Chen, Rong-Wei Pai, Po-Yen Yeh, Jo-Peng Chiang, Iu-Tshian Phoann, Winnie Chang, Chenxuan Cui, Noel Chen, Jiatong Shi'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.1,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.06674': {'id': 'http://arxiv.org/abs/2312.06674',\n",
       "  'title': 'Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations. (arXiv:2312.06674v1 [cs.CL])',\n",
       "  'abstract': \"We introduce Llama Guard, an LLM-based input-output safeguard model geared\\ntowards Human-AI conversation use cases. Our model incorporates a safety risk\\ntaxonomy, a valuable tool for categorizing a specific set of safety risks found\\nin LLM prompts (i.e., prompt classification). This taxonomy is also\\ninstrumental in classifying the responses generated by LLMs to these prompts, a\\nprocess we refer to as response classification. For the purpose of both prompt\\nand response classification, we have meticulously gathered a dataset of high\\nquality. Llama Guard, a Llama2-7b model that is instruction-tuned on our\\ncollected dataset, albeit low in volume, demonstrates strong performance on\\nexisting benchmarks such as the OpenAI Moderation Evaluation dataset and\\nToxicChat, where its performance matches or exceeds that of currently available\\ncontent moderation tools. Llama Guard functions as a language model, carrying\\nout multi-class classification and generating binary decision scores.\\nFurthermore, the instruction fine-tuning of Llama Guard allows for the\\ncustomization of tasks and the adaptation of output formats. This feature\\nenhances the model's capabilities, such as enabling the adjustment of taxonomy\\ncategories to align with specific use cases, and facilitating zero-shot or\\nfew-shot prompting with diverse taxonomies at the input. We are making Llama\\nGuard model weights available and we encourage researchers to further develop\\nand adapt them to meet the evolving needs of the community for AI safety.\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2312.06674',\n",
       "  'authors': ['Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, Madian Khabsa'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 1,\n",
       "    'reason': 'The paper introduces Llama Guard, an LLM-based input-output safeguard model geared towards Human-AI conversation use cases. It discusses incorporating a safety risk taxonomy and classifying responses generated by LLMs, demonstrating strong performance in content moderation, which directly relates to the security of AI and language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.6,\n",
       "    'reason': 'The paper does not explicitly mention methods to increase factuality of language model responses. However, it does discuss multi-class classification and binary decision scores, which are related to evaluating the factuality of language model responses.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.4,\n",
       "    'reason': 'The paper does not focus on generating misinformation or fact-checking, but it does mention content moderation and adapting taxonomy categories, which could indirectly relate to fact-checking.'}}},\n",
       " 'http://arxiv.org/abs/2312.06677': {'id': 'http://arxiv.org/abs/2312.06677',\n",
       "  'title': 'Intelligent Virtual Assistants with LLM-based Process Automation. (arXiv:2312.06677v1 [cs.LG])',\n",
       "  'abstract': 'While intelligent virtual assistants like Siri, Alexa, and Google Assistant\\nhave become ubiquitous in modern life, they still face limitations in their\\nability to follow multi-step instructions and accomplish complex goals\\narticulated in natural language. However, recent breakthroughs in large\\nlanguage models (LLMs) show promise for overcoming existing barriers by\\nenhancing natural language processing and reasoning capabilities. Though\\npromising, applying LLMs to create more advanced virtual assistants still faces\\nchallenges like ensuring robust performance and handling variability in\\nreal-world user commands. This paper proposes a novel LLM-based virtual\\nassistant that can automatically perform multi-step operations within mobile\\napps based on high-level user requests. The system represents an advance in\\nassistants by providing an end-to-end solution for parsing instructions,\\nreasoning about goals, and executing actions. LLM-based Process Automation\\n(LLMPA) has modules for decomposing instructions, generating descriptions,\\ndetecting interface elements, predicting next actions, and error checking.\\nExperiments demonstrate the system completing complex mobile operation tasks in\\nAlipay based on natural language instructions. This showcases how large\\nlanguage models can enable automated assistants to accomplish real-world tasks.\\nThe main contributions are the novel LLMPA architecture optimized for app\\nprocess automation, the methodology for applying LLMs to mobile apps, and\\ndemonstrations of multi-step task completion in a real-world environment.\\nNotably, this work represents the first real-world deployment and extensive\\nevaluation of a large language model-based virtual assistant in a widely used\\nmobile application with an enormous user base numbering in the hundreds of\\nmillions.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.06677',\n",
       "  'authors': ['Yanchu Guan, Dong Wang, Zhixuan Chu, Shiyu Wang, Feiyue Ni, Ruihua Song, Longfei Li, Jinjie Gu, Chenyi Zhuang'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0.7,\n",
       "    'reason': 'The paper discusses challenges in applying LLMs to create more advanced virtual assistants, including ensuring robust performance and handling variability in real-world user commands, which touches on the security aspects of AI and language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0.2,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.3,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.06681': {'id': 'http://arxiv.org/abs/2312.06681',\n",
       "  'title': 'Steering Llama 2 via Contrastive Activation Addition. (arXiv:2312.06681v1 [cs.CL])',\n",
       "  'abstract': \"We introduce Contrastive Activation Addition (CAA), an innovative method for\\nsteering language models by modifying activations during their forward passes.\\nCAA computes ``steering vectors'' by averaging the difference in residual\\nstream activations between pairs of positive and negative examples of a\\nparticular behavior such as factual versus hallucinatory responses. During\\ninference, these steering vectors are added at all token positions after the\\nuser's prompt with either a positive or negative coefficient, allowing precise\\ncontrol over the degree of the targeted behavior. We evaluate CAA's\\neffectiveness on Llama 2 Chat using both multiple-choice behavioral question\\ndatasets and open-ended generation tasks. We demonstrate that CAA significantly\\nalters model behavior, outperforms traditional methods like finetuning and\\nfew-shot prompting, and minimally reduces capabilities. Moreover, by employing\\nvarious activation space interpretation methods, we gain deeper insights into\\nCAA's mechanisms. CAA both accurately steers model outputs and also sheds light\\non how high-level concepts are represented in Large Language Models (LLMs).\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2312.06681',\n",
       "  'authors': ['Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, Alexander Matt Turner'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0.8,\n",
       "    'reason': 'The paper is relevant as it introduces a method, CAA, for steering language models by modifying activations, which can be applied in simulations of human-like responses.'},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.9,\n",
       "    'reason': 'The paper directly addresses the use of CAA to control and alter behaviors such as factual versus hallucinatory responses in language models, thus relevant to methods for increasing factuality.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.9,\n",
       "    'reason': \"The paper discusses CAA's effectiveness in altering model behavior, sheds light on how high-level concepts are represented in Large Language Models, and addresses the manipulation of responses, including factual and misleading, so it is relevant to the generation of misinformation and fact-checking.\"}}},\n",
       " 'http://arxiv.org/abs/2312.06705': {'id': 'http://arxiv.org/abs/2312.06705',\n",
       "  'title': \"Perceiving University Student's Opinions from Google App Reviews. (arXiv:2312.06705v1 [cs.CL])\",\n",
       "  'abstract': \"Google app market captures the school of thought of users from every corner\\nof the globe via ratings and text reviews, in a multilinguistic arena. The\\npotential information from the reviews cannot be extracted manually, due to its\\nexponential growth. So, Sentiment analysis, by machine learning and deep\\nlearning algorithms employing NLP, explicitly uncovers and interprets the\\nemotions. This study performs the sentiment classification of the app reviews\\nand identifies the university student's behavior towards the app market via\\nexploratory analysis. We applied machine learning algorithms using the TP, TF,\\nand TF IDF text representation scheme and evaluated its performance on Bagging,\\nan ensemble learning method. We used word embedding, Glove, on the deep\\nlearning paradigms. Our model was trained on Google app reviews and tested on\\nStudent's App Reviews(SAR). The various combinations of these algorithms were\\ncompared amongst each other using F score and accuracy and inferences were\\nhighlighted graphically. SVM, amongst other classifiers, gave fruitful\\naccuracy(93.41%), F score(89%) on bigram and TF IDF scheme. Bagging enhanced\\nthe performance of LR and NB with accuracy of 87.88% and 86.69% and F score of\\n86% and 78% respectively. Overall, LSTM on Glove embedding recorded the highest\\naccuracy(95.2%) and F score(88%).\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2312.06705',\n",
       "  'authors': ['Sakshi Ranjan, Subhankar Mishra'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.8,\n",
       "    'reason': \"The paper discusses the application of machine learning and deep learning algorithms, specifically sentiment analysis, to uncover and interpret university students' behavior towards the app market via exploratory analysis. While not directly focused on social science research, the use of AI and language models in understanding human behavior aligns with the broader applicability in social science research.\"},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.06722': {'id': 'http://arxiv.org/abs/2312.06722',\n",
       "  'title': 'EgoPlan-Bench: Benchmarking Egocentric Embodied Planning with Multimodal Large Language Models. (arXiv:2312.06722v1 [cs.CV])',\n",
       "  'abstract': 'Multimodal Large Language Models (MLLMs), building upon the powerful Large\\nLanguage Models (LLMs) with exceptional reasoning and generalization\\ncapability, have opened up new avenues for embodied task planning. MLLMs excel\\nin their ability to integrate diverse environmental inputs, such as real-time\\ntask progress, visual observations, and open-form language instructions, which\\nare crucial for executable task planning. In this work, we introduce a\\nbenchmark with human annotations, EgoPlan-Bench, to quantitatively investigate\\nthe potential of MLLMs as embodied task planners in real-world scenarios. Our\\nbenchmark is distinguished by realistic tasks derived from real-world videos, a\\ndiverse set of actions involving interactions with hundreds of different\\nobjects, and complex visual observations from varied environments. We evaluate\\nvarious open-source MLLMs, revealing that these models have not yet evolved\\ninto embodied planning generalists (even GPT-4V). We further construct an\\ninstruction-tuning dataset EgoPlan-IT from videos of human-object interactions,\\nto facilitate the learning of high-level task planning in intricate real-world\\nsituations. The experiment results demonstrate that the model tuned on\\nEgoPlan-IT not only significantly improves performance on our benchmark, but\\nalso effectively acts as embodied planner in simulations.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.06722',\n",
       "  'authors': ['Yi Chen, Yuying Ge, Yixiao Ge, Mingyu Ding, Bohao Li, Rui Wang, Ruifeng Xu, Ying Shan, Xihui Liu'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 1,\n",
       "    'reason': 'The paper discusses the use of Multimodal Large Language Models (MLLMs) for embodied task planning, involving interactions with objects and complex visual observations in real-world scenarios, which aligns with the concept of simulating humans in various contexts.'},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.06742': {'id': 'http://arxiv.org/abs/2312.06742',\n",
       "  'title': 'Honeybee: Locality-enhanced Projector for Multimodal LLM. (arXiv:2312.06742v1 [cs.CV])',\n",
       "  'abstract': \"In Multimodal Large Language Models (MLLMs), a visual projector plays a\\ncrucial role in bridging pre-trained vision encoders with LLMs, enabling\\nprofound visual understanding while harnessing the LLMs' robust capabilities.\\nDespite the importance of the visual projector, it has been relatively less\\nexplored. In this study, we first identify two essential projector properties:\\n(i) flexibility in managing the number of visual tokens, crucial for MLLMs'\\noverall efficiency, and (ii) preservation of local context from visual\\nfeatures, vital for spatial understanding. Based on these findings, we propose\\na novel projector design that is both flexible and locality-enhanced,\\neffectively satisfying the two desirable properties. Additionally, we present\\ncomprehensive strategies to effectively utilize multiple and multifaceted\\ninstruction datasets. Through extensive experiments, we examine the impact of\\nindividual design choices. Finally, our proposed MLLM, Honeybee, remarkably\\noutperforms previous state-of-the-art methods across various benchmarks,\\nincluding MME, MMBench, SEED-Bench, and LLaVA-Bench, achieving significantly\\nhigher efficiency. Code and models are available at\\nhttps://github.com/kakaobrain/honeybee.\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2312.06742',\n",
       "  'authors': ['Junbum Cha, Wooyoung Kang, Jonghwan Mun, Byungseok Roh'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.7,\n",
       "    'reason': 'The paper does not directly mention methods to increase factuality of language model response, but it discusses the design and performance of a multimodal large language model (MLLM), which could indirectly relate to the topic.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.06798': {'id': 'http://arxiv.org/abs/2312.06798',\n",
       "  'title': 'Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety. (arXiv:2312.06798v1 [cs.AI])',\n",
       "  'abstract': \"Explainability and Safety engender Trust. These require a model to exhibit\\nconsistency and reliability. To achieve these, it is necessary to use and\\nanalyze data and knowledge with statistical and symbolic AI methods relevant to\\nthe AI application - neither alone will do. Consequently, we argue and seek to\\ndemonstrate that the NeuroSymbolic AI approach is better suited for making AI a\\ntrusted AI system. We present the CREST framework that shows how Consistency,\\nReliability, user-level Explainability, and Safety are built on NeuroSymbolic\\nmethods that use data and knowledge to support requirements for critical\\napplications such as health and well-being. This article focuses on Large\\nLanguage Models (LLMs) as the chosen AI system within the CREST framework. LLMs\\nhave garnered substantial attention from researchers due to their versatility\\nin handling a broad array of natural language processing (NLP) scenarios. For\\nexample, ChatGPT and Google's MedPaLM have emerged as highly promising\\nplatforms for providing information in general and health-related queries,\\nrespectively. Nevertheless, these models remain black boxes despite\\nincorporating human feedback and instruction-guided tuning. For instance,\\nChatGPT can generate unsafe responses despite instituting safety guardrails.\\nCREST presents a plausible approach harnessing procedural and graph-based\\nknowledge within a NeuroSymbolic framework to shed light on the challenges\\nassociated with LLMs.\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2312.06798',\n",
       "  'authors': ['Manas Gaur, Amit Sheth'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 1,\n",
       "    'reason': 'The paper directly discusses the importance of safety and reliability in AI systems, particularly in the context of language models. It mentions the challenges associated with black box models and the need for safety guardrails, which aligns with the topic of security of AI and language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.3,\n",
       "    'reason': 'While the paper focuses on the need for consistency, reliability, and safety in AI systems, it does not directly address specific methods to increase the factuality of language model responses.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.7,\n",
       "    'reason': 'The paper indirectly touches on the challenges related to language models generating unsafe responses, which could be related to the generation of misinformation. However, it does not explicitly address fact-checking or specific methods for generating or combating misinformation.'}}},\n",
       " 'http://arxiv.org/abs/2312.06820': {'id': 'http://arxiv.org/abs/2312.06820',\n",
       "  'title': 'Extracting Self-Consistent Causal Insights from Users Feedback with LLMs and In-context Learning. (arXiv:2312.06820v1 [cs.AI])',\n",
       "  'abstract': \"Microsoft Windows Feedback Hub is designed to receive customer feedback on a\\nwide variety of subjects including critical topics such as power and battery.\\nFeedback is one of the most effective ways to have a grasp of users' experience\\nwith Windows and its ecosystem. However, the sheer volume of feedback received\\nby Feedback Hub makes it immensely challenging to diagnose the actual cause of\\nreported issues. To better understand and triage issues, we leverage Double\\nMachine Learning (DML) to associate users' feedback with telemetry signals. One\\nof the main challenges we face in the DML pipeline is the necessity of domain\\nknowledge for model design (e.g., causal graph), which sometimes is either not\\navailable or hard to obtain. In this work, we take advantage of reasoning\\ncapabilities in Large Language Models (LLMs) to generate a prior model that\\nwhich to some extent compensates for the lack of domain knowledge and could be\\nused as a heuristic for measuring feedback informativeness. Our LLM-based\\napproach is able to extract previously known issues, uncover new bugs, and\\nidentify sequences of events that lead to a bug, while minimizing out-of-domain\\noutputs.\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2312.06820',\n",
       "  'authors': ['Sara Abdali, Anjali Parikh, Steve Lim, Emre Kiciman'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0.3,\n",
       "    'reason': 'The paper does not directly mention security of AI and language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.1,\n",
       "    'reason': 'The paper does not directly mention applications in social science research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0.1,\n",
       "    'reason': 'The paper does not directly mention using AI to simulate humans.'},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.2,\n",
       "    'reason': 'The paper does not directly mention methods to increase factuality of language model response.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.4,\n",
       "    'reason': 'The paper does discuss the use of Large Language Models (LLMs) to generate a prior model that compensates for the lack of domain knowledge and measure feedback informativeness. This could be loosely related to fact-checking and misinformation.'}}},\n",
       " 'http://arxiv.org/abs/2312.06825': {'id': 'http://arxiv.org/abs/2312.06825',\n",
       "  'title': 'Utilization of Non-verbal Behaviour and Social Gaze in Classroom Human-Robot Interaction Communications. (arXiv:2312.06825v1 [cs.RO])',\n",
       "  'abstract': 'This abstract explores classroom Human-Robot Interaction (HRI) scenarios with\\nan emphasis on the adaptation of human-inspired social gaze models in robot\\ncognitive architecture to facilitate a more seamless social interaction. First,\\nwe detail the HRI scenarios explored by us in our studies followed by a\\ndescription of the social gaze model utilized for our research. We highlight\\nthe advantages of utilizing such an attentional model in classroom HRI\\nscenarios. We also detail the intended goals of our upcoming study involving\\nthis social gaze model.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.06825',\n",
       "  'authors': ['Sahand Shaghaghi, Pourya Aliasghari, Bryan Tripp, Kerstin Dautenhahn, Chrystopher Nehaniv'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.9,\n",
       "    'reason': 'The paper discusses the utilization of non-verbal behavior and social gaze in classroom human-robot interaction scenarios, which could be considered a form of AI application in social science research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0.8,\n",
       "    'reason': 'The paper explores the adaptation of human-inspired social gaze models in robot cognitive architecture to facilitate social interaction, which is related to using AI to simulate human behavior.'},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.06855': {'id': 'http://arxiv.org/abs/2312.06855',\n",
       "  'title': 'Multimodal Pretraining of Medical Time Series and Notes. (arXiv:2312.06855v1 [cs.LG])',\n",
       "  'abstract': 'Within the intensive care unit (ICU), a wealth of patient data, including\\nclinical measurements and clinical notes, is readily available. This data is a\\nvaluable resource for comprehending patient health and informing medical\\ndecisions, but it also contains many challenges in analysis. Deep learning\\nmodels show promise in extracting meaningful patterns, but they require\\nextensive labeled data, a challenge in critical care. To address this, we\\npropose a novel approach employing self-supervised pretraining, focusing on the\\nalignment of clinical measurements and notes. Our approach combines contrastive\\nand masked token prediction tasks during pretraining. Semi-supervised\\nexperiments on the MIMIC-III dataset demonstrate the effectiveness of our\\nself-supervised pretraining. In downstream tasks, including in-hospital\\nmortality prediction and phenotyping, our pretrained model outperforms\\nbaselines in settings where only a fraction of the data is labeled, emphasizing\\nits ability to enhance ICU data analysis. Notably, our method excels in\\nsituations where very few labels are available, as evidenced by an increase in\\nthe AUC-ROC for in-hospital mortality by 0.17 and in AUC-PR for phenotyping by\\n0.1 when only 1% of labels are accessible. This work advances self-supervised\\nlearning in the healthcare domain, optimizing clinical insights from abundant\\nyet challenging ICU data.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.06855',\n",
       "  'authors': ['Ryan King, Tianbao Yang, Bobak Mortazavi'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': 'The paper does not mention anything related to the security of AI and language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': 'The paper focuses on pretraining medical time series and notes, and does not discuss applications in social science research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': 'The paper does not discuss the use of AI to simulate humans in various contexts.'},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': 'The paper does not address the methods to increase the factuality of language model response.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': 'The paper does not involve AI and language models for generating misinformation or fact-checking.'}}},\n",
       " 'http://arxiv.org/abs/2312.06867': {'id': 'http://arxiv.org/abs/2312.06867',\n",
       "  'title': 'Get an A in Math: Progressive Rectification Prompting. (arXiv:2312.06867v1 [cs.CL])',\n",
       "  'abstract': 'Chain-of-Thought (CoT) prompting methods have enabled large language models\\n(LLMs) to generate reasoning paths and solve math word problems (MWPs).\\nHowever, they are sensitive to mistakes in the paths, as any mistake can result\\nin an incorrect answer. We propose a novel method named Progressive\\nRectification Prompting (PRP) to improve average accuracy on eight MWP datasets\\nfrom 77.3 to 90.5. Given an initial answer from CoT, PRP iterates a\\nverify-then-rectify process to progressively identify incorrect answers and\\nrectify the reasoning paths. With the most likely correct answer, the LLM\\npredicts a masked numerical value in the question; if the prediction does not\\nmatch the masked value, the answer is likely incorrect. Then the LLM is\\nprompted to re-generate the reasoning path hinted with a set of incorrect\\nanswers to prevent itself from repeating previous mistakes. PRP achieves the\\nbest performance compared against the CoT methods. Our implementation is made\\npublicly available at https://wzy6642.github.io/prp.github.io/.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.06867',\n",
       "  'authors': ['Zhenyu Wu, Meng Jiang, Chao Shen'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.8,\n",
       "    'reason': 'The paper is not directly about factuality, but it does discuss methods to improve the accuracy and correctness of language model responses, which demonstrates relevance to the broader concept of factuality.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.06874': {'id': 'http://arxiv.org/abs/2312.06874',\n",
       "  'title': 'Dozerformer: Sequence Adaptive Sparse Transformer for Multivariate Time Series Forecasting. (arXiv:2312.06874v1 [cs.LG])',\n",
       "  'abstract': 'Transformers have achieved remarkable performance in multivariate time\\nseries(MTS) forecasting due to their capability to capture long-term\\ndependencies. However, the canonical attention mechanism has two key\\nlimitations: (1) its quadratic time complexity limits the sequence length, and\\n(2) it generates future values from the entire historical sequence. To address\\nthis, we propose a Dozer Attention mechanism consisting of three sparse\\ncomponents: (1) Local, each query exclusively attends to keys within a\\nlocalized window of neighboring time steps. (2) Stride, enables each query to\\nattend to keys at predefined intervals. (3) Vary, allows queries to selectively\\nattend to keys from a subset of the historical sequence. Notably, the size of\\nthis subset dynamically expands as forecasting horizons extend. Those three\\ncomponents are designed to capture essential attributes of MTS data, including\\nlocality, seasonality, and global temporal dependencies. Additionally, we\\npresent the Dozerformer Framework, incorporating the Dozer Attention mechanism\\nfor the MTS forecasting task. We evaluated the proposed Dozerformer framework\\nwith recent state-of-the-art methods on nine benchmark datasets and confirmed\\nits superior performance. The code will be released after the manuscript is\\naccepted.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.06874',\n",
       "  'authors': ['Yifan Zhang, Rui Wu, Sergiu M. Dascalu, Frederick C. Harris Jr'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.7,\n",
       "    'reason': 'The paper does not directly mention applications in social science research, but it focuses on the use of transformers for multivariate time series forecasting which could potentially have applications in social science research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.06881': {'id': 'http://arxiv.org/abs/2312.06881',\n",
       "  'title': 'DYAD: A Descriptive Yet Abjuring Density efficient approximation to linear neural network layers. (arXiv:2312.06881v1 [cs.LG])',\n",
       "  'abstract': 'We devise, implement and performance-asses DYAD, a layer which can serve as a\\nfaster and more memory-efficient approximate replacement for linear layers,\\n(nn.Linear() in Pytorch). These layers appear in common subcomponents, such as\\nin the ff module of Transformers. DYAD is based on a bespoke near-sparse matrix\\nstructure which approximates the dense \"weight\" matrix W that matrix-multiplies\\nthe input in the typical realization of such a layer, a.k.a DENSE. Our\\nalternative near-sparse matrix structure is decomposable to a sum of 2 matrices\\npermutable to a block-sparse counterpart. These can be represented as 3D\\ntensors, which in unison allow a faster execution of matrix multiplication with\\nthe mini-batched input matrix X compared to DENSE (O(rows(W ) x cols(W )) -->\\nO( rows(W ) x cols(W ) # of blocks )). As the crux of our experiments, we\\npretrain both DYAD and DENSE variants of 2 sizes of the OPT arch and 1 size of\\nthe Pythia arch, including at different token scales of the babyLM benchmark.\\nWe find DYAD to be competitive (>= 90%) of DENSE performance on zero-shot (e.g.\\nBLIMP), few-shot (OPENLM) and finetuning (GLUE) benchmarks, while being >=7-15%\\nfaster to train on-GPU even at 125m scale, besides surfacing larger speedups at\\nincreasing scale and model width.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.06881',\n",
       "  'authors': ['Sarin Chandy, Varun Gangal, Yi Yang, Gabriel Maggiotti'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': 'The paper does not mention any direct relevance to security aspects of AI and language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': 'The paper does not discuss applications in social science research; it primarily focuses on efficiency improvements for neural network layers.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': 'The paper does not address the use of AI to simulate humans; instead, it focuses on optimizing neural network layers for efficiency.'},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': 'The paper does not mention any relevance to improving factuality of language model responses; its focus is on performance and efficiency of neural network layers.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': 'The paper does not pertain to generating misinformation or fact-checking; it focuses on optimizing neural network layers for performance.'}}},\n",
       " 'http://arxiv.org/abs/2312.06924': {'id': 'http://arxiv.org/abs/2312.06924',\n",
       "  'title': 'Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack. (arXiv:2312.06924v1 [cs.CL])',\n",
       "  'abstract': 'Recent developments in balancing the usefulness and safety of Large Language\\nModels (LLMs) have raised a critical question: Are mainstream NLP tasks\\nadequately aligned with safety consideration? Our study, focusing on\\nsafety-sensitive documents obtained through adversarial attacks, reveals\\nsignificant disparities in the safety alignment of various NLP tasks. For\\ninstance, LLMs can effectively summarize malicious long documents but often\\nrefuse to translate them. This discrepancy highlights a previously unidentified\\nvulnerability: attacks exploiting tasks with weaker safety alignment, like\\nsummarization, can potentially compromise the integraty of tasks traditionally\\ndeemed more robust, such as translation and question-answering (QA). Moreover,\\nthe concurrent use of multiple NLP tasks with lesser safety alignment increases\\nthe risk of LLMs inadvertently processing harmful content. We demonstrate these\\nvulnerabilities in various safety-aligned LLMs, particularly Llama2 models and\\nGPT-4, indicating an urgent need for strengthening safety alignments across a\\nbroad spectrum of NLP tasks.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.06924',\n",
       "  'authors': ['Yu Fu, Yufei Li, Wen Xiao, Cong Liu, Yue Dong'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 1,\n",
       "    'reason': 'The paper directly addresses safety considerations and vulnerabilities in the context of language models, highlighting the disparity in safety alignment of various NLP tasks and the potential risks associated with weaker safety alignment.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.8,\n",
       "    'reason': 'While not directly discussing the generation of misinformation or fact-checking, the paper does touch on the potential risks of processing harmful content in NLP tasks, which indirectly relates to the broader topic of generating misinformation and fact-checking.'}}},\n",
       " 'http://arxiv.org/abs/2312.06926': {'id': 'http://arxiv.org/abs/2312.06926',\n",
       "  'title': 'Content-Localization based Neural Machine Translation for Informal Dialectal Arabic: Spanish/French to Levantine/Gulf Arabic. (arXiv:2312.06926v1 [cs.CL])',\n",
       "  'abstract': 'Resources in high-resource languages have not been efficiently exploited in\\nlow-resource languages to solve language-dependent research problems. Spanish\\nand French are considered high resource languages in which an adequate level of\\ndata resources for informal online social behavior modeling, is observed.\\nHowever, a machine translation system to access those data resources and\\ntransfer their context and tone to a low-resource language like dialectal\\nArabic, does not exist. In response, we propose a framework that localizes\\ncontents of high-resource languages to a low-resource language/dialects by\\nutilizing AI power. To the best of our knowledge, we are the first work to\\nprovide a parallel translation dataset from/to informal Spanish and French\\nto/from informal Arabic dialects. Using this, we aim to enrich the\\nunder-resource-status dialectal Arabic and fast-track the research of diverse\\nonline social behaviors within and across smart cities in different\\ngeo-regions. The experimental results have illustrated the capability of our\\nproposed solution in exploiting the resources between high and low resource\\nlanguages and dialects. Not only this, but it has also been proven that\\nignoring dialects within the same language could lead to misleading analysis of\\nonline social behavior.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.06926',\n",
       "  'authors': ['Fatimah Alzamzami, Abdulmotaleb El Saddik'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.9,\n",
       "    'reason': 'The paper discusses the utilization of AI for enriching the under-resource-status dialectal Arabic and fast-tracking the research of diverse online social behaviors within and across smart cities in different geo-regions.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.06950': {'id': 'http://arxiv.org/abs/2312.06950',\n",
       "  'title': 'READ-PVLA: Recurrent Adapter with Partial Video-Language Alignment for Parameter-Efficient Transfer Learning in Low-Resource Video-Language Modeling. (arXiv:2312.06950v1 [cs.CV])',\n",
       "  'abstract': \"Fully fine-tuning pretrained large-scale transformer models has become a\\npopular paradigm for video-language modeling tasks, such as temporal language\\ngrounding and video-language summarization. With a growing number of tasks and\\nlimited training data, such full fine-tuning approach leads to costly model\\nstorage and unstable training. To overcome these shortcomings, we introduce\\nlightweight adapters to the pre-trained model and only update them at\\nfine-tuning time. However, existing adapters fail to capture intrinsic temporal\\nrelations among video frames or textual words. Moreover, they neglect the\\npreservation of critical task-related information that flows from the raw\\nvideo-language input into the adapter's low-dimensional space. To address these\\nissues, we first propose a novel REcurrent ADapter (READ) that employs\\nrecurrent computation to enable temporal modeling capability. Second, we\\npropose Partial Video-Language Alignment (PVLA) objective via the use of\\npartial optimal transport to maintain task-related information flowing into our\\nREAD modules. We validate our READ-PVLA framework through extensive experiments\\nwhere READ-PVLA significantly outperforms all existing fine-tuning strategies\\non multiple low-resource temporal language grounding and video-language\\nsummarization benchmarks.\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2312.06950',\n",
       "  'authors': ['Thong Nguyen, Xiaobao Wu, Xinshuai Dong, Khoi Le, Zhiyuan Hu, Cong-Duy Nguyen, See-Kiong Ng, Luu Anh Tuan'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.8,\n",
       "    'reason': 'The paper does not directly mention social science research, but it discusses the use of lightweight adapters and fine-tuning strategies for video-language modeling, which could be applicable to various areas including social science research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.7,\n",
       "    'reason': 'The paper does not explicitly mention factuality of language model response, but it introduces novel techniques such as Partial Video-Language Alignment (PVLA) objective to maintain task-related information, which could potentially contribute to increasing the factuality of language model responses.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.06974': {'id': 'http://arxiv.org/abs/2312.06974',\n",
       "  'title': 'SM70: A Large Language Model for Medical Devices. (arXiv:2312.06974v1 [cs.CL])',\n",
       "  'abstract': \"We are introducing SM70, a 70 billion-parameter Large Language Model that is\\nspecifically designed for SpassMed's medical devices under the brand name\\n'JEE1' (pronounced as G1 and means 'Life'). This large language model provides\\nmore accurate and safe responses to medical-domain questions. To fine-tune\\nSM70, we used around 800K data entries from the publicly available dataset\\nMedAlpaca. The Llama2 70B open-sourced model served as the foundation for SM70,\\nand we employed the QLoRA technique for fine-tuning. The evaluation is\\nconducted across three benchmark datasets - MEDQA - USMLE, PUBMEDQA, and USMLE\\n- each representing a unique aspect of medical knowledge and reasoning. The\\nperformance of SM70 is contrasted with other notable LLMs, including Llama2\\n70B, Clinical Camel 70 (CC70), GPT 3.5, GPT 4, and Med-Palm, to provide a\\ncomparative understanding of its capabilities within the medical domain. Our\\nresults indicate that SM70 outperforms several established models in these\\ndatasets, showcasing its proficiency in handling a range of medical queries,\\nfrom fact-based questions derived from PubMed abstracts to complex clinical\\ndecision-making scenarios. The robust performance of SM70, particularly in the\\nUSMLE and PUBMEDQA datasets, suggests its potential as an effective tool in\\nclinical decision support and medical information retrieval. Despite its\\npromising results, the paper also acknowledges the areas where SM70 lags behind\\nthe most advanced model, GPT 4, thereby highlighting the need for further\\ndevelopment, especially in tasks demanding extensive medical knowledge and\\nintricate reasoning.\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2312.06974',\n",
       "  'authors': ['Anubhav Bhatti, Surajsinh Parmar, San Lee'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.8,\n",
       "    'reason': 'The paper does not directly address this topic, but it discusses creating a large language model specifically designed for medical devices to provide more accurate and safe responses to medical-domain questions. This could indirectly relate to improving the factuality of language model responses in the medical context.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.07000': {'id': 'http://arxiv.org/abs/2312.07000',\n",
       "  'title': 'Alignment for Honesty. (arXiv:2312.07000v1 [cs.CL])',\n",
       "  'abstract': \"Recent research has made significant strides in applying alignment techniques\\nto enhance the helpfulness and harmlessness of large language models (LLMs) in\\naccordance with human intentions. In this paper, we argue for the importance of\\nalignment for honesty, ensuring that LLMs proactively refuse to answer\\nquestions when they lack knowledge, while still not being overly conservative.\\nHowever, a pivotal aspect of alignment for honesty involves discerning the\\nlimits of an LLM's knowledge, which is far from straightforward. This challenge\\ndemands comprehensive solutions in terms of metric development, benchmark\\ncreation, and training methodologies. In this paper, we address these\\nchallenges by first establishing a precise problem definition and defining\\n``honesty'' inspired by the Analects of Confucius. This serves as a cornerstone\\nfor developing metrics that effectively measure an LLM's honesty by quantifying\\nits progress post-alignment. Furthermore, we introduce a flexible training\\nframework which is further instantiated by several efficient fine-tuning\\ntechniques that emphasize honesty without sacrificing performance on other\\ntasks. Our extensive experiments reveal that these aligned models show a marked\\nincrease in honesty, as indicated by our proposed metrics. We open-source a\\nwealth of resources to facilitate future research at\\nhttps://github.com/GAIR-NLP/alignment-for-honesty, including honesty-aligned\\nmodels, training and evaluation datasets for honesty alignment, concept\\nglossary, as well as all relevant source code.\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2312.07000',\n",
       "  'authors': ['Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, Pengfei Liu'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0.9,\n",
       "    'reason': 'The paper discusses the importance of alignment for honesty in language models to ensure they proactively refuse to answer questions when they lack knowledge, while still not being overly conservative. This aligns with the broader topic of security and safety in AI and language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.7,\n",
       "    'reason': \"The paper addresses the challenge of discerning the limits of a language model's knowledge and proposes metrics to measure the honesty of language models. While not directly focused on increasing factuality, the discussion on measuring honesty is somewhat related to the broader topic of factuality.\"},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.6,\n",
       "    'reason': \"The paper emphasizes the importance of aligning language models for honesty and discusses the development of metrics to measure an LLM's honesty. While not directly focused on generating misinformation or fact-checking, the discussion on honesty alignment has implications for factuality and misinformation.\"}}},\n",
       " 'http://arxiv.org/abs/2312.07028': {'id': 'http://arxiv.org/abs/2312.07028',\n",
       "  'title': 'Dynamic Corrective Self-Distillation for Better Fine-Tuning of Pretrained Models. (arXiv:2312.07028v1 [cs.CL])',\n",
       "  'abstract': 'We tackle the challenging issue of aggressive fine-tuning encountered during\\nthe process of transfer learning of pre-trained language models (PLMs) with\\nlimited labeled downstream data. This problem primarily results in a decline in\\nperformance on the subsequent task. Inspired by the adaptive boosting method in\\ntraditional machine learning, we present an effective dynamic corrective\\nself-distillation (DCS) approach to improve the fine-tuning of the PLMs. Our\\ntechnique involves performing a self-distillation mechanism where, at each\\niteration, the student model actively adapts and corrects itself by dynamically\\nadjusting the weights assigned to individual data points. This iterative\\nself-correcting process significantly enhances the overall fine-tuning\\ncapability of PLMs, leading to improved performance and robustness. We\\nconducted comprehensive evaluations using the GLUE benchmark demonstrating the\\nefficacy of our method in enhancing the fine-tuning process for various PLMs\\nacross diverse downstream tasks.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.07028',\n",
       "  'authors': ['Ibtihel Amara, Vinija Jain, Aman Chadha'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.8,\n",
       "    'reason': 'The paper does not directly mention social science research, but it discusses improving fine-tuning of pre-trained language models which could have potential applications in social science research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.85,\n",
       "    'reason': 'The paper addresses the issue of improving fine-tuning of language models, which indirectly relates to the factuality of the language model response.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.07046': {'id': 'http://arxiv.org/abs/2312.07046',\n",
       "  'title': 'Rethinking Compression: Reduced Order Modelling of Latent Features in Large Language Models. (arXiv:2312.07046v1 [cs.LG])',\n",
       "  'abstract': 'Due to the substantial scale of Large Language Models (LLMs), the direct\\napplication of conventional compression methodologies proves impractical. The\\ncomputational demands associated with even minimal gradient updates present\\nchallenges, particularly on consumer-grade hardware. This paper introduces an\\ninnovative approach for the parametric and practical compression of LLMs based\\non reduced order modelling, which entails low-rank decomposition within the\\nfeature space and re-parameterization in the weight space. Notably, this\\ncompression technique operates in a layer-wise manner, obviating the need for a\\nGPU device and enabling the compression of billion-scale models within\\nstringent constraints of both memory and time. Our method represents a\\nsignificant advancement in model compression by leveraging matrix\\ndecomposition, demonstrating superior efficacy compared to the prevailing\\nstate-of-the-art structured pruning method.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.07046',\n",
       "  'authors': ['Arnav Chavan, Nahush Lele, Deepak Gupta'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0.8,\n",
       "    'reason': 'The paper discusses the compression of Large Language Models (LLMs), which may have implications for security by potentially reducing the computational demands and memory constraints.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.7,\n",
       "    'reason': 'The paper introduces an innovative approach for the compression of LLMs, which could have implications for improving the factuality of language model responses through reduced order modelling.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.07049': {'id': 'http://arxiv.org/abs/2312.07049',\n",
       "  'title': 'Improving Factual Error Correction by Learning to Inject Factual Errors. (arXiv:2312.07049v1 [cs.CL])',\n",
       "  'abstract': 'Factual error correction (FEC) aims to revise factual errors in false claims\\nwith minimal editing, making them faithful to the provided evidence. This task\\nis crucial for alleviating the hallucination problem encountered by large\\nlanguage models. Given the lack of paired data (i.e., false claims and their\\ncorresponding correct claims), existing methods typically adopt the\\nmask-then-correct paradigm. This paradigm relies solely on unpaired false\\nclaims and correct claims, thus being referred to as distantly supervised\\nmethods. These methods require a masker to explicitly identify factual errors\\nwithin false claims before revising with a corrector. However, the absence of\\npaired data to train the masker makes accurately pinpointing factual errors\\nwithin claims challenging. To mitigate this, we propose to improve FEC by\\nLearning to Inject Factual Errors (LIFE), a three-step distantly supervised\\nmethod: mask-corrupt-correct. Specifically, we first train a corruptor using\\nthe mask-then-corrupt procedure, allowing it to deliberately introduce factual\\nerrors into correct text. The corruptor is then applied to correct claims,\\ngenerating a substantial amount of paired data. After that, we filter out\\nlow-quality data, and use the remaining data to train a corrector. Notably, our\\ncorrector does not require a masker, thus circumventing the bottleneck\\nassociated with explicit factual error identification. Our experiments on a\\npublic dataset verify the effectiveness of LIFE in two key aspects: Firstly, it\\noutperforms the previous best-performing distantly supervised method by a\\nnotable margin of 10.59 points in SARI Final (19.3% improvement). Secondly,\\neven compared to ChatGPT prompted with in-context examples, LIFE achieves a\\nsuperiority of 7.16 points in SARI Final.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.07049',\n",
       "  'authors': ['Xingwei He, Qianru Zhang, A-Long Jin, Jun Ma, Yuan Yuan, Siu Ming Yiu'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0.6,\n",
       "    'reason': 'The paper discusses the challenges encountered by large language models and proposes a method to alleviate the hallucination problem, which is a security concern related to the generation of false information by language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.9,\n",
       "    'reason': 'The paper directly addresses the improvement of factual error correction in language models, which is closely related to increasing the factuality of language model responses.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.8,\n",
       "    'reason': 'The paper addresses the task of factual error correction, which is relevant to the broader topic of fact-checking and mitigating the generation of misinformation by language models.'}}},\n",
       " 'http://arxiv.org/abs/2312.07066': {'id': 'http://arxiv.org/abs/2312.07066',\n",
       "  'title': 'DiffuVST: Narrating Fictional Scenes with Global-History-Guided Denoising Models. (arXiv:2312.07066v1 [cs.CL])',\n",
       "  'abstract': 'Recent advances in image and video creation, especially AI-based image\\nsynthesis, have led to the production of numerous visual scenes that exhibit a\\nhigh level of abstractness and diversity. Consequently, Visual Storytelling\\n(VST), a task that involves generating meaningful and coherent narratives from\\na collection of images, has become even more challenging and is increasingly\\ndesired beyond real-world imagery. While existing VST techniques, which\\ntypically use autoregressive decoders, have made significant progress, they\\nsuffer from low inference speed and are not well-suited for synthetic scenes.\\nTo this end, we propose a novel diffusion-based system DiffuVST, which models\\nthe generation of a series of visual descriptions as a single conditional\\ndenoising process. The stochastic and non-autoregressive nature of DiffuVST at\\ninference time allows it to generate highly diverse narratives more\\nefficiently. In addition, DiffuVST features a unique design with bi-directional\\ntext history guidance and multimodal adapter modules, which effectively improve\\ninter-sentence coherence and image-to-text fidelity. Extensive experiments on\\nthe story generation task covering four fictional visual-story datasets\\ndemonstrate the superiority of DiffuVST over traditional autoregressive models\\nin terms of both text quality and inference speed.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.07066',\n",
       "  'authors': ['Shengguang Wu, Mei Yuan, Qi Su'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.9,\n",
       "    'reason': 'The paper focuses on AI-based image synthesis and storytelling, which can be used in creating fictional scenarios, an area with potential application in the generation of misinformation and fact-checking.'}}},\n",
       " 'http://arxiv.org/abs/2312.07069': {'id': 'http://arxiv.org/abs/2312.07069',\n",
       "  'title': 'Context Matter: Data-Efficient Augmentation of Large Language Models for Scientific Applications. (arXiv:2312.07069v1 [cs.CL])',\n",
       "  'abstract': \"In this paper, we explore the challenges inherent to Large Language Models\\n(LLMs) like GPT-4, particularly their propensity for hallucinations, logic\\nmistakes, and incorrect conclusions when tasked with answering complex\\nquestions. The capacity of LLMs to present erroneous answers in a coherent and\\nsemantically rigorous manner further complicates the detection of factual\\ninaccuracies. This issue is especially pronounced in fields that require\\nspecialized expertise. Our work delves into these challenges, aiming to enhance\\nthe understanding and mitigation of such errors, thereby contributing to the\\nimprovement of LLM accuracy and reliability in scientific and other specialized\\ndomains. Our findings reveal a non-linear relationship between the context's\\nrelevancy and the answers' measured quality. In addition, we demonstrate that\\nwith the correct calibration, it is possible to automate the grading procedure\\n-- a finding suggesting that, at least to some degree, the LLMs can be used to\\nself-examine the quality of their own performance. Finally, we describe an\\nexperimental platform that can be seen as a proof-of-concept of the techniques\\ndescribed in this work.\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2312.07069',\n",
       "  'authors': ['Xiang Li, Haoran Tang, Siyu Chen, Ziwei Wang, Anurag Maravi, Marcin Abram'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0.9,\n",
       "    'reason': 'The paper discusses the challenges and errors inherent to Large Language Models (LLMs), including their propensity for logic mistakes and incorrect conclusions, which are relevant to the security implications of AI and language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.9,\n",
       "    'reason': 'The paper aims to enhance the understanding and mitigation of errors in Large Language Models, contributing to the improvement of LLM accuracy and reliability, which is directly related to increasing the factuality of language model responses.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.07088': {'id': 'http://arxiv.org/abs/2312.07088',\n",
       "  'title': 'BED: Bi-Encoder-Decoder Model for Canonical Relation Extraction. (arXiv:2312.07088v1 [cs.CL])',\n",
       "  'abstract': 'Canonical relation extraction aims to extract relational triples from\\nsentences, where the triple elements (entity pairs and their relationship) are\\nmapped to the knowledge base. Recently, methods based on the encoder-decoder\\narchitecture are proposed and achieve promising results. However, these methods\\ncannot well utilize the entity information, which is merely used as augmented\\ntraining data. Moreover, they are incapable of representing novel entities,\\nsince no embeddings have been learned for them. In this paper, we propose a\\nnovel framework, Bi-Encoder-Decoder (BED), to solve the above issues.\\nSpecifically, to fully utilize entity information, we employ an encoder to\\nencode semantics of this information, leading to high-quality entity\\nrepresentations. For novel entities, given a trained entity encoder, their\\nrepresentations can be easily generated. Experimental results on two datasets\\nshow that, our method achieves a significant performance improvement over the\\nprevious state-of-the-art and handle novel entities well without retraining.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.07088',\n",
       "  'authors': ['Nantao Zheng, Siyu Long, Xinyu Dai'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.8,\n",
       "    'reason': 'The paper introduces a novel framework, Bi-Encoder-Decoder (BED), for canonical relation extraction, which involves analyzing and representing relationships between entities in text. While it does not explicitly mention factuality, the methods proposed could potentially contribute to increasing the factuality of language model responses by improving the extraction and representation of relational triples from sentences.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.07110': {'id': 'http://arxiv.org/abs/2312.07110',\n",
       "  'title': 'LLMs Perform Poorly at Concept Extraction in Cyber-security Research Literature. (arXiv:2312.07110v1 [cs.CL])',\n",
       "  'abstract': 'The cybersecurity landscape evolves rapidly and poses threats to\\norganizations. To enhance resilience, one needs to track the latest\\ndevelopments and trends in the domain. It has been demonstrated that standard\\nbibliometrics approaches show their limits in such a fast-evolving domain. For\\nthis purpose, we use large language models (LLMs) to extract relevant knowledge\\nentities from cybersecurity-related texts. We use a subset of arXiv preprints\\non cybersecurity as our data and compare different LLMs in terms of entity\\nrecognition (ER) and relevance. The results suggest that LLMs do not produce\\ngood knowledge entities that reflect the cybersecurity context, but our results\\nshow some potential for noun extractors. For this reason, we developed a noun\\nextractor boosted with some statistical analysis to extract specific and\\nrelevant compound nouns from the domain. Later, we tested our model to identify\\ntrends in the LLM domain. We observe some limitations, but it offers promising\\nresults to monitor the evolution of emergent trends.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.07110',\n",
       "  'authors': ['Maxime Würsch, Andrei Kucharavy, Dimitri Percia David, Alain Mermoud'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0.9,\n",
       "    'reason': 'The paper directly discusses the use of large language models (LLMs) in the cybersecurity domain, which involves aspects of security. However, it does not specifically focus on the security of AI and language models themselves.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.07141': {'id': 'http://arxiv.org/abs/2312.07141',\n",
       "  'title': 'Multilingual large language models leak human stereotypes across language boundaries. (arXiv:2312.07141v1 [cs.CL])',\n",
       "  'abstract': \"Multilingual large language models have been increasingly popular for their\\nproficiency in comprehending and generating text across various languages.\\nPrevious research has shown that the presence of stereotypes and biases in\\nmonolingual large language models can be attributed to the nature of their\\ntraining data, which is collected from humans and reflects societal biases.\\nMultilingual language models undergo the same training procedure as monolingual\\nones, albeit with training data sourced from various languages. This raises the\\nquestion: do stereotypes present in one social context leak across languages\\nwithin the model? In our work, we first define the term ``stereotype leakage''\\nand propose a framework for its measurement. With this framework, we\\ninvestigate how stereotypical associations leak across four languages: English,\\nRussian, Chinese, and Hindi. To quantify the stereotype leakage, we employ an\\napproach from social psychology, measuring stereotypes via group-trait\\nassociations. We evaluate human stereotypes and stereotypical associations\\nmanifested in multilingual large language models such as mBERT, mT5, and\\nChatGPT. Our findings show a noticeable leakage of positive, negative, and\\nnon-polar associations across all languages. Notably, Hindi within multilingual\\nmodels appears to be the most susceptible to influence from other languages,\\nwhile Chinese is the least. Additionally, ChatGPT exhibits a better alignment\\nwith human scores than other models.\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2312.07141',\n",
       "  'authors': ['Yang Trista Cao, Anna Sotnikova, Jieyu Zhao, Linda X. Zou, Rachel Rudinger, Hal Daume III'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0.1,\n",
       "    'reason': 'The paper does not directly discuss security aspects of AI and language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.9,\n",
       "    'reason': 'The paper directly addresses the application of multilingual large language models in investigating stereotypes and biases across languages, which is highly relevant to social science research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.07182': {'id': 'http://arxiv.org/abs/2312.07182',\n",
       "  'title': 'Classifying complex documents: comparing bespoke solutions to large language models. (arXiv:2312.07182v1 [cs.CL])',\n",
       "  'abstract': 'Here we search for the best automated classification approach for a set of\\ncomplex legal documents. Our classification task is not trivial: our aim is to\\nclassify ca 30,000 public courthouse records from 12 states and 267 counties at\\ntwo different levels using nine sub-categories. Specifically, we investigated\\nwhether a fine-tuned large language model (LLM) can achieve the accuracy of a\\nbespoke custom-trained model, and what is the amount of fine-tuning necessary.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.07182',\n",
       "  'authors': ['Glen Hopkins, Kristjan Kalm'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.8,\n",
       "    'reason': 'The paper discusses the application of a large language model (LLM) for automated classification of complex legal documents, which can potentially be applicable to social science research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.7,\n",
       "    'reason': 'The paper explores the comparison of a fine-tuned large language model (LLM) with bespoke custom-trained models, which indirectly relates to the factuality of language model responses.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.3,\n",
       "    'reason': 'The paper does not directly address the generation of misinformation or fact-checking using AI and language models.'}}},\n",
       " 'http://arxiv.org/abs/2312.07194': {'id': 'http://arxiv.org/abs/2312.07194',\n",
       "  'title': 'Verbreitungsmechanismen sch\\\\\"adigender Sprache im Netz: Anatomie zweier Shitstorms. (arXiv:2312.07194v1 [cs.CL])',\n",
       "  'abstract': 'In this working paper, we turn our attention to two exemplary, cross-media\\nshitstorms directed against well-known individuals from the business world.\\nBoth have in common, first, the trigger, a controversial statement by the\\nperson who thereby becomes the target of the shitstorm, and second, the\\nidentity of this target as relatively privileged: cis-male, white, successful.\\nWe examine the spread of the outrage wave across two media at a time and test\\nthe applicability of computational linguistic methods for analyzing its time\\ncourse. Assuming that harmful language spreads like a virus in digital space,\\nwe are primarily interested in the events and constellations that lead to the\\nuse of harmful language, and whether and how a linguistic formation of \"tribes\"\\noccurs. Our research therefore focuses, first, on the distribution of\\nlinguistic features within the overall shitstorm: are individual words or\\nphrases increasingly used after their introduction, and through which pathways\\nthey spread. Second, we ask whether \"tribes,\" for example, one group of\\nsupporters and one of opponents of the target, have a distinguished linguistic\\nform. Our hypothesis is that supporters remain equally active over time, while\\nthe dynamic \"ripple\" effect of the shitstorm is based on the varying\\nparticipation of opponents.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.07194',\n",
       "  'authors': ['Tatjana Scheffler, Veronika Solopova, Mihaela Popa-Wyatt'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': 'The paper does not directly mention security of AI and language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.9,\n",
       "    'reason': 'The paper directly discusses the application of computational linguistic methods for analyzing the time course of outrage wave, which is relevant to the application of AI and language models in social science research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': 'The paper does not directly mention the use of AI to simulate humans in various contexts.'},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': 'The paper does not directly mention methods to increase the factuality of language model response.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.8,\n",
       "    'reason': \"The paper discusses the spread of harmful language in digital space and the linguistic formation of 'tribes,' which is indirectly related to the topic of AI and language models for generating misinformation or fact-checking.\"}}},\n",
       " 'http://arxiv.org/abs/2312.07228': {'id': 'http://arxiv.org/abs/2312.07228',\n",
       "  'title': 'Toxic language detection: a systematic survey of Arabic datasets. (arXiv:2312.07228v1 [cs.CL])',\n",
       "  'abstract': 'This paper offers a comprehensive survey of Arabic datasets focused on online\\ntoxic language. We systematically gathered a total of 49 available datasets and\\ntheir corresponding papers and conducted a thorough analysis, considering 16\\ncriteria across three primary dimensions: content, annotation process, and\\nreusability. This analysis enabled us to identify existing gaps and make\\nrecommendations for future research works.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.07228',\n",
       "  'authors': ['Imene Bensalem, Paolo Rosso, Hanane Zitouni'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.8,\n",
       "    'reason': 'The paper systematically surveys Arabic datasets focused on online toxic language, which can be relevant to social science research involving the application of AI and language models to understand online behaviors and interactions.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.7,\n",
       "    'reason': 'The paper addresses the detection of toxic language in Arabic datasets, which is relevant to the broader topic of AI and language models for identifying and addressing misinformation and fact-checking.'}}},\n",
       " 'http://arxiv.org/abs/2312.07250': {'id': 'http://arxiv.org/abs/2312.07250',\n",
       "  'title': 'Neural Machine Translation of Clinical Text: An Empirical Investigation into Multilingual Pre-Trained Language Models and Transfer-Learning. (arXiv:2312.07250v1 [cs.CL])',\n",
       "  'abstract': 'We conduct investigations on clinical text machine translation by examining\\nmultilingual neural network models using deep learning such as Transformer\\nbased structures. Furthermore, to address the language resource imbalance\\nissue, we also carry out experiments using a transfer learning methodology\\nbased on massive multilingual pre-trained language models (MMPLMs). The\\nexperimental results on three subtasks including 1) clinical case (CC), 2)\\nclinical terminology (CT), and 3) ontological concept (OC) show that our models\\nachieved top-level performances in the ClinSpEn-2022 shared task on\\nEnglish-Spanish clinical domain data. Furthermore, our expert-based human\\nevaluations demonstrate that the small-sized pre-trained language model (PLM)\\nwon over the other two extra-large language models by a large margin, in the\\nclinical domain fine-tuning, which finding was never reported in the field.\\nFinally, the transfer learning method works well in our experimental setting\\nusing the WMT21fb model to accommodate a new language space Spanish that was\\nnot seen at the pre-training stage within WMT21fb itself, which deserves more\\nexploitation for clinical knowledge transformation, e.g. to investigate into\\nmore languages. These research findings can shed some light on domain-specific\\nmachine translation development, especially in clinical and healthcare fields.\\nFurther research projects can be carried out based on our work to improve\\nhealthcare text analytics and knowledge transformation.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.07250',\n",
       "  'authors': ['Lifeng Han, Serge Gladkoff, Gleb Erofeev, Irina Sorokina, Betty Galiano, Goran Nenadic'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': 'The paper does not mention any content related to the security of AI and language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': 'The paper focuses on clinical text machine translation and transfer learning for clinical domain data, which is not directly related to social science research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': 'The paper does not discuss the simulation of humans using AI in various contexts.'},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': 'The paper does not address methods to increase the factuality of language model response.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': 'The paper does not explore AI and language models for generating misinformation or fact-checking.'}}},\n",
       " 'http://arxiv.org/abs/2312.07254': {'id': 'http://arxiv.org/abs/2312.07254',\n",
       "  'title': 'The GUA-Speech System Description for CNVSRC Challenge 2023. (arXiv:2312.07254v1 [cs.CL])',\n",
       "  'abstract': 'This study describes our system for Task 1 Single-speaker Visual Speech\\nRecognition (VSR) fixed track in the Chinese Continuous Visual Speech\\nRecognition Challenge (CNVSRC) 2023. Specifically, we use intermediate\\nconnectionist temporal classification (Inter CTC) residual modules to relax the\\nconditional independence assumption of CTC in our model. Then we use a\\nbi-transformer decoder to enable the model to capture both past and future\\ncontextual information. In addition, we use Chinese characters as the modeling\\nunits to improve the recognition accuracy of our model. Finally, we use a\\nrecurrent neural network language model (RNNLM) for shallow fusion in the\\ninference stage. Experiments show that our system achieves a character error\\nrate (CER) of 38.09% on the Eval set which reaches a relative CER reduction of\\n21.63% over the official baseline, and obtains a second place in the challenge.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.07254',\n",
       "  'authors': ['Shengqiang Li, Chao Lei, Baozhong Ma, Binbin Zhang, Fuping Pan'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.07255': {'id': 'http://arxiv.org/abs/2312.07255',\n",
       "  'title': 'GIST: Improving Parameter Efficient Fine Tuning via Knowledge Interaction. (arXiv:2312.07255v1 [cs.CL])',\n",
       "  'abstract': 'The Parameter-Efficient Fine-Tuning (PEFT) method, which adjusts or\\nintroduces fewer trainable parameters to calibrate pre-trained models on\\ndownstream tasks, has become a recent research interest. However, existing PEFT\\nmethods within the traditional fine-tiuning framework have two main\\nshortcomings: 1) They overlook the explicit association between trainable\\nparameters and downstream task knowledge. 2) They neglect the interaction\\nbetween the intrinsic task-agnostic knowledge of pre-trained models and the\\ntask-specific knowledge in downstream tasks. To address this gap, we propose a\\nnovel fine-tuning framework, named GIST, in a plug-and-play manner.\\nSpecifically, our framework first introduces a trainable token, called the Gist\\ntoken, when applying PEFT methods on downstream tasks. This token serves as an\\naggregator of the task-specific knowledge learned by the PEFT methods and forms\\nan explicit association with downstream knowledge. Furthermore, to facilitate\\nexplicit interaction between task-agnostic and task-specific knowledge, we\\nintroduce the concept of Knowledge Interaction via a Bidirectional\\nKullback-Leibler Divergence objective. As a result, PEFT methods within our\\nframework can make the pre-trained model understand downstream tasks more\\ncomprehensively by leveraging the knowledge interaction. Extensive experiments\\ndemonstrate the universality and scalability of our framework. Notably, on the\\nVTAB-1K benchmark, we employ the Adapter (a prevalent PEFT method) within our\\nGIST framework and achieve a performance boost of 2.25%, with an increase of\\nonly 0.8K parameters. The Code will be released.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.07255',\n",
       "  'authors': ['Jiacheng Ruan, Jingsheng Gao, Mingye Xie, Suncheng Xiang, Zefang Yu, Ting Liu, Yuzhuo Fu'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.8,\n",
       "    'reason': 'The paper focuses on improving parameter-efficient fine-tuning methods for pre-trained models on downstream tasks, which indirectly relates to the quality and factuality of language model response.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.7,\n",
       "    'reason': 'While the paper does not directly mention generating misinformation or fact-checking, it discusses the interaction between task-agnostic and task-specific knowledge, which could potentially impact the generation of misinformation or fact-checking using language models.'}}},\n",
       " 'http://arxiv.org/abs/2312.07280': {'id': 'http://arxiv.org/abs/2312.07280',\n",
       "  'title': 'Towards Equipping Transformer with the Ability of Systematic Compositionality. (arXiv:2312.07280v1 [cs.CL])',\n",
       "  'abstract': 'One of the key factors in language productivity and human cognition is the\\nability of systematic compositionality, which refers to understanding composed\\nunseen examples of seen primitives. However, recent evidence reveals that the\\nTransformers have difficulty generalizing the composed context based on the\\nseen primitives. To this end, we take the first step to propose a\\ncompositionality-aware Transformer called CAT and two novel pre-training tasks\\nto facilitate systematic compositionality. We tentatively provide a successful\\nimplementation of a multi-layer CAT on the basis of the especially popular\\nBERT. The experimental results demonstrate that CAT outperforms baselines on\\ncompositionality-aware tasks with minimal impact on the effectiveness on\\nstandardized language understanding tasks.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.07280',\n",
       "  'authors': ['Chen Huang, Peixin Qin, Wenqiang Lei, Jiancheng Lv'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.6,\n",
       "    'reason': 'The paper does not mention social science research specifically, but it does discuss the ability of systematic compositionality in language, which could have implications for natural language processing in social science research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0.7,\n",
       "    'reason': 'The paper addresses the ability of systematic compositionality in language, which is relevant to the simulation of human language understanding and compositionality in various contexts.'},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.07305': {'id': 'http://arxiv.org/abs/2312.07305',\n",
       "  'title': 'SCCA: Shifted Cross Chunk Attention for long contextual semantic expansion. (arXiv:2312.07305v1 [cs.CL])',\n",
       "  'abstract': 'Sparse attention as a efficient method can significantly decrease the\\ncomputation cost, but current sparse attention tend to rely on window self\\nattention which block the global information flow. For this problem, we present\\nShifted Cross Chunk Attention (SCCA), using different KV shifting strategy to\\nextend respective field in each attention layer. Except, we combine Dilated\\nAttention(DA) and Dilated Neighborhood Attention(DNA) to present Shifted\\nDilated Attention(SDA). Both SCCA and SDA can accumulate attention results in\\nmulti head attention to obtain approximate respective field in full attention.\\nIn this paper, we conduct language modeling experiments using different pattern\\nof SCCA and combination of SCCA and SDA. The proposed shifted cross chunk\\nattention (SCCA) can effectively extend large language models (LLMs) to longer\\ncontext combined with Positional interpolation(PI) and LoRA than current sparse\\nattention. Notably, SCCA adopts LLaMA2 7B from 4k context to 8k in single V100.\\nThis attention pattern can provide a Plug-and-play fine-tuning method to extend\\nmodel context while retaining their original architectures, and is compatible\\nwith most existing techniques.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.07305',\n",
       "  'authors': ['Yuxiang Guo'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.8,\n",
       "    'reason': 'The paper introduces Shifted Cross Chunk Attention (SCCA), which is aimed at extending large language models to longer contexts. While it does not directly mention factuality, the concept of extending context can potentially impact the factuality of language model responses.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.07338': {'id': 'http://arxiv.org/abs/2312.07338',\n",
       "  'title': 'Self-supervised Adaptive Pre-training of Multilingual Speech Models for Language and Dialect Identification. (arXiv:2312.07338v1 [cs.CL])',\n",
       "  'abstract': 'Pre-trained Transformer-based speech models have shown striking performance\\nwhen fine-tuned on various downstream tasks such as automatic speech\\nrecognition and spoken language identification (SLID). However, the problem of\\ndomain mismatch remains a challenge in this area, where the domain of the\\npre-training data might differ from that of the downstream labeled data used\\nfor fine-tuning. In multilingual tasks such as SLID, the pre-trained speech\\nmodel may not support all the languages in the downstream task. To address this\\nchallenge, we propose self-supervised adaptive pre-training (SAPT) to adapt the\\npre-trained model to the target domain and languages of the downstream task. We\\napply SAPT to the XLSR-128 model and investigate the effectiveness of this\\napproach for the SLID task. First, we demonstrate that SAPT improves XLSR\\nperformance on the FLEURS benchmark with substantial gains up to 40.1% for\\nunder-represented languages. Second, we apply SAPT on four different datasets\\nin a few-shot learning setting, showing that our approach improves the sample\\nefficiency of XLSR during fine-tuning. Our experiments provide strong empirical\\nevidence that continual adaptation via self-supervision improves downstream\\nperformance for multilingual speech models.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.07338',\n",
       "  'authors': ['Mohammed Maqsood Shaik, Dietrich Klakow, Badr M. Abdullah'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.3,\n",
       "    'reason': 'The paper does not directly mention applications in social science research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.8,\n",
       "    'reason': 'The paper discusses methods to improve downstream performance for multilingual speech models, which indirectly relates to increasing the factuality of language model responses.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.07395': {'id': 'http://arxiv.org/abs/2312.07395',\n",
       "  'title': 'A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames. (arXiv:2312.07395v1 [cs.CV])',\n",
       "  'abstract': 'Understanding long, real-world videos requires modeling of long-range visual\\ndependencies. To this end, we explore video-first architectures, building on\\nthe common paradigm of transferring large-scale, image--text models to video\\nvia shallow temporal fusion. However, we expose two limitations to the\\napproach: (1) decreased spatial capabilities, likely due to poor\\nvideo--language alignment in standard video datasets, and (2) higher memory\\nconsumption, bottlenecking the number of frames that can be processed. To\\nmitigate the memory bottleneck, we systematically analyze the memory/accuracy\\ntrade-off of various efficient methods: factorized attention,\\nparameter-efficient image-to-video adaptation, input masking, and\\nmulti-resolution patchification. Surprisingly, simply masking large portions of\\nthe video (up to 75%) during contrastive pre-training proves to be one of the\\nmost robust ways to scale encoders to videos up to 4.3 minutes at 1 FPS. Our\\nsimple approach for training long video-to-text models, which scales to 1B\\nparameters, does not add new architectural complexity and is able to outperform\\nthe popular paradigm of using much larger LLMs as an information aggregator\\nover segment-based information on benchmarks with long-range temporal\\ndependencies (YouCook2, EgoSchema).\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.07395',\n",
       "  'authors': ['Pinelopi Papalampidi, Skanda Koppula, Shreya Pathak, Justin Chiu, Joe Heyward, Viorica Patraucean, Jiajun Shen, Antoine Miech, Andrew Zisserman, Aida Nematzdeh'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0.8,\n",
       "    'reason': 'The paper does not explicitly mention simulating humans, but it discusses scaling encoders for long video-to-text models, which could be relevant to simulating human understanding of long videos.'},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.07398': {'id': 'http://arxiv.org/abs/2312.07398',\n",
       "  'title': 'LLMEval: A Preliminary Study on How to Evaluate Large Language Models. (arXiv:2312.07398v1 [cs.AI])',\n",
       "  'abstract': \"Recently, the evaluation of Large Language Models has emerged as a popular\\narea of research. The three crucial questions for LLM evaluation are ``what,\\nwhere, and how to evaluate''. However, the existing research mainly focuses on\\nthe first two questions, which are basically what tasks to give the LLM during\\ntesting and what kind of knowledge it should deal with. As for the third\\nquestion, which is about what standards to use, the types of evaluators, how to\\nscore, and how to rank, there hasn't been much discussion. In this paper, we\\nanalyze evaluation methods by comparing various criteria with both manual and\\nautomatic evaluation, utilizing onsite, crowd-sourcing, public annotators and\\nGPT-4, with different scoring methods and ranking systems. We propose a new\\ndataset, LLMEval and conduct evaluations on 20 LLMs. A total of 2,186\\nindividuals participated, leading to the generation of 243,337 manual\\nannotations and 57,511 automatic evaluation results. We perform comparisons and\\nanalyses of different settings and conduct 10 conclusions that can provide some\\ninsights for evaluating LLM in the future. The dataset and the results are\\npublicly available at https://github.com/llmeval .\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2312.07398',\n",
       "  'authors': ['Yue Zhang, Ming Zhang, Haipeng Yuan, Shichun Liu, Yongyao Shi, Tao Gui, Qi Zhang, Xuanjing Huang'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0.6,\n",
       "    'reason': 'The paper discusses the evaluation of Large Language Models, which could indirectly relate to security considerations in AI and language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.3,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.3,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.2,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.07399': {'id': 'http://arxiv.org/abs/2312.07399',\n",
       "  'title': 'Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales. (arXiv:2312.07399v1 [cs.CL])',\n",
       "  'abstract': \"Machine reasoning has made great progress in recent years owing to large\\nlanguage models (LLMs). In the clinical domain, however, most NLP-driven\\nprojects mainly focus on clinical classification or reading comprehension, and\\nunder-explore clinical reasoning for disease diagnosis due to the expensive\\nrationale annotation with clinicians. In this work, we present a\\n``reasoning-aware'' diagnosis framework that rationalizes the diagnostic\\nprocess via prompt-based learning in a time- and labor-efficient manner, and\\nlearns to reason over the prompt-generated rationales. Specifically, we address\\nthe clinical reasoning for disease diagnosis, where the LLM generates\\ndiagnostic rationales providing its insight on presented patient data and the\\nreasoning path towards the diagnosis, namely Clinical Chain-of-Thought\\n(Clinical CoT). We empirically demonstrate LLMs/LMs' ability of clinical\\nreasoning via extensive experiments and analyses on both rationale generation\\nand disease diagnosis in various settings. We further propose a novel set of\\ncriteria for evaluating machine-generated rationales' potential for real-world\\nclinical settings, facilitating and benefiting future research in this area.\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2312.07399',\n",
       "  'authors': ['Taeyoon Kwon, Kai Tzu-iunn Ong, Dongjin Kang, Seungjun Moon, Jeong Ryong Lee, Dosik Hwang, Yongsik Sim, Beomseok Sohn, Dongha Lee, Jinyoung Yeo'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0.2,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.3,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0.3,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.8,\n",
       "    'reason': 'The paper discusses the diagnostic process via prompt-based learning in a time- and labor-efficient manner, which could indirectly relate to improving the factuality of language model responses.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.7,\n",
       "    'reason': \"The paper proposes a novel set of criteria for evaluating machine-generated rationales' potential for real-world clinical settings, which could contribute to the development of methods for fact-checking in the context of AI and language models.\"}}},\n",
       " 'http://arxiv.org/abs/2312.07405': {'id': 'http://arxiv.org/abs/2312.07405',\n",
       "  'title': 'ICL Markup: Structuring In-Context Learning using Soft-Token Tags. (arXiv:2312.07405v1 [cs.CL])',\n",
       "  'abstract': \"Large pretrained language models (LLMs) can be rapidly adapted to a wide\\nvariety of tasks via a text-to-text approach, where the instruction and input\\nare fed to the model in natural language. Combined with in-context learning\\n(ICL), this paradigm is impressively flexible and powerful. However, it also\\nburdens users with an overwhelming number of choices, many of them arbitrary.\\nInspired by markup languages like HTML, we contribute a method of using\\nsoft-token tags to compose prompt templates. This approach reduces arbitrary\\ndecisions and streamlines the application of ICL. Our method is a form of\\nmeta-learning for ICL; it learns these tags in advance during a\\nparameter-efficient fine-tuning ``warm-up'' process. The tags can subsequently\\nbe used in templates for ICL on new, unseen tasks without any additional\\nfine-tuning. Our experiments with this approach yield promising initial\\nresults, improving LLM performance on important enterprise applications such as\\nfew-shot and open-world intent detection, as well as text classification in\\nnews and legal domains.\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2312.07405',\n",
       "  'authors': ['Marc-Etienne Brunet, Ashton Anderson, Richard Zemel'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.3,\n",
       "    'reason': 'The paper does not directly mention social science research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0.4,\n",
       "    'reason': 'The paper does not directly mention simulating humans in various contexts.'},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.7,\n",
       "    'reason': 'The paper discusses reducing arbitrary decisions and improving LLM performance, which may indirectly relate to factuality of language model response.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.6,\n",
       "    'reason': 'The paper discusses improving LLM performance, which could indirectly impact fact-checking, but does not directly address generating misinformation or fact-checking.'}}},\n",
       " 'http://arxiv.org/abs/2312.07419': {'id': 'http://arxiv.org/abs/2312.07419',\n",
       "  'title': 'Towards Faster k-Nearest-Neighbor Machine Translation. (arXiv:2312.07419v1 [cs.CL])',\n",
       "  'abstract': 'Recent works have proven the effectiveness of k-nearest-neighbor machine\\ntranslation(a.k.a kNN-MT) approaches to produce remarkable improvement in\\ncross-domain translations. However, these models suffer from heavy retrieve\\noverhead on the entire datastore when decoding each token. We observe that\\nduring the decoding phase, about 67% to 84% of tokens are unvaried after\\nsearching over the corpus datastore, which means most of the tokens cause\\nfutile retrievals and introduce unnecessary computational costs by initiating\\nk-nearest-neighbor searches. We consider this phenomenon is explainable in\\nlinguistics and propose a simple yet effective multi-layer perceptron (MLP)\\nnetwork to predict whether a token should be translated jointly by the neural\\nmachine translation model and probabilities produced by the kNN or just by the\\nneural model. The results show that our method succeeds in reducing redundant\\nretrieval operations and significantly reduces the overhead of kNN retrievals\\nby up to 53% at the expense of a slight decline in translation quality.\\nMoreover, our method could work together with all existing kNN-MT systems.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.07419',\n",
       "  'authors': ['Xiangyu Shi, Yunlong Liang, Jinan Xu, Yufeng Chen'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': 'The paper does not directly mention any aspects related to the security of AI and language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': 'The paper does not directly mention any applications of AI and language models in social science research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': 'The paper does not directly mention the use of AI to simulate humans in various contexts.'},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.7,\n",
       "    'reason': 'The paper is relevant to this topic as it discusses methods to improve the efficiency of k-nearest-neighbor machine translation and mentions a slight decline in translation quality as a trade-off for reducing redundant retrieval operations.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.5,\n",
       "    'reason': 'The paper does not directly mention AI and language models for generating misinformation or fact-checking, but it touches upon the optimization of machine translation, which could indirectly relate to the generation of misinformation or fact-checking in language models.'}}},\n",
       " 'http://arxiv.org/abs/2312.07435': {'id': 'http://arxiv.org/abs/2312.07435',\n",
       "  'title': 'Cross-modal Contrastive Learning with Asymmetric Co-attention Network for Video Moment Retrieval. (arXiv:2312.07435v1 [cs.CV])',\n",
       "  'abstract': 'Video moment retrieval is a challenging task requiring fine-grained\\ninteractions between video and text modalities. Recent work in image-text\\npretraining has demonstrated that most existing pretrained models suffer from\\ninformation asymmetry due to the difference in length between visual and\\ntextual sequences. We question whether the same problem also exists in the\\nvideo-text domain with an auxiliary need to preserve both spatial and temporal\\ninformation. Thus, we evaluate a recently proposed solution involving the\\naddition of an asymmetric co-attention network for video grounding tasks.\\nAdditionally, we incorporate momentum contrastive loss for robust,\\ndiscriminative representation learning in both modalities. We note that the\\nintegration of these supplementary modules yields better performance compared\\nto state-of-the-art models on the TACoS dataset and comparable results on\\nActivityNet Captions, all while utilizing significantly fewer parameters with\\nrespect to baseline.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.07435',\n",
       "  'authors': ['Love Panta, Prashant Shrestha, Brabeem Sapkota, Amrita Bhattarai, Suresh Manandhar, Anand Kumar Sah'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0.8,\n",
       "    'reason': 'The paper does not directly mention simulating humans, but it focuses on cross-modal contrastive learning for video moment retrieval, which involves understanding human activities in videos using AI techniques.'},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.07476': {'id': 'http://arxiv.org/abs/2312.07476',\n",
       "  'title': 'Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection. (arXiv:2312.07476v1 [cs.CL])',\n",
       "  'abstract': \"In-Context Learning (ICL) is an important paradigm for adapting Large\\nLanguage Models (LLMs) to downstream tasks through a few demonstrations.\\nDespite the great success of ICL, the limitation of the demonstration number\\nmay lead to demonstration bias, i.e. the input-label mapping induced by LLMs\\nmisunderstands the task's essence. Inspired by human experience, we attempt to\\nmitigate such bias through the perspective of the inter-demonstration\\nrelationship. Specifically, we construct Comparable Demonstrations (CDs) by\\nminimally editing the texts to flip the corresponding labels, in order to\\nhighlight the task's essence and eliminate potential spurious correlations\\nthrough the inter-demonstration comparison. Through a series of experiments on\\nCDs, we find that (1) demonstration bias does exist in LLMs, and CDs can\\nsignificantly reduce such bias; (2) CDs exhibit good performance in ICL,\\nespecially in out-of-distribution scenarios. In summary, this study explores\\nthe ICL mechanisms from a novel perspective, providing a deeper insight into\\nthe demonstration selection strategy for ICL.\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2312.07476',\n",
       "  'authors': ['Caoyun Fan, Jidong Tian, Yitian Li, Hao He, Yaohui Jin'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': 'The paper does not directly mention security aspects of AI and language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.7,\n",
       "    'reason': 'The paper does not explicitly mention social science, but it discusses the adaptation of Large Language Models (LLMs) to downstream tasks, which could potentially have applications in social science research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0.7,\n",
       "    'reason': 'The paper talks about mitigating bias in language models through the perspective of inter-demonstration relationships, which could relate to the simulation of human behavior in various contexts.'},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': 'The paper does not directly address methods to increase the factuality of language model responses.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': 'The paper does not discuss the generation of misinformation or fact-checking.'}}},\n",
       " 'http://arxiv.org/abs/2210.01959': {'id': 'http://arxiv.org/abs/2210.01959',\n",
       "  'title': 'Detect, Retrieve, Comprehend: A Flexible Framework for Zero-Shot Document-Level Question Answering. (arXiv:2210.01959v3 [cs.CL] UPDATED)',\n",
       "  'abstract': 'Researchers produce thousands of scholarly documents containing valuable\\ntechnical knowledge. The community faces the laborious task of reading these\\ndocuments to identify, extract, and synthesize information. To automate\\ninformation gathering, document-level question answering (QA) offers a flexible\\nframework where human-posed questions can be adapted to extract diverse\\nknowledge. Finetuning QA systems requires access to labeled data (tuples of\\ncontext, question and answer). However, data curation for document QA is\\nuniquely challenging because the context (i.e. answer evidence passage) needs\\nto be retrieved from potentially long, ill-formatted documents. Existing QA\\ndatasets sidestep this challenge by providing short, well-defined contexts that\\nare unrealistic in real-world applications. We present a three-stage document\\nQA approach: (1) text extraction from PDF; (2) evidence retrieval from\\nextracted texts to form well-posed contexts; (3) QA to extract knowledge from\\ncontexts to return high-quality answers -- extractive, abstractive, or Boolean.\\nUsing QASPER for evaluation, our detect-retrieve-comprehend (DRC) system\\nachieves a +7.19 improvement in Answer-F1 over existing baselines while\\ndelivering superior context selection. Our results demonstrate that DRC holds\\ntremendous promise as a flexible framework for practical scientific document\\nQA.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2210.01959',\n",
       "  'authors': ['Tavish McDonald, Brian Tsan, Amar Saini, Juanita Ordonez, Luis Gutierrez, Phan Nguyen, Blake Mason, Brenda Ng'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.8,\n",
       "    'reason': 'The paper does not explicitly mention social science research, but it discusses the automation of information gathering and document-level question answering, which could be applied in social science research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.9,\n",
       "    'reason': 'The paper directly addresses the challenge of extracting well-posed contexts from potentially long, ill-formatted documents, which is relevant to increasing the factuality of language model responses.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.4,\n",
       "    'reason': 'While the paper does not explicitly address misinformation or fact-checking, it discusses QA to extract knowledge from contexts, which could be related to fact-checking applications.'}}},\n",
       " 'http://arxiv.org/abs/2210.09932': {'id': 'http://arxiv.org/abs/2210.09932',\n",
       "  'title': 'Making Science Simple: Corpora for the Lay Summarisation of Scientific Literature. (arXiv:2210.09932v2 [cs.CL] UPDATED)',\n",
       "  'abstract': 'Lay summarisation aims to jointly summarise and simplify a given text, thus\\nmaking its content more comprehensible to non-experts. Automatic approaches for\\nlay summarisation can provide significant value in broadening access to\\nscientific literature, enabling a greater degree of both interdisciplinary\\nknowledge sharing and public understanding when it comes to research findings.\\nHowever, current corpora for this task are limited in their size and scope,\\nhindering the development of broadly applicable data-driven approaches. Aiming\\nto rectify these issues, we present two novel lay summarisation datasets, PLOS\\n(large-scale) and eLife (medium-scale), each of which contains biomedical\\njournal articles alongside expert-written lay summaries. We provide a thorough\\ncharacterisation of our lay summaries, highlighting differing levels of\\nreadability and abstractiveness between datasets that can be leveraged to\\nsupport the needs of different applications. Finally, we benchmark our datasets\\nusing mainstream summarisation approaches and perform a manual evaluation with\\ndomain experts, demonstrating their utility and casting light on the key\\nchallenges of this task.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2210.09932',\n",
       "  'authors': ['Tomas Goldsack, Zhihao Zhang, Chenghua Lin, Carolina Scarton'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.8,\n",
       "    'reason': 'The paper is not directly focused on social science research, but it discusses the broadening access to scientific literature, which could indirectly relate to the applications of AI and language models in the context of social science research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.8,\n",
       "    'reason': 'The paper discusses the value of automatic approaches for lay summarisation in broadening access to scientific literature, which indirectly relates to the methods to increase the factuality of language model response.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.6,\n",
       "    'reason': 'The paper mentions benchmarking datasets using mainstream summarisation approaches and performing a manual evaluation with domain experts, which could relate to the broader context of AI and language models for fact-checking. However, it does not directly focus on generating misinformation or fact-checking.'}}},\n",
       " 'http://arxiv.org/abs/2212.13201': {'id': 'http://arxiv.org/abs/2212.13201',\n",
       "  'title': 'Highlighting Named Entities in Input for Auto-Formulation of Optimization Problems. (arXiv:2212.13201v3 [cs.CL] UPDATED)',\n",
       "  'abstract': 'Operations research deals with modeling and solving real-world problems as\\nmathematical optimization problems. While solving mathematical systems is\\naccomplished by analytical software, formulating a problem as a set of\\nmathematical operations has been typically done manually by domain experts.\\nRecent machine learning methods have shown promise in converting textual\\nproblem descriptions to corresponding mathematical formulations. This paper\\npresents an approach that converts linear programming word problems into\\nmathematical formulations. We leverage the named entities in the input and\\naugment the input to highlight these entities. Our approach achieves the\\nhighest accuracy among all submissions to the NL4Opt Competition, securing\\nfirst place in the generation track.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2212.13201',\n",
       "  'authors': ['Neeraj Gangwar, Nickvash Kani'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.9,\n",
       "    'reason': 'The paper presents an approach that converts linear programming word problems into mathematical formulations, which can be applied in social science research where optimization problems are often encountered.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2302.11091': {'id': 'http://arxiv.org/abs/2302.11091',\n",
       "  'title': 'GTRL: An Entity Group-Aware Temporal Knowledge Graph Representation Learning Method. (arXiv:2302.11091v2 [cs.LG] UPDATED)',\n",
       "  'abstract': 'Temporal Knowledge Graph (TKG) representation learning embeds entities and\\nevent types into a continuous low-dimensional vector space by integrating the\\ntemporal information, which is essential for downstream tasks, e.g., event\\nprediction and question answering. Existing methods stack multiple graph\\nconvolution layers to model the influence of distant entities, leading to the\\nover-smoothing problem. To alleviate the problem, recent studies infuse\\nreinforcement learning to obtain paths that contribute to modeling the\\ninfluence of distant entities. However, due to the limited number of hops,\\nthese studies fail to capture the correlation between entities that are far\\napart and even unreachable. To this end, we propose GTRL, an entity Group-aware\\nTemporal knowledge graph Representation Learning method. GTRL is the first work\\nthat incorporates the entity group modeling to capture the correlation between\\nentities by stacking only a finite number of layers. Specifically, the entity\\ngroup mapper is proposed to generate entity groups from entities in a learning\\nway. Based on entity groups, the implicit correlation encoder is introduced to\\ncapture implicit correlations between any pairwise entity groups. In addition,\\nthe hierarchical GCNs are exploited to accomplish the message aggregation and\\nrepresentation updating on the entity group graph and the entity graph.\\nFinally, GRUs are employed to capture the temporal dependency in TKGs.\\nExtensive experiments on three real-world datasets demonstrate that GTRL\\nachieves the state-of-the-art performances on the event prediction task,\\noutperforming the best baseline by an average of 13.44%, 9.65%, 12.15%, and\\n15.12% in MRR, Hits@1, Hits@3, and Hits@10, respectively.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2302.11091',\n",
       "  'authors': ['Xing Tang, Ling Chen'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.1,\n",
       "    'reason': 'The paper is focused on temporal knowledge graph representation learning and event prediction, which does not directly pertain to methods for increasing the factuality of language model response.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2303.00855': {'id': 'http://arxiv.org/abs/2303.00855',\n",
       "  'title': 'Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents. (arXiv:2303.00855v2 [cs.RO] UPDATED)',\n",
       "  'abstract': \"Recent progress in large language models (LLMs) has demonstrated the ability\\nto learn and leverage Internet-scale knowledge through pre-training with\\nautoregressive models. Unfortunately, applying such models to settings with\\nembodied agents, such as robots, is challenging due to their lack of experience\\nwith the physical world, inability to parse non-language observations, and\\nignorance of rewards or safety constraints that robots may require. On the\\nother hand, language-conditioned robotic policies that learn from interaction\\ndata can provide the necessary grounding that allows the agent to be correctly\\nsituated in the real world, but such policies are limited by the lack of\\nhigh-level semantic understanding due to the limited breadth of the interaction\\ndata available for training them. Thus, if we want to make use of the semantic\\nknowledge in a language model while still situating it in an embodied setting,\\nwe must construct an action sequence that is both likely according to the\\nlanguage model and also realizable according to grounded models of the\\nenvironment. We frame this as a problem similar to probabilistic filtering:\\ndecode a sequence that both has high probability under the language model and\\nhigh probability under a set of grounded model objectives. We demonstrate how\\nsuch grounded models can be obtained across three simulation and real-world\\ndomains, and that the proposed decoding strategy is able to solve complex,\\nlong-horizon embodiment tasks in a robotic setting by leveraging the knowledge\\nof both models. The project's website can be found at\\ngrounded-decoding.github.io.\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2303.00855',\n",
       "  'authors': ['Wenlong Huang, Fei Xia, Dhruv Shah, Danny Driess, Andy Zeng, Yao Lu, Pete Florence, Igor Mordatch, Sergey Levine, Karol Hausman, Brian Ichter'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 1,\n",
       "    'reason': 'The paper discusses using language-conditioned robotic policies to provide grounding for embodied agents, which can be considered as a form of simulating humans in various contexts.'},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.8,\n",
       "    'reason': 'The paper addresses the challenge of constructing an action sequence that is both likely according to the language model and also realizable according to grounded models of the environment. This indirectly relates to ensuring the factuality of language model response.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2304.07699': {'id': 'http://arxiv.org/abs/2304.07699',\n",
       "  'title': 'A Clustering Framework for Unsupervised and Semi-supervised New Intent Discovery. (arXiv:2304.07699v2 [cs.CL] UPDATED)',\n",
       "  'abstract': 'New intent discovery is of great value to natural language processing,\\nallowing for a better understanding of user needs and providing friendly\\nservices. However, most existing methods struggle to capture the complicated\\nsemantics of discrete text representations when limited or no prior knowledge\\nof labeled data is available. To tackle this problem, we propose a novel\\nclustering framework, USNID, for unsupervised and semi-supervised new intent\\ndiscovery, which has three key technologies. First, it fully utilizes\\nunsupervised or semi-supervised data to mine shallow semantic similarity\\nrelations and provide well-initialized representations for clustering. Second,\\nit designs a centroid-guided clustering mechanism to address the issue of\\ncluster allocation inconsistency and provide high-quality self-supervised\\ntargets for representation learning. Third, it captures high-level semantics in\\nunsupervised or semi-supervised data to discover fine-grained intent-wise\\nclusters by optimizing both cluster-level and instance-level objectives. We\\nalso propose an effective method for estimating the cluster number in\\nopen-world scenarios without knowing the number of new intents beforehand.\\nUSNID performs exceptionally well on several benchmark intent datasets,\\nachieving new state-of-the-art results in unsupervised and semi-supervised new\\nintent discovery and demonstrating robust performance with different cluster\\nnumbers.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2304.07699',\n",
       "  'authors': ['Hanlei Zhang, Hua Xu, Xin Wang, Fei Long, Kai Gao'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.8,\n",
       "    'reason': 'The paper introduces a novel clustering framework, USNID, for unsupervised and semi-supervised new intent discovery, which contributes to the field of language models and natural language processing. While it does not directly mention misinformation or fact-checking, it addresses the broader topic of intent discovery, which is related to understanding user needs and providing friendly services.'}}},\n",
       " 'http://arxiv.org/abs/2305.07402': {'id': 'http://arxiv.org/abs/2305.07402',\n",
       "  'title': 'Synergistic Interplay between Search and Large Language Models for Information Retrieval. (arXiv:2305.07402v3 [cs.CL] UPDATED)',\n",
       "  'abstract': 'Information retrieval (IR) plays a crucial role in locating relevant\\nresources from vast amounts of data, and its applications have evolved from\\ntraditional knowledge bases to modern retrieval models (RMs). The emergence of\\nlarge language models (LLMs) has further revolutionized the IR field by\\nenabling users to interact with search systems in natural languages. In this\\npaper, we explore the advantages and disadvantages of LLMs and RMs,\\nhighlighting their respective strengths in understanding user-issued queries\\nand retrieving up-to-date information. To leverage the benefits of both\\nparadigms while circumventing their limitations, we propose InteR, a novel\\nframework that facilitates information refinement through synergy between RMs\\nand LLMs. InteR allows RMs to expand knowledge in queries using LLM-generated\\nknowledge collections and enables LLMs to enhance prompt formulation using\\nretrieved documents. This iterative refinement process augments the inputs of\\nRMs and LLMs, leading to more accurate retrieval. Experiments on large-scale\\nretrieval benchmarks involving web search and low-resource retrieval tasks\\ndemonstrate that InteR achieves overall superior zero-shot retrieval\\nperformance compared to state-of-the-art methods, even those using relevance\\njudgment. Source code is available at https://github.com/Cyril-JZ/InteR\\n',\n",
       "  'url': 'http://arxiv.org/abs/2305.07402',\n",
       "  'authors': ['Jiazhan Feng, Chongyang Tao, Xiubo Geng, Tao Shen, Can Xu, Guodong Long, Dongyan Zhao, Daxin Jiang'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.8,\n",
       "    'reason': 'The paper discusses the evolution of information retrieval models, including large language models, which could potentially have applications in social science research for retrieving and analyzing large amounts of data.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.7,\n",
       "    'reason': 'The paper explores the advantages and disadvantages of large language models and retrieval models, with a focus on enhancing prompt formulation and refining information. While not directly about factuality, the discussion of refining information could indirectly relate to increasing factuality of language model responses.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.9,\n",
       "    'reason': 'The paper proposes a framework called InteR, which facilitates information refinement through synergy between retrieval models and large language models. The iterative refinement process aims to enhance prompt formulation and retrieval accuracy, which could potentially be used for fact-checking or mitigating misinformation.'}}},\n",
       " 'http://arxiv.org/abs/2305.15057': {'id': 'http://arxiv.org/abs/2305.15057',\n",
       "  'title': 'Linear-Time Modeling of Linguistic Structure: An Order-Theoretic Perspective. (arXiv:2305.15057v2 [cs.CL] UPDATED)',\n",
       "  'abstract': 'Tasks that model the relation between pairs of tokens in a string are a vital\\npart of understanding natural language. Such tasks, in general, require\\nexhaustive pair-wise comparisons of tokens, thus having a quadratic runtime\\ncomplexity in the length of the string. We show that these exhaustive\\ncomparisons can be avoided, and, moreover, the complexity of such tasks can be\\nreduced to linear by casting the relation between tokens as a partial order\\nover the string. Our method predicts real numbers for each token in a string in\\nparallel and sorts the tokens accordingly, resulting in total orders of the\\ntokens in the string. Each total order implies a set of arcs oriented from\\nsmaller to greater tokens, sorted by their predicted numbers. The intersection\\nof total orders results in a partial order over the set of tokens in the\\nstring, which is then decoded into a directed graph representing the desired\\nlinguistic structure. Our experiments on dependency parsing and coreference\\nresolution show that our method achieves state-of-the-art or comparable\\nperformance. Moreover, the linear complexity and parallelism of our method\\ndouble the speed of graph-based coreference resolution models, and bring a\\n10-times speed-up over graph-based dependency parsers.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2305.15057',\n",
       "  'authors': ['Tianyu Liu, Afra Amini, Mrinmaya Sachan, Ryan Cotterell'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.8,\n",
       "    'reason': 'The paper does not directly mention methods to increase factuality of language model response, but it discusses a method for modeling linguistic structure, which indirectly relates to the broader concept of factuality in language models.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2306.03241': {'id': 'http://arxiv.org/abs/2306.03241',\n",
       "  'title': 'Early Weight Averaging meets High Learning Rates for LLM Pre-training. (arXiv:2306.03241v2 [cs.LG] UPDATED)',\n",
       "  'abstract': 'Training Large Language Models (LLMs) incurs significant cost; hence, any\\nstrategy that accelerates model convergence is helpful. In this paper, we\\ninvestigate the ability of a simple idea checkpoint averaging along the\\ntrajectory of a training run to improve both convergence and generalization\\nquite early on during training. Here we show that models trained with high\\nlearning rates observe higher gains due to checkpoint averaging. Furthermore,\\nthese gains are amplified when checkpoints are sampled with considerable\\nspacing in training steps. Our training recipe outperforms conventional\\ntraining and popular checkpoint averaging baselines such as exponential moving\\naverage (EMA) and stochastic moving average (SWA). We evaluate our training\\nrecipe by pre-training LLMs, where high learning rates are inherently preferred\\ndue to extremely large batch sizes. Specifically, we pre-trained nanoGPT-2\\nmodels of varying sizes, small (125M), medium (335M), and large (770M)on the\\nOpenWebText dataset, comprised of 9B tokens. Additionally, we present results\\nfor publicly available Pythia LLMs, ranging from 1B to 12B, which were trained\\non the PILE-deduped dataset containing 207B tokens.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2306.03241',\n",
       "  'authors': ['Sunny Sanyal, Atula Neerkaje, Jean Kaddour, Abhishek Kumar, Sujay Sanghavi'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.7,\n",
       "    'reason': 'The paper is not directly focused on social science research applications, but it discusses the pre-training of large language models, which could be relevant for certain social science research applications.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.9,\n",
       "    'reason': 'The paper directly addresses the methods to improve convergence and generalization during the training of large language models, which is closely related to increasing the factuality of language model responses.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2306.12424': {'id': 'http://arxiv.org/abs/2306.12424',\n",
       "  'title': 'VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution. (arXiv:2306.12424v3 [cs.CV] UPDATED)',\n",
       "  'abstract': 'We introduce VisoGender, a novel dataset for benchmarking gender bias in\\nvision-language models. We focus on occupation-related biases within a\\nhegemonic system of binary gender, inspired by Winograd and Winogender schemas,\\nwhere each image is associated with a caption containing a pronoun relationship\\nof subjects and objects in the scene. VisoGender is balanced by gender\\nrepresentation in professional roles, supporting bias evaluation in two ways:\\ni) resolution bias, where we evaluate the difference between pronoun resolution\\naccuracies for image subjects with gender presentations perceived as masculine\\nversus feminine by human annotators and ii) retrieval bias, where we compare\\nratios of professionals perceived to have masculine and feminine gender\\npresentations retrieved for a gender-neutral search query. We benchmark several\\nstate-of-the-art vision-language models and find that they demonstrate bias in\\nresolving binary gender in complex scenes. While the direction and magnitude of\\ngender bias depends on the task and the model being evaluated, captioning\\nmodels are generally less biased than Vision-Language Encoders. Dataset and\\ncode are available at https://github.com/oxai/visogender\\n',\n",
       "  'url': 'http://arxiv.org/abs/2306.12424',\n",
       "  'authors': ['Siobhan Mackenzie Hall, Fernanda Gonçalves Abrantes, Hanwen Zhu, Grace Sodunke, Aleksandar Shtedritski, Hannah Rose Kirk'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.9,\n",
       "    'reason': 'The paper introduces a dataset for benchmarking gender bias in vision-language models, which could be relevant for social science research related to bias evaluation and gender representation.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2308.06077': {'id': 'http://arxiv.org/abs/2308.06077',\n",
       "  'title': 'Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling. (arXiv:2308.06077v2 [cs.CL] UPDATED)',\n",
       "  'abstract': 'Generative language models (LMs) have become omnipresent across data science.\\nFor a wide variety of tasks, inputs can be phrased as natural language prompts\\nfor an LM, from whose output the solution can then be extracted. LM performance\\nhas consistently been increasing with model size - but so has the monetary cost\\nof querying the ever larger models. Importantly, however, not all inputs are\\nequally hard: some require larger LMs for obtaining a satisfactory solution,\\nwhereas for others smaller LMs suffice. Based on this fact, we design a\\nframework for Cost-Effective Language Model Choice (CELMOC). Given a set of\\ninputs and a set of candidate LMs, CELMOC judiciously assigns each input to an\\nLM predicted to do well on the input according to a so-called meta-model,\\naiming to achieve high overall performance at low cost. The cost-performance\\ntrade-off can be flexibly tuned by the user. Options include, among others,\\nmaximizing total expected performance (or the number of processed inputs) while\\nstaying within a given cost budget, or minimizing total cost while processing\\nall inputs. We evaluate CELMOC on 14 datasets covering five natural language\\ntasks, using four candidate LMs of vastly different size and cost. With CELMOC,\\nwe match the performance of the largest available LM while achieving a cost\\nreduction of 63%. Via our publicly available library, researchers as well as\\npractitioners can thus save large amounts of money without sacrificing\\nperformance.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2308.06077',\n",
       "  'authors': ['Marija Šakota, Maxime Peyrard, Robert West'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.6,\n",
       "    'reason': 'The paper discusses the judicious assignment of inputs to language models to achieve high overall performance at low cost, which indirectly relates to the concept of factuality in language model response.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.9,\n",
       "    'reason': 'The paper introduces a framework for Cost-Effective Language Model Choice (CELMOC), which can help in saving large amounts of money without sacrificing performance, thus indirectly addressing the use of AI and language models for generating misinformation or fact-checking.'}}},\n",
       " 'http://arxiv.org/abs/2309.07870': {'id': 'http://arxiv.org/abs/2309.07870',\n",
       "  'title': 'Agents: An Open-source Framework for Autonomous Language Agents. (arXiv:2309.07870v3 [cs.CL] UPDATED)',\n",
       "  'abstract': 'Recent advances on large language models (LLMs) enable researchers and\\ndevelopers to build autonomous language agents that can automatically solve\\nvarious tasks and interact with environments, humans, and other agents using\\nnatural language interfaces. We consider language agents as a promising\\ndirection towards artificial general intelligence and release Agents, an\\nopen-source library with the goal of opening up these advances to a wider\\nnon-specialist audience. Agents is carefully engineered to support important\\nfeatures including planning, memory, tool usage, multi-agent communication, and\\nfine-grained symbolic control. Agents is user-friendly as it enables\\nnon-specialists to build, customize, test, tune, and deploy state-of-the-art\\nautonomous language agents without much coding. The library is also\\nresearch-friendly as its modularized design makes it easily extensible for\\nresearchers. Agents is available at https://github.com/aiwaves-cn/agents.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2309.07870',\n",
       "  'authors': ['Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Xiangru Tang, Ningyu Zhang, Huajun Chen, Peng Cui, Mrinmaya Sachan'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.7,\n",
       "    'reason': 'The paper discusses the use of autonomous language agents in various tasks and interactions, which could potentially be applied to social science research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0.9,\n",
       "    'reason': 'The paper mentions building autonomous language agents that interact with environments, humans, and other agents using natural language interfaces, indicating the simulation of human interactions.'},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.1,\n",
       "    'reason': 'The paper focuses on building autonomous language agents for various tasks, but does not specifically discuss generating or fact-checking misinformation.'}}},\n",
       " 'http://arxiv.org/abs/2309.09357': {'id': 'http://arxiv.org/abs/2309.09357',\n",
       "  'title': 'Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model. (arXiv:2309.09357v4 [cs.CL] UPDATED)',\n",
       "  'abstract': \"Despite the plethora of telehealth applications to assist home-based older\\nadults and healthcare providers, basic messaging and phone calls are still the\\nmost common communication methods, which suffer from limited availability,\\ninformation loss, and process inefficiencies. One promising solution to\\nfacilitate patient-provider communication is to leverage large language models\\n(LLMs) with their powerful natural conversation and summarization capability.\\nHowever, there is a limited understanding of LLMs' role during the\\ncommunication. We first conducted two interview studies with both older adults\\n(N=10) and healthcare providers (N=9) to understand their needs and\\nopportunities for LLMs in patient-provider asynchronous communication. Based on\\nthe insights, we built an LLM-powered communication system, Talk2Care, and\\ndesigned interactive components for both groups: (1) For older adults, we\\nleveraged the convenience and accessibility of voice assistants (VAs) and built\\nan LLM-powered VA interface for effective information collection. (2) For\\nhealth providers, we built an LLM-based dashboard to summarize and present\\nimportant health information based on older adults' conversations with the VA.\\nWe further conducted two user studies with older adults and providers to\\nevaluate the usability of the system. The results showed that Talk2Care could\\nfacilitate the communication process, enrich the health information collected\\nfrom older adults, and considerably save providers' efforts and time. We\\nenvision our work as an initial exploration of LLMs' capability in the\\nintersection of healthcare and interpersonal communication.\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2309.09357',\n",
       "  'authors': ['Ziqi Yang, Xuhai Xu, Bingsheng Yao, Shao Zhang, Ethan Rogers, Stephen Intille, Nawar Shara, Guodong Gordon Gao, Dakuo Wang'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.8,\n",
       "    'reason': 'The paper does not directly discuss increasing factuality, but it does leverage large language models to facilitate patient-provider communication, which implies a consideration for accurate and factual information exchange.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2309.13788': {'id': 'http://arxiv.org/abs/2309.13788',\n",
       "  'title': 'Can LLM-Generated Misinformation Be Detected?. (arXiv:2309.13788v2 [cs.CL] UPDATED)',\n",
       "  'abstract': 'The advent of Large Language Models (LLMs) has made a transformative impact.\\nHowever, the potential that LLMs such as ChatGPT can be exploited to generate\\nmisinformation has posed a serious concern to online safety and public trust. A\\nfundamental research question is: will LLM-generated misinformation cause more\\nharm than human-written misinformation? We propose to tackle this question from\\nthe perspective of detection difficulty. We first build a taxonomy of\\nLLM-generated misinformation. Then we categorize and validate the potential\\nreal-world methods for generating misinformation with LLMs. Then, through\\nextensive empirical investigation, we discover that LLM-generated\\nmisinformation can be harder to detect for humans and detectors compared to\\nhuman-written misinformation with the same semantics, which suggests it can\\nhave more deceptive styles and potentially cause more harm. We also discuss the\\nimplications of our discovery on combating misinformation in the age of LLMs\\nand the countermeasures.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2309.13788',\n",
       "  'authors': ['Canyu Chen, Kai Shu'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0.9,\n",
       "    'reason': 'The paper directly addresses the potential exploitation of LLMs to generate misinformation, posing a serious concern to online safety and public trust. It discusses the difficulty in detecting LLM-generated misinformation, which is highly relevant to the security of AI and language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.9,\n",
       "    'reason': 'The paper directly investigates the potential for LLMs, such as ChatGPT, to be exploited to generate misinformation and discusses the implications of LLM-generated misinformation on combating misinformation. It is directly relevant to the topic of AI and language models for generating misinformation.'}}},\n",
       " 'http://arxiv.org/abs/2309.17453': {'id': 'http://arxiv.org/abs/2309.17453',\n",
       "  'title': 'Efficient Streaming Language Models with Attention Sinks. (arXiv:2309.17453v3 [cs.CL] UPDATED)',\n",
       "  'abstract': \"Deploying Large Language Models (LLMs) in streaming applications such as\\nmulti-round dialogue, where long interactions are expected, is urgently needed\\nbut poses two major challenges. Firstly, during the decoding stage, caching\\nprevious tokens' Key and Value states (KV) consumes extensive memory. Secondly,\\npopular LLMs cannot generalize to longer texts than the training sequence\\nlength. Window attention, where only the most recent KVs are cached, is a\\nnatural approach -- but we show that it fails when the text length surpasses\\nthe cache size. We observe an interesting phenomenon, namely attention sink,\\nthat keeping the KV of initial tokens will largely recover the performance of\\nwindow attention. In this paper, we first demonstrate that the emergence of\\nattention sink is due to the strong attention scores towards initial tokens as\\na ``sink'' even if they are not semantically important. Based on the above\\nanalysis, we introduce StreamingLLM, an efficient framework that enables LLMs\\ntrained with a finite length attention window to generalize to infinite\\nsequence lengths without any fine-tuning. We show that StreamingLLM can enable\\nLlama-2, MPT, Falcon, and Pythia to perform stable and efficient language\\nmodeling with up to 4 million tokens and more. In addition, we discover that\\nadding a placeholder token as a dedicated attention sink during pre-training\\ncan further improve streaming deployment. In streaming settings, StreamingLLM\\noutperforms the sliding window recomputation baseline by up to 22.2x speedup.\\nCode and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2309.17453',\n",
       "  'authors': ['Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.8,\n",
       "    'reason': 'The paper discusses the challenges of deploying large language models in streaming applications, which can indirectly relate to the need for factuality in language model response.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2310.14505': {'id': 'http://arxiv.org/abs/2310.14505',\n",
       "  'title': 'Sentiment analysis with adaptive multi-head attention in Transformer. (arXiv:2310.14505v3 [cs.CL] UPDATED)',\n",
       "  'abstract': 'We propose a novel framework based on the attention mechanism to identify the\\nsentiment of a movie review document. Previous efforts on deep neural networks\\nwith attention mechanisms focus on encoder and decoder with fixed numbers of\\nmulti-head attention. Therefore, we need a mechanism to stop the attention\\nprocess automatically if no more useful information can be read from the\\nmemory.In this paper, we propose an adaptive multi-head attention architecture\\n(AdaptAttn) which varies the number of attention heads based on length of\\nsentences. AdaptAttn has a data preprocessing step where each document is\\nclassified into any one of the three bins small, medium or large based on\\nlength of the sentence. The document classified as small goes through two heads\\nin each layer, the medium group passes four heads and the large group is\\nprocessed by eight heads. We examine the merit of our model on the Stanford\\nlarge movie review dataset. The experimental results show that the F1 score\\nfrom our model is on par with the baseline model.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2310.14505',\n",
       "  'authors': ['Fanfei Meng, David Demeter'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.6,\n",
       "    'reason': 'The paper does not directly mention social science research; however, it discusses sentiment analysis of movie reviews, which could potentially have applications in social science research related to understanding public opinion.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2310.18368': {'id': 'http://arxiv.org/abs/2310.18368',\n",
       "  'title': 'Muslim-Violence Bias Persists in Debiased GPT Models. (arXiv:2310.18368v2 [cs.CL] UPDATED)',\n",
       "  'abstract': 'Abid et al. (2021) showed a tendency in GPT-3 to generate mostly violent\\ncompletions when prompted about Muslims, compared with other religions. Two\\npre-registered replication attempts found few violent completions and only a\\nweak anti-Muslim bias in the more recent InstructGPT, fine-tuned to eliminate\\nbiased and toxic outputs. However, more pre-registered experiments showed that\\nusing common names associated with the religions in prompts increases\\nseveral-fold the rate of violent completions, revealing a significant\\nsecond-order anti-Muslim bias. ChatGPT showed a bias many times stronger\\nregardless of prompt format, suggesting that the effects of debiasing were\\nreduced with continued model development. Our content analysis revealed\\nreligion-specific themes containing offensive stereotypes across all\\nexperiments. Our results show the need for continual de-biasing of models in\\nways that address both explicit and higher-order associations.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2310.18368',\n",
       "  'authors': ['Babak Hemmatian, Razan Baltaji, Lav R. Varshney'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0.9,\n",
       "    'reason': 'The paper addresses the bias and potential harmful outputs of GPT models, indicating the need for continual debiasing to mitigate security risks.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.8,\n",
       "    'reason': 'The paper discusses the bias and implications of language models when prompted about religion-specific themes, which is relevant to the social science aspect of understanding cultural biases and stereotypes.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.7,\n",
       "    'reason': 'The paper reveals the potential for biased and offensive completions in language models, which indirectly relates to the need for factual and unbiased responses.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.8,\n",
       "    'reason': \"The paper's discussion on biased completions and offensive stereotypes can contribute to understanding the potential for language models to generate misinformation and the importance of fact-checking.\"}}},\n",
       " 'http://arxiv.org/abs/2311.06062': {'id': 'http://arxiv.org/abs/2311.06062',\n",
       "  'title': 'Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration. (arXiv:2311.06062v2 [cs.CL] UPDATED)',\n",
       "  'abstract': 'Membership Inference Attacks (MIA) aim to infer whether a target data record\\nhas been utilized for model training or not. Prior attempts have quantified the\\nprivacy risks of language models (LMs) via MIAs, but there is still no\\nconsensus on whether existing MIA algorithms can cause remarkable privacy\\nleakage on practical Large Language Models (LLMs). Existing MIAs designed for\\nLMs can be classified into two categories: reference-free and reference-based\\nattacks. They are both based on the hypothesis that training records\\nconsistently strike a higher probability of being sampled. Nevertheless, this\\nhypothesis heavily relies on the overfitting of target models, which will be\\nmitigated by multiple regularization methods and the generalization of LLMs.\\nThe reference-based attack seems to achieve promising effectiveness in LLMs,\\nwhich measures a more reliable membership signal by comparing the probability\\ndiscrepancy between the target model and the reference model. However, the\\nperformance of reference-based attack is highly dependent on a reference\\ndataset that closely resembles the training dataset, which is usually\\ninaccessible in the practical scenario. Overall, existing MIAs are unable to\\neffectively unveil privacy leakage over practical fine-tuned LLMs that are\\noverfitting-free and private. We propose a Membership Inference Attack based on\\nSelf-calibrated Probabilistic Variation (SPV-MIA). Specifically, since\\nmemorization in LLMs is inevitable during the training process and occurs\\nbefore overfitting, we introduce a more reliable membership signal,\\nprobabilistic variation, which is based on memorization rather than\\noverfitting. Furthermore, we introduce a self-prompt approach, which constructs\\nthe dataset to fine-tune the reference model by prompting the target LLM\\nitself. In this manner, the adversary can collect a dataset with a similar\\ndistribution from public APIs.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2311.06062',\n",
       "  'authors': ['Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, Tao Jiang'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 1,\n",
       "    'reason': 'The paper directly discusses Membership Inference Attacks (MIAs) and their privacy risks on language models, specifically Large Language Models (LLMs), which is highly relevant to the security of AI and language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2311.08360': {'id': 'http://arxiv.org/abs/2311.08360',\n",
       "  'title': 'The Transient Nature of Emergent In-Context Learning in Transformers. (arXiv:2311.08360v3 [cs.LG] UPDATED)',\n",
       "  'abstract': 'Transformer neural networks can exhibit a surprising capacity for in-context\\nlearning (ICL) despite not being explicitly trained for it. Prior work has\\nprovided a deeper understanding of how ICL emerges in transformers, e.g.\\nthrough the lens of mechanistic interpretability, Bayesian inference, or by\\nexamining the distributional properties of training data. However, in each of\\nthese cases, ICL is treated largely as a persistent phenomenon; namely, once\\nICL emerges, it is assumed to persist asymptotically. Here, we show that the\\nemergence of ICL during transformer training is, in fact, often transient. We\\ntrain transformers on synthetic data designed so that both ICL and in-weights\\nlearning (IWL) strategies can lead to correct predictions. We find that ICL\\nfirst emerges, then disappears and gives way to IWL, all while the training\\nloss decreases, indicating an asymptotic preference for IWL. The transient\\nnature of ICL is observed in transformers across a range of model sizes and\\ndatasets, raising the question of how much to \"overtrain\" transformers when\\nseeking compact, cheaper-to-run models. We find that L2 regularization may\\noffer a path to more persistent ICL that removes the need for early stopping\\nbased on ICL-style validation tasks. Finally, we present initial evidence that\\nICL transience may be caused by competition between ICL and IWL circuits.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2311.08360',\n",
       "  'authors': ['Aaditya K. Singh, Stephanie C.Y. Chan, Ted Moskovitz, Erin Grant, Andrew M. Saxe, Felix Hill'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0.2,\n",
       "    'reason': 'The paper does not directly mention security concerns related to AI and language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': 'The paper does not directly mention applications in social science research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0.2,\n",
       "    'reason': 'The paper does not directly discuss using AI to simulate humans in various contexts.'},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.1,\n",
       "    'reason': 'The paper does not directly focus on methods to increase the factuality of language model response.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.1,\n",
       "    'reason': 'The paper does not directly address the use of AI and language models for generating misinformation or fact-checking.'}}},\n",
       " 'http://arxiv.org/abs/2312.04344': {'id': 'http://arxiv.org/abs/2312.04344',\n",
       "  'title': 'Enhancing Medical Task Performance in GPT-4V: A Comprehensive Study on Prompt Engineering Strategies. (arXiv:2312.04344v2 [cs.CL] UPDATED)',\n",
       "  'abstract': \"OpenAI's latest large vision-language model (LVLM), GPT-4V(ision), has piqued\\nconsiderable interest for its potential in medical applications. Despite its\\npromise, recent studies and internal reviews highlight its underperformance in\\nspecialized medical tasks. This paper explores the boundary of GPT-4V's\\ncapabilities in medicine, particularly in processing complex imaging data from\\nendoscopies, CT scans, and MRIs etc. Leveraging open-source datasets, we\\nassessed its foundational competencies, identifying substantial areas for\\nenhancement. Our research emphasizes prompt engineering, an often-underutilized\\nstrategy for improving AI responsiveness. Through iterative testing, we refined\\nthe model's prompts, significantly improving its interpretative accuracy and\\nrelevance in medical imaging. From our comprehensive evaluations, we distilled\\n10 effective prompt engineering techniques, each fortifying GPT-4V's medical\\nacumen. These methodical enhancements facilitate more reliable, precise, and\\nclinically valuable insights from GPT-4V, advancing its operability in critical\\nhealthcare environments. Our findings are pivotal for those employing AI in\\nmedicine, providing clear, actionable guidance on harnessing GPT-4V's full\\ndiagnostic potential.\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2312.04344',\n",
       "  'authors': ['Pengcheng Chen, Ziyan Huang, Zhongying Deng, Tianbin Li, Yanzhou Su, Haoyu Wang, Jin Ye, Yu Qiao, Junjun He'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.1,\n",
       "    'reason': 'The paper does not mention any social science applications or research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.04691': {'id': 'http://arxiv.org/abs/2312.04691',\n",
       "  'title': 'Simul-LLM: A Framework for Exploring High-Quality Simultaneous Translation with Large Language Models. (arXiv:2312.04691v2 [cs.CL] UPDATED)',\n",
       "  'abstract': 'Large language models (LLMs) with billions of parameters and pretrained on\\nmassive amounts of data are now capable of near or better than state-of-the-art\\nperformance in a variety of downstream natural language processing tasks.\\nNeural machine translation (NMT) is one such task that LLMs have been applied\\nto with great success. However, little research has focused on applying LLMs to\\nthe more difficult subset of NMT called simultaneous translation (SimulMT),\\nwhere translation begins before the entire source context is available to the\\nmodel. In this paper, we address key challenges facing LLMs fine-tuned for\\nSimulMT, validate classical SimulMT concepts and practices in the context of\\nLLMs, explore adapting LLMs that are fine-tuned for NMT to the task of SimulMT,\\nand introduce Simul-LLM, the first open-source fine-tuning and evaluation\\npipeline development framework for LLMs focused on SimulMT.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.04691',\n",
       "  'authors': ['Victor Agostinelli, Max Wild, Matthew Raffel, Kazi Ahmed Asif Fuad, Lizhong Chen'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0.9,\n",
       "    'reason': 'The paper does not directly mention using AI to simulate humans in various contexts.'},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.04837': {'id': 'http://arxiv.org/abs/2312.04837',\n",
       "  'title': 'Localized Symbolic Knowledge Distillation for Visual Commonsense Models. (arXiv:2312.04837v2 [cs.AI] UPDATED)',\n",
       "  'abstract': 'Instruction following vision-language (VL) models offer a flexible interface\\nthat supports a broad range of multimodal tasks in a zero-shot fashion.\\nHowever, interfaces that operate on full images do not directly enable the user\\nto \"point to\" and access specific regions within images. This capability is\\nimportant not only to support reference-grounded VL benchmarks, but also, for\\npractical applications that require precise within-image reasoning. We build\\nLocalized Visual Commonsense models, which allow users to specify (multiple)\\nregions as input. We train our model by sampling localized commonsense\\nknowledge from a large language model (LLM): specifically, we prompt an LLM to\\ncollect commonsense knowledge given a global literal image description and a\\nlocal literal region description automatically generated by a set of VL models.\\nWith a separately trained critic model that selects high-quality examples, we\\nfind that training on the localized commonsense corpus can successfully distill\\nexisting VL models to support a reference-as-input interface. Empirical results\\nand human evaluations in a zero-shot setup demonstrate that our distillation\\nmethod results in more precise VL models of reasoning compared to a baseline of\\npassing a generated referring expression to an LLM.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.04837',\n",
       "  'authors': ['Jae Sung Park, Jack Hessel, Khyathi Raghavi Chandu, Paul Pu Liang, Ximing Lu, Peter West, Youngjae Yu, Qiuyuan Huang, Jianfeng Gao, Ali Farhadi, Yejin Choi'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.8,\n",
       "    'reason': 'The paper discusses the training of localized visual commonsense models, which could indirectly contribute to increasing the factuality of language model responses in specific instances within images.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.7,\n",
       "    'reason': 'The paper investigates the training of localized visual commonsense models and how distillation methods result in more precise reasoning, which could potentially be relevant to fact-checking applications.'}}},\n",
       " 'http://arxiv.org/abs/2312.05180': {'id': 'http://arxiv.org/abs/2312.05180',\n",
       "  'title': 'PathFinder: Guided Search over Multi-Step Reasoning Paths. (arXiv:2312.05180v2 [cs.CL] UPDATED)',\n",
       "  'abstract': 'With recent advancements in large language models, methods like\\nchain-of-thought prompting to elicit reasoning chains have been shown to\\nimprove results on reasoning tasks. However, tasks that require multiple steps\\nof reasoning still pose significant challenges to state-of-the-art models.\\nDrawing inspiration from the beam search algorithm, we propose PathFinder, a\\ntree-search-based reasoning path generation approach. It enhances diverse\\nbranching and multi-hop reasoning through the integration of dynamic decoding,\\nenabled by varying sampling methods and parameters. Using constrained\\nreasoning, PathFinder integrates novel quality constraints, pruning, and\\nexploration methods to enhance the efficiency and the quality of generation.\\nMoreover, it includes scoring and ranking features to improve candidate\\nselection. Our approach outperforms competitive baselines on three complex\\narithmetic and commonsense reasoning tasks by 6% on average. Our model\\ngeneralizes well to longer, unseen reasoning chains, reflecting similar\\ncomplexities to beam search with large branching factors.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.05180',\n",
       "  'authors': [\"Olga Golovneva, Sean O'Brien, Ramakanth Pasunuru, Tianlu Wang, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz\"],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': 'The paper does not mention any direct relevance to security aspects of AI and language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': 'The paper focuses on multi-step reasoning paths and performance improvements on reasoning tasks, without mentioning social science research specifically.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': 'The paper does not discuss AI simulation of human behavior, focusing instead on reasoning paths and performance enhancements.'},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.9,\n",
       "    'reason': 'The paper introduces a tree-search-based reasoning path generation approach, integrating quality constraints and scoring features to improve candidate selection, which indirectly relates to increasing the factuality of language model responses.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': 'The paper does not address the generation of misinformation or fact-checking using AI and language models.'}}},\n",
       " 'http://arxiv.org/abs/2312.05488': {'id': 'http://arxiv.org/abs/2312.05488',\n",
       "  'title': 'Can Large Language Models Serve as Rational Players in Game Theory? A Systematic Analysis. (arXiv:2312.05488v2 [cs.AI] UPDATED)',\n",
       "  'abstract': \"Game theory, as an analytical tool, is frequently utilized to analyze human\\nbehavior in social science research. With the high alignment between the\\nbehavior of Large Language Models (LLMs) and humans, a promising research\\ndirection is to employ LLMs as substitutes for humans in game experiments,\\nenabling social science research. However, despite numerous empirical\\nresearches on the combination of LLMs and game theory, the capability\\nboundaries of LLMs in game theory remain unclear. In this research, we endeavor\\nto systematically analyze LLMs in the context of game theory. Specifically,\\nrationality, as the fundamental principle of game theory, serves as the metric\\nfor evaluating players' behavior -- building a clear desire, refining belief\\nabout uncertainty, and taking optimal actions. Accordingly, we select three\\nclassical games (dictator game, Rock-Paper-Scissors, and ring-network game) to\\nanalyze to what extent LLMs can achieve rationality in these three aspects. The\\nexperimental results indicate that even the current state-of-the-art LLM\\n(GPT-4) exhibits substantial disparities compared to humans in game theory. For\\ninstance, LLMs struggle to build desires based on uncommon preferences, fail to\\nrefine belief from many simple patterns, and may overlook or modify refined\\nbelief when taking actions. Therefore, we consider that introducing LLMs into\\ngame experiments in the field of social science should be approached with\\ngreater caution.\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2312.05488',\n",
       "  'authors': ['Caoyun Fan, Jindou Chen, Yaohui Jin, Hao He'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.9,\n",
       "    'reason': 'The paper directly focuses on the application of Large Language Models (LLMs) in social science research, specifically in the context of game theory and human behavior analysis.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0.8,\n",
       "    'reason': 'The paper discusses the potential use of Large Language Models (LLMs) as substitutes for humans in game experiments, which aligns with the concept of simulating human behavior using AI.'},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.05497': {'id': 'http://arxiv.org/abs/2312.05497',\n",
       "  'title': 'History Matters: Temporal Knowledge Editing in Large Language Model. (arXiv:2312.05497v2 [cs.CL] UPDATED)',\n",
       "  'abstract': \"The imperative task of revising or updating the knowledge stored within large\\nlanguage models arises from two distinct sources: intrinsic errors inherent in\\nthe model which should be corrected and outdated knowledge due to external\\nshifts in the real world which should be updated. Prevailing efforts in model\\nediting conflate these two distinct categories of edits arising from distinct\\nreasons and directly modify the original knowledge in models into new\\nknowledge. However, we argue that preserving the model's original knowledge\\nremains pertinent. Specifically, if a model's knowledge becomes outdated due to\\nevolving worldly dynamics, it should retain recollection of the historical\\nknowledge while integrating the newfound knowledge. In this work, we introduce\\nthe task of Temporal Knowledge Editing (TKE) and establish a benchmark AToKe\\n(Assessment of TempOral Knowledge Editing) to evaluate current model editing\\nmethods. We find that while existing model editing methods are effective at\\nmaking models remember new knowledge, the edited model catastrophically forgets\\nhistorical knowledge. To address this gap, we propose a simple and general\\nframework termed Multi-Editing with Time Objective (METO) for enhancing\\nexisting editing models, which edits both historical and new knowledge\\nconcurrently and optimizes the model's prediction for the time of each fact.\\nOur assessments demonstrate that while AToKe is still difficult, METO maintains\\nthe effectiveness of learning new knowledge and meanwhile substantially\\nimproves the performance of edited models on utilizing historical knowledge.\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2312.05497',\n",
       "  'authors': ['Xunjian Yin, Jin Jiang, Liming Yang, Xiaojun Wan'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': 'The paper does not directly mention security of AI and language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.8,\n",
       "    'reason': 'The paper is relevant as it discusses the task of Temporal Knowledge Editing (TKE) and evaluates current model editing methods, which could be applicable to social science research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.9,\n",
       "    'reason': \"The paper is directly relevant as it proposes a framework for enhancing existing editing models to optimize the model's prediction for the time of each fact, which could increase the factuality of language model responses.\"},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.05603': {'id': 'http://arxiv.org/abs/2312.05603',\n",
       "  'title': 'Sim-GPT: Text Similarity via GPT Annotated Data. (arXiv:2312.05603v2 [cs.CL] UPDATED)',\n",
       "  'abstract': 'Due to the lack of a large collection of high-quality labeled sentence pairs\\nwith textual similarity scores, existing approaches for Semantic Textual\\nSimilarity (STS) mostly rely on unsupervised techniques or training signals\\nthat are only partially correlated with textual similarity, e.g., NLI-based\\ndatasets. To tackle this issue, in this paper, we propose the strategy of\\nmeasuring text similarity via GPT annotated data (Sim-GPT for short). The core\\nidea of Sim-GPT is to generate data with STS labels using GPT-4, based on which\\nan STS model is trained. Sim-GPT framework utilizes LLMs to provide a\\nsubstantial amount of reliable annotated data filling the gap of the lack of\\ntraining signals for STS. Sim-GPT is trained on a one-time generated dataset\\nusing BERT or RoBERTa as the backbone, which offers long-term savings in cost\\nand speed compared to repeatedly invoking LLMs for each sentence pair. Trained\\non the examples from GPT-4 (371K), Sim-GPT yields SOTA performances on the\\nwidely-used seven STS benchmarks: +0.99 over supervised-SimCSE, and +0.42 over\\nthe current SOTA PromCSE model. To encourage further advancements of the field,\\nwe release both models and the 371K annotated examples from GPT-4. Code, models\\nand annotated data are available at: https://github.com/ShuheWang1998/Sim-GPT.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.05603',\n",
       "  'authors': ['Shuhe Wang, Beiming Cao, Shengyu Zhang, Xiaoya Li, Jiwei Li, Fei Wu, Guoyin Wang, Eduard Hovy'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0.8,\n",
       "    'reason': 'The paper does not directly mention security, but it focuses on utilizing GPT-4 to generate annotated data for training an STS model. While the main focus is on text similarity, the use of language models like GPT-4 and the release of annotated data could indirectly relate to security aspects of language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.6,\n",
       "    'reason': 'The paper focuses on using GPT-4 to generate reliable annotated data for training a Semantic Textual Similarity (STS) model. While the primary objective is not directly related to factuality, the emphasis on generating reliable data using language models could indirectly contribute to improving the factuality of language model responses.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.3,\n",
       "    'reason': 'The paper does not mention generating misinformation or fact-checking. It primarily focuses on utilizing GPT-4 for generating annotated data and training an STS model, which is not directly related to generating misinformation or fact-checking.'}}},\n",
       " 'http://arxiv.org/abs/2312.05662': {'id': 'http://arxiv.org/abs/2312.05662',\n",
       "  'title': 'Understanding the Effect of Model Compression on Social Bias in Large Language Models. (arXiv:2312.05662v2 [cs.CL] UPDATED)',\n",
       "  'abstract': \"Large Language Models (LLMs) trained with self-supervision on vast corpora of\\nweb text fit to the social biases of that text. Without intervention, these\\nsocial biases persist in the model's predictions in downstream tasks, leading\\nto representational harm. Many strategies have been proposed to mitigate the\\neffects of inappropriate social biases learned during pretraining.\\nSimultaneously, methods for model compression have become increasingly popular\\nto reduce the computational burden of LLMs. Despite the popularity and need for\\nboth approaches, little work has been done to explore the interplay between\\nthese two. We perform a carefully controlled study of the impact of model\\ncompression via quantization and knowledge distillation on measures of social\\nbias in LLMs. Longer pretraining and larger models led to higher social bias,\\nand quantization showed a regularizer effect with its best trade-off around 20%\\nof the original pretraining time.\\n\",\n",
       "  'url': 'http://arxiv.org/abs/2312.05662',\n",
       "  'authors': ['Gustavo Gonçalves, Emma Strubell'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': 'The paper does not directly mention security aspects of AI and language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.8,\n",
       "    'reason': 'The paper directly addresses the impact of social bias in large language models, which is highly relevant to the applications of AI and language models in social science research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.4,\n",
       "    'reason': 'While the paper does not directly focus on increasing factuality, it does touch on the impact of social biases in language models, which indirectly relates to the factuality of language model responses.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0.3,\n",
       "    'reason': 'The paper does not directly discuss the generation of misinformation or fact-checking using language models.'}}},\n",
       " 'http://arxiv.org/abs/2312.06363': {'id': 'http://arxiv.org/abs/2312.06363',\n",
       "  'title': 'MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples. (arXiv:2312.06363v2 [cs.AI] UPDATED)',\n",
       "  'abstract': 'Although In-Context Learning (ICL) brings remarkable performance gains to\\nLarge Language Models (LLMs), the improvements remain lower than fine-tuning on\\ndownstream tasks. This paper introduces Multi-Modal In-Context Tuning (MMICT),\\na novel multi-modal fine-tuning paradigm that boosts multi-modal fine-tuning by\\nfully leveraging the promising ICL capability of multi-modal LLMs (MM-LLMs). We\\npropose the Multi-Modal Hub (M-Hub), a unified module that captures various\\nmulti-modal features according to different inputs and objectives. Based on\\nM-Hub, MMICT enables MM-LLMs to learn from in-context visual-guided textual\\nfeatures and subsequently generate outputs conditioned on the textual-guided\\nvisual features. Moreover, leveraging the flexibility of M-Hub, we design a\\nvariety of in-context demonstrations. Extensive experiments on a diverse range\\nof downstream multi-modal tasks demonstrate that MMICT significantly\\noutperforms traditional fine-tuning strategy and the vanilla ICT method that\\ndirectly takes the concatenation of all information from different modalities\\nas input.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.06363',\n",
       "  'authors': ['Tao Chen, Enwei Zhang, Yuting Gao, Ke Li, Xing Sun, Yan Zhang, Hui Li'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0.8,\n",
       "    'reason': 'The paper introduces a novel multi-modal fine-tuning paradigm for large language models, which could potentially be applied to simulate human-like behavior in multi-modal contexts.'},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.06635': {'id': 'http://arxiv.org/abs/2312.06635',\n",
       "  'title': 'Gated Linear Attention Transformers with Hardware-Efficient Training. (arXiv:2312.06635v2 [cs.LG] UPDATED)',\n",
       "  'abstract': 'Transformers with linear attention allow for efficient parallel training but\\ncan simultaneously be formulated as an RNN with 2D (matrix-valued) hidden\\nstates, thus enjoying linear (with respect to output length) inference\\ncomplexity. Recent works such as RetNet (Sun et al., 2023) and TransNormerLLM\\n(Qin et al., 2023a) observe that adding a global decay term to the additive RNN\\nupdate rule greatly improves performance, sometimes outperforming standard\\nTransformers with softmax attention when trained at scale. In this work we show\\nthat adding a data-dependent gating mechanism further improves performance. We\\nderive a parallel form of this gated linear attention layer that enables\\nefficient training. However, a straightforward, numerically stable\\nimplementation of this parallel form requires generalized matrix\\nmultiplications in log-space for numerical stability, and thus cannot take\\nadvantage of tensor cores on modern GPUs which are optimized for standard\\nmatrix multiplications. We develop a hardware-efficient version of the parallel\\nform that can still make use of tensor cores through block-parallel\\ncomputations over sequence chunks. Experiments on moderate-scale language\\nmodeling (340M-parameter models trained on 15B tokens, 1.3B-parameter models\\ntrained on 100B tokens) show that gated linear attention (GLA) Transformers\\nperform competitively against a strong LLaMA-architecture Transformer baseline\\n(Touvron et al., 2023) as well as Mamba (Gu & Dao, 2023), a recently introduced\\nstate-space model with a data-dependent state transition mechanism. For\\ntraining speed, our Triton-based implementation performs comparably to\\nCUDA-optimized FlashAttention-2 (Dao, 2023) under the regular 2048 training\\nlength setting, while outperforming FlashAttention-2 when training on longer\\nsequences beyond 4096.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.06635',\n",
       "  'authors': ['Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': ''},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.9,\n",
       "    'reason': 'The paper discusses methods to improve the performance of Transformers with linear attention, which could indirectly contribute to the factuality of language model responses.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': ''}}},\n",
       " 'http://arxiv.org/abs/2312.06648': {'id': 'http://arxiv.org/abs/2312.06648',\n",
       "  'title': 'Dense X Retrieval: What Retrieval Granularity Should We Use?. (arXiv:2312.06648v2 [cs.CL] UPDATED)',\n",
       "  'abstract': 'Dense retrieval has become a prominent method to obtain relevant context or\\nworld knowledge in open-domain NLP tasks. When we use a learned dense retriever\\non a retrieval corpus at inference time, an often-overlooked design choice is\\nthe retrieval unit in which the corpus is indexed, e.g. document, passage, or\\nsentence. We discover that the retrieval unit choice significantly impacts the\\nperformance of both retrieval and downstream tasks. Distinct from the typical\\napproach of using passages or sentences, we introduce a novel retrieval unit,\\nproposition, for dense retrieval. Propositions are defined as atomic\\nexpressions within text, each encapsulating a distinct factoid and presented in\\na concise, self-contained natural language format. We conduct an empirical\\ncomparison of different retrieval granularity. Our results reveal that\\nproposition-based retrieval significantly outperforms traditional passage or\\nsentence-based methods in dense retrieval. Moreover, retrieval by proposition\\nalso enhances the performance of downstream QA tasks, since the retrieved texts\\nare more condensed with question-relevant information, reducing the need for\\nlengthy input tokens and minimizing the inclusion of extraneous, irrelevant\\ninformation.\\n',\n",
       "  'url': 'http://arxiv.org/abs/2312.06648',\n",
       "  'authors': ['Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin Ma, Xinran Zhao, Hongming Zhang, Dong Yu'],\n",
       "  'judgement': {'Security of AI and language models': {'relevance': 0,\n",
       "    'reason': 'The paper does not directly mention security of AI and language models.'},\n",
       "   'Applications of AI and language models in social science research': {'relevance': 0.8,\n",
       "    'reason': 'The paper does not explicitly mention social science research, but it discusses the use of dense retrieval in NLP tasks, which could potentially have applications in social science research.'},\n",
       "   'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "    'reason': 'The paper does not address the topic of using AI to simulate humans in various contexts.'},\n",
       "   'Methods to increase the factuality of language model response': {'relevance': 0.9,\n",
       "    'reason': 'The paper is directly relevant as it discusses the impact of different retrieval units on the performance of dense retrieval and downstream tasks, which could potentially relate to improving the factuality of language model responses.'},\n",
       "   'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "    'reason': 'The paper does not address the topic of generating misinformation or fact-checking using AI and language models.'}}}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0339fc25-1ecf-4fac-8a1e-34d276d7de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "judgement_file = os.path.join(temp_data_dir, f\"{date}.resp.json\")\n",
    "judgement_results = {}\n",
    "with open(judgement_file) as f:\n",
    "    for line in f:\n",
    "        paper = json.loads(line)\n",
    "        judgement_results[paper['id']] = paper['judgement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36785ebe-0004-4b11-9d61-526f7d9fbf87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Security of AI and language models': {'relevance': 0,\n",
       "  'reason': 'The paper does not directly address security of AI and language models.'},\n",
       " 'Applications of AI and language models in social science research': {'relevance': 0.9,\n",
       "  'reason': 'The paper directly addresses the application of AI and surveillance technology in social science research, particularly in the context of public safety concerns.'},\n",
       " 'Using AI to simulate humans in various contexts': {'relevance': 0,\n",
       "  'reason': ''},\n",
       " 'Methods to increase the factuality of language model response': {'relevance': 0,\n",
       "  'reason': ''},\n",
       " 'AI and language models for generating misinformation or fact-checking': {'relevance': 0,\n",
       "  'reason': ''}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judgement_results['http://arxiv.org/abs/2312.06707']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "741709b8-9349-469c-a4c0-d48ea5f4bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper_id in paper_dict.keys():\n",
    "    paper_judgement = judgement_results[paper_id]\n",
    "    paper_dict[paper_id]['judgement'] = paper_judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d079d6f-e08e-43c6-9c20-90bc6fae3148",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_threshold = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c2e2f1c-190a-4325-9535-faff47db3f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_papers = {}\n",
    "for id, paper_info in paper_dict.items():\n",
    "    for topic, relevance in paper_info['judgement'].items():\n",
    "        if relevance['relevance'] > relevance_threshold:\n",
    "            temp_obj = {\n",
    "                    \"paper_id\": id,\n",
    "                    \"relevance\": relevance\n",
    "                }\n",
    "            if topic in relevant_papers:\n",
    "                relevant_papers[topic].append(temp_obj)\n",
    "            else:\n",
    "                relevant_papers[topic] = [temp_obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e59b6d0-c858-403a-8b55-6a3d4dd23802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Applications of AI and language models in social science research': [{'paper_id': 'http://arxiv.org/abs/2312.06707',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper directly addresses the application of AI and surveillance technology in social science research, particularly in the context of public safety concerns.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2312.06861',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper directly discusses the application of AI-based tools in moderating social media platforms and ensuring safety of conversational AI technologies, which aligns with the topic of applications of AI and language models in social science research.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2312.07492',\n",
       "   'relevance': {'relevance': 1,\n",
       "    'reason': 'The paper directly addresses the application of generative language models in social science research by introducing a comprehensive benchmark to capture the amplification of social bias via stigmas in language models.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2303.00357',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper directly addresses the application of language models in analyzing online discourse and the effectiveness of counter speech strategies, which is clearly relevant to social science research.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2309.16808',\n",
       "   'relevance': {'relevance': 1,\n",
       "    'reason': 'The paper directly addresses the application of machine learning and computer vision in estimating neighborhood socioeconomic indicators, which is a form of social science research.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2312.06825',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper discusses the utilization of non-verbal behavior and social gaze in classroom human-robot interaction scenarios, which could be considered a form of AI application in social science research.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2312.06926',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper discusses the utilization of AI for enriching the under-resource-status dialectal Arabic and fast-tracking the research of diverse online social behaviors within and across smart cities in different geo-regions.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2312.07141',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper directly addresses the application of multilingual large language models in investigating stereotypes and biases across languages, which is highly relevant to social science research.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2312.07194',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper directly discusses the application of computational linguistic methods for analyzing the time course of outrage wave, which is relevant to the application of AI and language models in social science research.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2212.13201',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper presents an approach that converts linear programming word problems into mathematical formulations, which can be applied in social science research where optimization problems are often encountered.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2306.12424',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper introduces a dataset for benchmarking gender bias in vision-language models, which could be relevant for social science research related to bias evaluation and gender representation.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2312.05488',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper directly focuses on the application of Large Language Models (LLMs) in social science research, specifically in the context of game theory and human behavior analysis.'}}],\n",
       " 'Methods to increase the factuality of language model response': [{'paper_id': 'http://arxiv.org/abs/2312.06833',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper does not directly address this topic.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2312.06681',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper directly addresses the use of CAA to control and alter behaviors such as factual versus hallucinatory responses in language models, thus relevant to methods for increasing factuality.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2312.07028',\n",
       "   'relevance': {'relevance': 0.85,\n",
       "    'reason': 'The paper addresses the issue of improving fine-tuning of language models, which indirectly relates to the factuality of the language model response.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2312.07049',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper directly addresses the improvement of factual error correction in language models, which is closely related to increasing the factuality of language model responses.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2312.07069',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper aims to enhance the understanding and mitigation of errors in Large Language Models, contributing to the improvement of LLM accuracy and reliability, which is directly related to increasing the factuality of language model responses.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2210.01959',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper directly addresses the challenge of extracting well-posed contexts from potentially long, ill-formatted documents, which is relevant to increasing the factuality of language model responses.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2306.03241',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper directly addresses the methods to improve convergence and generalization during the training of large language models, which is closely related to increasing the factuality of language model responses.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2312.05180',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper introduces a tree-search-based reasoning path generation approach, integrating quality constraints and scoring features to improve candidate selection, which indirectly relates to increasing the factuality of language model responses.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2312.05497',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': \"The paper is directly relevant as it proposes a framework for enhancing existing editing models to optimize the model's prediction for the time of each fact, which could increase the factuality of language model responses.\"}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2312.06635',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper discusses methods to improve the performance of Transformers with linear attention, which could indirectly contribute to the factuality of language model responses.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2312.06648',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper is directly relevant as it discusses the impact of different retrieval units on the performance of dense retrieval and downstream tasks, which could potentially relate to improving the factuality of language model responses.'}}],\n",
       " 'Security of AI and language models': [{'paper_id': 'http://arxiv.org/abs/2312.07492',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper discusses the amplification of social bias in generative language models, which directly relates to the security aspect of AI and language models as it highlights the potential harm caused by biased outputs.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2312.06674',\n",
       "   'relevance': {'relevance': 1,\n",
       "    'reason': 'The paper introduces Llama Guard, an LLM-based input-output safeguard model geared towards Human-AI conversation use cases. It discusses incorporating a safety risk taxonomy and classifying responses generated by LLMs, demonstrating strong performance in content moderation, which directly relates to the security of AI and language models.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2312.06798',\n",
       "   'relevance': {'relevance': 1,\n",
       "    'reason': 'The paper directly discusses the importance of safety and reliability in AI systems, particularly in the context of language models. It mentions the challenges associated with black box models and the need for safety guardrails, which aligns with the topic of security of AI and language models.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2312.06924',\n",
       "   'relevance': {'relevance': 1,\n",
       "    'reason': 'The paper directly addresses safety considerations and vulnerabilities in the context of language models, highlighting the disparity in safety alignment of various NLP tasks and the potential risks associated with weaker safety alignment.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2312.07000',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper discusses the importance of alignment for honesty in language models to ensure they proactively refuse to answer questions when they lack knowledge, while still not being overly conservative. This aligns with the broader topic of security and safety in AI and language models.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2312.07069',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper discusses the challenges and errors inherent to Large Language Models (LLMs), including their propensity for logic mistakes and incorrect conclusions, which are relevant to the security implications of AI and language models.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2312.07110',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper directly discusses the use of large language models (LLMs) in the cybersecurity domain, which involves aspects of security. However, it does not specifically focus on the security of AI and language models themselves.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2309.13788',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper directly addresses the potential exploitation of LLMs to generate misinformation, posing a serious concern to online safety and public trust. It discusses the difficulty in detecting LLM-generated misinformation, which is highly relevant to the security of AI and language models.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2310.18368',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper addresses the bias and potential harmful outputs of GPT models, indicating the need for continual debiasing to mitigate security risks.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2311.06062',\n",
       "   'relevance': {'relevance': 1,\n",
       "    'reason': 'The paper directly discusses Membership Inference Attacks (MIAs) and their privacy risks on language models, specifically Large Language Models (LLMs), which is highly relevant to the security of AI and language models.'}}],\n",
       " 'AI and language models for generating misinformation or fact-checking': [{'paper_id': 'http://arxiv.org/abs/2312.06681',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': \"The paper discusses CAA's effectiveness in altering model behavior, sheds light on how high-level concepts are represented in Large Language Models, and addresses the manipulation of responses, including factual and misleading, so it is relevant to the generation of misinformation and fact-checking.\"}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2312.07066',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper focuses on AI-based image synthesis and storytelling, which can be used in creating fictional scenarios, an area with potential application in the generation of misinformation and fact-checking.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2305.07402',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper proposes a framework called InteR, which facilitates information refinement through synergy between retrieval models and large language models. The iterative refinement process aims to enhance prompt formulation and retrieval accuracy, which could potentially be used for fact-checking or mitigating misinformation.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2308.06077',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper introduces a framework for Cost-Effective Language Model Choice (CELMOC), which can help in saving large amounts of money without sacrificing performance, thus indirectly addressing the use of AI and language models for generating misinformation or fact-checking.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2309.13788',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper directly investigates the potential for LLMs, such as ChatGPT, to be exploited to generate misinformation and discusses the implications of LLM-generated misinformation on combating misinformation. It is directly relevant to the topic of AI and language models for generating misinformation.'}}],\n",
       " 'Using AI to simulate humans in various contexts': [{'paper_id': 'http://arxiv.org/abs/2312.06722',\n",
       "   'relevance': {'relevance': 1,\n",
       "    'reason': 'The paper discusses the use of Multimodal Large Language Models (MLLMs) for embodied task planning, involving interactions with objects and complex visual observations in real-world scenarios, which aligns with the concept of simulating humans in various contexts.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2303.00855',\n",
       "   'relevance': {'relevance': 1,\n",
       "    'reason': 'The paper discusses using language-conditioned robotic policies to provide grounding for embodied agents, which can be considered as a form of simulating humans in various contexts.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2309.07870',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper mentions building autonomous language agents that interact with environments, humans, and other agents using natural language interfaces, indicating the simulation of human interactions.'}},\n",
       "  {'paper_id': 'http://arxiv.org/abs/2312.04691',\n",
       "   'relevance': {'relevance': 0.9,\n",
       "    'reason': 'The paper does not directly mention using AI to simulate humans in various contexts.'}}]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_papers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
